<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="github-pandoc.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>
<div id="TOC">
<ul>
<li><a href="#a-comparison-and-generalization-of-blocking-and-windowing-algorithms-for-dd">A comparison and generalization of blocking and windowing algorithms for DD</a></li>
<li><a href="#a-comparison-of-fast-blocking">A comparison of fast blocking</a></li>
<li><a href="#adaptive-duplicate-detection-using-learnable-string-similarity-measures">Adaptive Duplicate Detection Using Learnable String Similarity Measures</a></li>
<li><a href="#adaptive-windowing-for-dd">Adaptive Windowing for DD</a></li>
<li><a href="#context-aware-approximate-string-matching-for-large-scale-real-time-entity-resolution">Context-Aware Approximate String Matching for Large-Scale Real-Time Entity Resolution</a></li>
<li><a href="#dynamic-similarity-aware-inverted-indexing-for-real-time-entity-resolution">Dynamic similarity-aware inverted indexing for real-time entity resolution</a></li>
<li><a href="#dynamic-sorted-neighborhood-indexing-for-real-time-entity-resolution">Dynamic Sorted Neighborhood Indexing for Real-Time Entity Resolution</a></li>
<li><a href="#effiziente-mr-parallelisierung-von-er-workflows">Effiziente MR Parallelisierung von ER Workflows</a></li>
<li><a href="#entity-resolution-approach-of-data-stream-management-systems">Entity Resolution Approach of Data Stream Management Systems</a></li>
<li><a href="#evaluation-of-er-approaches-on-real-world-match-problems">Evaluation of ER approaches on Real-World-Match problems</a></li>
<li><a href="#similarity-aware-inverted-indexing-for-real-time-entity-resolution">Similarity Aware Inverted Indexing for Real-time Entity Resolution</a></li>
<li><a href="#towards-scalable-real-time-entity-resolution-using-a-similarity-aware-inverted-index-approach-1">Towards Scalable Real-time Entity Resolution Using a Similarity-aware Inverted Index Approach</a></li>
</ul>
</div>
<h1 id="a-comparison-and-generalization-of-blocking-and-windowing-algorithms-for-dd">A comparison and generalization of blocking and windowing algorithms for DD</h1>
<p>This paper performs a evaluation between Standard Blocking and Sorted Neighborhood. From the results it introduces the Sorted Blocks blocking/windowing method for ER.</p>
<h2 id="standard-blocking">Standard Blocking</h2>
<p>Partitions sets of tuples into disjoint partitions (blocks). All tuples within each block are compared. Blocking result depends on a good partition predicate which controls number and size of the blocks. To detect duplicates in different partitions multi-pass blocking is applied.</p>
<h2 id="sorted-neighborhood">Sorted Neighborhood</h2>
<p>Step one assign sorting key to each tuple (must not be unique). Step two sort tuples according to thier sorting key. Final step slide a windows of fixed size across the sorted tuples and compare all tuples within the window. To avoid miss-sorts used multi-pass variants with differnt sorting keys per tupel.</p>
<h2 id="comparision">Comparision</h2>
<ul>
<li>Both rely on oderings</li>
<li>Both assume that tuples close to each other hava a higher chance of being duplicates.</li>
<li>Both methods perform approximatly the same total number of comparisions. Though the actual comparisions differ.</li>
<li>SB applies no overlap of partitions</li>
<li>SN applies total overlap of partitions</li>
</ul>
<h2 id="sorted-blocks">Sorted Blocks</h2>
<p>Sorted Blocks aim to optimize the overlap between partitions. As SB and SN are the two extrem examples this algorithm is a combination of both. Assumption is that the realy overlap should be big enough that real duplicates are detected but not too high in order to reduce the number of comparisions.</p>
<p>Idea: First sort all tuples, then partition recored into disjoint sorted subsets and finally overlap partitions. The number of comparisions can be controlled the size of the overlap <span class="math inline">\(u\)</span> e.g. <span class="math inline">\(u=3\)</span>.</p>
<p>Within each block every tuples is compared with each other. Within the overlap window <span class="math inline">\(u+1\)</span> which is slid across the partitions the first tuple of the windows is compared with the remaining.</p>
<p>The overall complexity is $O(n(+n)).</p>
<h1 id="a-comparison-of-fast-blocking">A comparison of fast blocking</h1>
<p>Daten liegen zunächst in Datensätzen (Records) vor. Diese werden dann anschließend bereinigt und standadisiert. Die Blocking Verfahren machen aus den Datensätzen Datensatzpaare (Record Pairs), welche verglichen werden. Das Ergebnis der Vergleiche sind Ähnlichkeitsvektoren (Comparision Vectors). Anhand dieser wird abschließend die Klassifizierung in Matches bzw. Non-Matches getätigt.</p>
<h1 id="adaptive-duplicate-detection-using-learnable-string-similarity-measures">Adaptive Duplicate Detection Using Learnable String Similarity Measures</h1>
<p>This paper present a framework for improving duplicate detection with trainable measures of textual simularity. Thereby it differanties between character based and token based measure. The former through edit distance and the latter through vector-space using TF.IDF.</p>
<p>Accurate simularity requires adapting the string simularity metric for each field of a tupel according to the data domain. The get the best results a training based system must use a two step approach. Step one is field based and step two is record based.</p>
<h1 id="adaptive-windowing-for-dd">Adaptive Windowing for DD</h1>
<p>This paper introduces a alternative method of Sorted Neighborhood that resizes its window according to the number of duplicates found.</p>
<h2 id="problems-with-snm">Problems with SNM</h2>
<p>If the window size is too small, duplicates are missed. On the other hand if the window size is too big unnecessary comparisions are performed. Even with the ideal windows size which is equal to the size of the largest cluster a lot of unnecessary comparisions are performed. This is due to dirty real world data which results in few large and many small clusters.</p>
<h2 id="duplicate-count-strategy">Duplicate Count Strategy</h2>
<p>Assumption: The more duplicates are found in close proximity the larger the window. If no duplicate found in close proximity there are none or they are very far from each other.</p>
<p>Thus increase the windows size according to the average duplicates found in the current windows. To reduce the number of comparisions calculate the transitive closure.</p>
<h2 id="conclusion">Conclusion</h2>
<p>DCS++ is more efficient than SNM without loss of effectiveness. DCS++ uses transitive closure thus an effective algorithm is needed.</p>
<h1 id="context-aware-approximate-string-matching-for-large-scale-real-time-entity-resolution">Context-Aware Approximate String Matching for Large-Scale Real-Time Entity Resolution</h1>
<p>This paper introduces a string matching algorthim for real-time entity resolution that incorperates contextual information into simularity calculation. These context information are frequencies of two strings to be matched, likelihood of edits between them and number of other strings that are highly similar. As this paper addresses real-time entity resolution the data are pre-calculated with an Index.</p>
<h2 id="building-the-index">Building the Index</h2>
<p>For each attribute a and EDIndex is generated. The EDIndex consists of an interted index of all strings in attribute a with their frequencies and record identifiers. A graph of string pairs within a maximum edit distance of m and and inverted index of edit path and their frequencies.</p>
<p>Re-implement to understand the rest of the paper</p>
<h2 id="summary">Summary</h2>
<p>Taking edit frequency and string neighborhood into consideration can lead to improvements in matching quality. However both string frequency and string inverse document frequency make things worse.</p>
<p>This approach can be integrated with Towards Scalable Real-Time Entity Resolution using a S imilarity-Aware Inverted Index Approach for example.</p>
<h1 id="dynamic-similarity-aware-inverted-indexing-for-real-time-entity-resolution">Dynamic similarity-aware inverted indexing for real-time entity resolution</h1>
<p>This paper improves the Similarity-Aware Inverted Index approach from <span class="citation">[@CG:Scalable:08]</span> to allows dynamic additions to the index. Also it investigates the frequency-distribution of data and the effect of reducing the index to hold only frequent records.</p>
<p>In recent years dynamic entity resolution becomes more and more important as many businesses used databases that are not static and change over time. Therefore blocking techniques for Real-time data should allow to change as well. Meaning that if new records are inserted they have to be applied to the blocked/indexed data as well.</p>
<h2 id="dynamic-similarity-aware-inverted-index-approach">Dynamic Similarity-Aware Inverted Index approach</h2>
<p>As this approach is an extension to <span class="citation">[@CG:Scalable:08]</span> it has the same components. Namely a Record Index (RI) holding the reference identifiers for each record attribute, the Block Index (BI) storing the unique attribute values assosiated with the RI keys and a Similarity Index (SI) holding the pre-calculated similarities between attribute values in the same block.</p>
<p><strong>Build Time</strong>. Building the Indexes is equal to the original approach. Insert or append to RI then insert into BI or if appending to BI calculate the similarities insert into SI and add new similiarity from all entries of BI into SI.</p>
<p><strong>Query Time</strong>. Works the same as the original approach if the record attribute can be found in the RI. Then insert RI retrived list into accumulator with similarity 1 then insert all entries in SI with their identifiers from RI and similarity values into the accumulator. If record attribute is not available insert it into the indexed before executing the search. Thereby the extending the index with the new record.</p>
<h2 id="frequency-filtered-index">Frequency-Filtered Index</h2>
<p>As this index can become quite large it is wise to reduce the amount of indexed records to the most frequent. As many real world datasets follow zips law, meaning there are few frequent attribute and many infrequent ones, the index can be optimized to only index most x% frequent values. This requires an list of the most frequent attritbutes which needs to be provided. I.e. by analysing available dictionaries.</p>
<h2 id="evaluation">Evaluation</h2>
<p>On a dataset with 2.5 million records the authors show that time needed to insert a single records is almost constant and thereby independent of the dataset size. Times average around 0.1 msec. The query time is also not effected by the growing index and averages around 0.1 sec.</p>
<p>The result from the frequency filtering show that the amount of memory can almost be halfed if only the top 10% frequent values are index. With 30% there is only a 2% drop in recall.</p>
<h1 id="dynamic-sorted-neighborhood-indexing-for-real-time-entity-resolution">Dynamic Sorted Neighborhood Indexing for Real-Time Entity Resolution</h1>
<p>This paper proposes a Sorted Neighborhood Indexing approach for Real-Time Entity resolution. Therefore instead of using a static array as datastructure the propose a tree-based technique and introduce many popular variations of the classic SNM with adaptive windowing. Also they enhance query time by pre-calculating attribute scores.</p>
<p>The proposed approach underlies the assumption that every query record is added to the index. By doing so the history of queries and changes in a address for example wont get lost which might be useful for a credit bureau.</p>
<p><strong>Challenge</strong> in Real-Time Indexing is to develop indexding techniques that allow dynamic updates and facilitye ral-time matching by generating a small number of high-quality candidate records that are to be compared.</p>
<p><strong>Problem</strong> for Real-Time ER: for each query record <span class="math inline">\(q_j\)</span> in a query stream <span class="math inline">\(Q\)</span>, find all the records in <span class="math inline">\(R\)</span> that belong to the same entity as <span class="math inline">\(q_j\)</span>, denoted as the set <span class="math inline">\(M_q\)</span>, in subsecond time, <span class="math inline">\(M_{q_j} = \{r_i|r_i.eid = q_j.eid, r_i \in R\}, M_{q_j} \subseteq R, q_j \in Q\)</span>.</p>
<h2 id="data-structure">Data Structure</h2>
<p>BraidedTree (BRT) is a balanced binary AVL tree where each node in the tree has a link to its predecessor and succesor nodes according to an alphabetical sorting of the key values in the nodes. A node consists of it's sorting key (SKV), a list of nodes with this sorting key and a link its predecessor and succesor. A query node (i.e. the node where a query record has been sorted in) is denoted as <span class="math inline">\(N_q\)</span>.</p>
<h2 id="dynamic-sorted-neighborhood-indexing-dysni">Dynamic Sorted Neighborhood Indexing (DySNI)</h2>
<p>The first assumption is that all records are kept unmodified after creation since the can provide evidence about earlier requests. DySNI has a inital build phase and a query phase similar to DySimII.</p>
<p><strong>Build Phase</strong>. The loads records from a possible empty dataset <span class="math inline">\(R\)</span> into the BRT. In a first step the SKV for a record <span class="math inline">\(r\)</span> is generated. Which may be a single attribute or a concatenation of multiple. If the SKV is new a new node is created in the tree. If the SKV already exists the query record is added to an existing node. The complete list of records <span class="math inline">\(R\)</span> are also indexed into an inverted list <span class="math inline">\(D\)</span> so they can be retrieved for a fast attribute by attribute comparision.</p>
<p><strong>Query Phase</strong>. As said earlier the assumption is that all records are inserted into the index. Therefore a SKV is generated for a query record <span class="math inline">\(q\)</span> together with a new unique record identifier <span class="math inline">\(q.id\)</span>. The SKV and <span class="math inline">\(q.id\)</span> is then inserted into the BRT as decribed in the build phase. The query record is added to <span class="math inline">\(D\)</span> as well.</p>
<p>Now the window of neighboring nodes can be generated. All records identifiers that are stored in the window's nodes are added the a candidate record set <span class="math inline">\(C\)</span>. The whole records from <span class="math inline">\(D\)</span> are then retrieved for all records in <span class="math inline">\(C\)</span> and a pair comparision for all records is performed. The compared candidate records are return in the list <span class="math inline">\(M\)</span> which is sorted by the overall similarity. For generating the window neighbors this paper introduces 4 approaches.</p>
<h3 id="fixed-window-size-dysni-f">Fixed Window Size (DySNI-f)</h3>
<p>This approach in based on the classical SNM from Hernández &amp; Stolfo. The window is set as number of neighboring tree nodes in one direction (previous or next). The window is set as number of neighboring tree nodes in one direction (previous or next) of the query Node <span class="math inline">\(N_q\)</span>. A window of size <span class="math inline">\(w=0\)</span> refers to the query node <span class="math inline">\(N_q\)</span> only. A fixed-size window can lead to both unnecessary comparision with records in nodes that are unlikely to have a high enough similarity, as well as potential true matches outside the window.</p>
<h3 id="canidates-based-adaptive-window-dysni-c">Canidates-Based Adaptive Window (DySNI-c)</h3>
<p>This apprach aims at matiching a certain minimum number of records. The total minimum number to records to be return as candidates is denoted as <span class="math inline">\(\delta\)</span>. The initial candidate set contains only the records in the query node <span class="math inline">\(N_q\)</span>. The decision to expand on both sides is made if the count of records at the qurey record's <span class="math inline">\(N_q\)</span> is smaller than the minimum threshold <span class="math inline">\(|C| &lt; \delta\)</span>. The remaining total records to reach is calculated by <span class="math inline">\(r = \delta - |C|\)</span>. The expansion threshold for each direction is then set to <span class="math inline">\(\lceil r/2 \rceil\)</span>.</p>
<h1 id="effiziente-mr-parallelisierung-von-er-workflows">Effiziente MR Parallelisierung von ER Workflows</h1>
<p>Duplikateerkennung ist ein paarweiser Vergleich von Datensätzen bzgl. verschiedener Ähnlichkeitsmaße. Die Auswertung entspricht dabei dem Kartesischem Produkt und hat bei n Datensätzen daher eine Komplexität von <span class="math inline">\(O(n^2)\)</span>. Zur Minimierung der Koplexität werden Blocking-, Indexing- oder Windowing-Techniken genutzt, um den Suchraum einzugrenzen. Dazu werden Datensätz bzgl. einer Mindesähnlichkeit geclustert.</p>
<h2 id="er-workflow">ER-Workflow</h2>
<div class="figure">
<img src="simple_er_workflow.png" alt="ER Workflow" />
<p class="caption">ER Workflow</p>
</div>
<p>Ein vereinfachter Entity Resolution Workflow ist in Abbildung 1 zu sehen. Zunächst werden die Datzensätze Vorverarbeitet, um kleinere Fehler zu korrigieren bzw. Attribute zu vereinheitlichen. Anschließend werden die Datensätze in Blöcke kategorisiert, wodurch der Suchraum beschränkt wird. Die Kategorisierung unterliegt der Annahme, dass Duplikate stets in einem Block landen. Anschließend wird die Ähnlichkeit aller Datensätze eines Blockes geprüft. Das Ergebnis daraus wird abschließend in <em>Matches</em> bzw. <em>Non-Matches</em> klassifiziert.</p>
<h3 id="vorverarbeitung">Vorverarbeitung</h3>
<p>Ziel: Einheitliche Struktur der Datensätze, sowie ein einheitliches Format.</p>
<p>Typische Beispiele:</p>
<ul>
<li>Konvertierung von Zahlen und Datumsformaten,</li>
<li>Korrektur von Tippfehlern</li>
<li>Ersetzen von Abkürzungen</li>
<li>Extratction einzelner Bestandteile (z.B. Name-Vorname, Straße-Hausnummer)</li>
<li>Entfernen von Stoppwörtern, Punkten, Bindestrichen, Kommata, aufeinanderfolgende Whitespaces, Anführungszeichen und Satzzeichen</li>
<li>Konvertierung in Kleinschreibweise</li>
</ul>
<p>Grund: erhöht die <strong>Robustheit</strong> der Ähnlichkeitsberechnung gegenüber kleineren Abweichungen. Zudem kann die Ähnlichkeit zweier Zahlen deutlich <strong>effizienter</strong> bestimmt werden wie bei Zahlenstrings.</p>
<h3 id="blocking">Blocking</h3>
<p>Identifikation von Duplikaten erfolgt durch Paarweises vergleichen. Für zwei Datenquellen A und B sind das <span class="math inline">\(|A|*|B|\)</span> Vergleiche. Bei einer Datenquelle A <span class="math inline">\(\dfrac{1}{2}*|A|*(|A|-1)\)</span> Vergleiche. In beiden Fällen also quadratisch zu den Eingabedaten. Zudem sind die Funktionen der Ähnlichkeitsberechnung selbst rechenintensiv. Auswertung des Kartesischen Produkt ist nachweislich nicht skalierbar! Zur Effizientssteigerung Einsatz von Blocking-Techniken.</p>
<p>Ziel: der Großteil der <em>Non-Matches</em> soll ausgeschlossen werden, ohne dabei <em>Matches</em> auszuschließen. Dadurch wird der Suchraum drastisch reduziert. Übrig bleibt die sog. Kandidatenpaarmenge.</p>
<p>Effizienz eines Blocking-Algorithmus wird durch <em>Reduction Ratio</em>, Reduktion im Vergleich zum Kartesischen Produkt, und <em>Pairs Completeness</em>, Anteil der tatsächlichen Duplikate, beschrieben.</p>
<p>Für das Blocking werden die Datensätze gruppiert oder sortiert. Dafür werden aus den Attributen eines Datensatzes <em>Block-</em> bzw. <em>Sortierschlüssel</em> abgeleitet (= Signatur des Datensatzes).</p>
<h4 id="standard-blocking-1">Standard Blocking</h4>
<p>Jedem Datensatz wird ein Blockschlüssel zugewiesen (=konzeptionelle Schlüssel eines inverted Index). Eine Gruppe von Datensätzen mit selben Blockschlüssel bilden einen Block. Es werden ausschließlich Datensätze eines Blockes miteinander verglichen. Anzahl der Vergleiche (=Kandidatenpaare) hängt von der Größe der Blöcke ab und ist abhängig von der Häufigkeitsverteilung der Blockschlüsselgenerierung. Ebenso ist die <em>Pair Completeness</em> abhängig von der Blockschlüsselgenerierung. Dem kann mit Multi-pass Blocking entgegengewirkt werden, d.h. fü einen Datensatz werden mehrere Schlüssel generiert.</p>
<h4 id="q-gram-indexing">Q-gram Indexing</h4>
<p>Idee: Datensätze mit unterschiedlichen aber ähnlichen Blockschlüsseln miteinander zu vergleichen.</p>
<p>Ein Blockschlüssel wird dazu in eine Liste von q-Grammen überführt. Ein q-Gram ist ein Substring der Länge q des ursprünglichen Blockschlüssels.</p>
<p>Nachteil: hoher Aufwand bei der Berechnung aller möglichen Sublisten. Ein Blockschlüssel mit n Zeichen muss in <span class="math inline">\(k=n-q+1\)</span> q-Gramme zerlegt werden. Insgesamt müssen <span class="math inline">\(\sum_{i=max\{1,[k*t]\}}^{k} {k \choose i}\)</span> Sublisten berechnet werden.</p>
<h4 id="suffix-array-indexing">Suffix Array Indexing</h4>
<p>Leitet ähnlich wie Q-gram Indexing auch mehrere Schlüssel aus dem Blockschlüssel ab. Grundidee ist es alle Suffixe mit einer Mindestlänge von l zu bestimmen. Ein Datensatz mit Blockschlüssellänge n wird in <span class="math inline">\(n-l+1\)</span> Blöcke eingeordnet. Ist <span class="math inline">\(n&lt;l\)</span> wird der Ausgangsschlüssel als einziger Schlüssel verwendet.</p>
<p>Nachteil: im Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze unnötigerweise mehrfach miteinander verglichen werden hoch.</p>
<p>Vorteil: durch die größere Menge an Kandidatenpaaren ist i.Allg. die <em>Pair Completeness</em> höher. Zudem ist der Aufwand der Berechung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer.</p>
<h4 id="sorted-neighborhood-1">Sorted Neighborhood</h4>
<p>Idee: anstatt Datensätze zu partitionieren werden diese anhand eines Sortierschlüssels geordnet. Dadurch werden ähnliche Datensätze &quot;nah beieinander&quot; angeordnet. Die Sortierung erfolge durch einen Schlüssel, welcher für jeden Datensatz generiert wird. Nach der Sortierung aller n Datensätze wird ein Fenster der Größe <span class="math inline">\(w\in[2,n]\)</span> über die sortierte Liste bewegt. Dabei werden jeweils alle Datensätze innerhalb des Fensters miteinander verglichen. Insgesamt git es <span class="math inline">\(n-w+1\)</span> Fensterpositionen und darausfolgend <span class="math inline">\((n-w/2)*(w-1)\)</span> Vergleiche. Für die Komplexität bedeutet dies <span class="math inline">\(O(n)+O(n*\log n)+O(n*w)\)</span>.</p>
<p>Das Verfahren ist besonders bei der Deduplizierung einer Datenquellen geeignet. Bei mehreren Datenquellen müssen diese gemischt werden. Dabei besteht die Gefahr das innerhalb eines Fensters vorrangig Datensätze einer Quelle sind.</p>
<p>Vorteil: Die Anzahl der Kandidatenpaare und damit Vergleiche kann über die Fenstergröße gesteuert werden.</p>
<p>Nachteil: Anfällig gegen Tippfehler, da zwei Duplikate, deren Blockschlüssel sich lediglich im ersten Zeichen unterscheidet, nicht erkannt werden.</p>
<p>Eine verbesserte <em>Pair Completeness</em> ist durch einen Multi-Pass-Ansatz möglich. Dabei werden mehrere Sortierschlüssel pro Datensatz generiert.</p>
<p>Problem: Bei beiden Ansätzen gibt es jedoch das Problem, dass die Fenstergröße w größer als die Anzahl der Datensätze mit dem Sortierschlüssel k sein sollte. Daher muss für n Datensätze mit Sortierschlüssel k und m Datensätzen mit Sortierschlüssel k+1 gelten <span class="math inline">\(w=n+m\)</span>. Nur dardurch kann sichergestellt werden, dass der erste Datensatz aus k auch mit allen Datensätzen in k+1 Verglichen wird. Diese Ausrichtung der Fenstergröße hat jedoch das Problem, das bei selten auftretenden Sortierschüsseln unnötig oft verglichen wird.</p>
<h5 id="hashtable-verfahren">Hashtable Verfahren</h5>
<p>Idee: für jeden Sortierschlüssel wird die Menge aller Datensätze in eine Liste gespeichert. Die Sortierschlüssel selbst werden in einer Hashtabelle abgelegt. Das Fenster wird dann über Hashtabelle geschoben.</p>
<p>Nachteil: Der häufig vorkommenste Schlüssel dominiert die benötigte Zeit zur Ähnlichkeitsberechnung.</p>
<h5 id="sorted-blocks-verfahren">Sorted Blocks Verfahren</h5>
<p>Idee: zunächst wird wie beim klassischen Verfahren sortiert. Dannach werden angrenzende Datensätze in disjunkte Partitionen zerlegt. Dabei soll beispielsweise ein Sortierschlüsselpräfix genuztz werden.</p>
<p>Analog zum Standard Blocking werden alle Datensätze einer Partition miteinander verglichen. Zusätzlich wird ein Fenster fester Größe über die Partitionsgrenze geschoben, dabei wird jeweils das erste Element im Fenster mit allen anderen verglichen. Um zu vermeiden, dass eine Partition dominiert, können größe Partitionen in Subpartitionen geteilt werden.</p>
<h5 id="adaptive-sorted-neighborhood">Adaptive Sorted Neighborhood</h5>
<p>Idee: optimale Fenstergröße bestimmen!</p>
<p>Variante 1: Fenster solange vergrößeren bis erster und letzer Datensatz eine gewisse Mindestähnlichkeit unterschreiten. Nach dem Vergleich aller Datensätze im Fenster wird dieses zurückgesetzt und an die Position des Datensatzes geschoben der zum Abbruch der Vergrößerung geführt hat. Diese Variante ist aufgrund des Aufwandes zur Schlüsselähnlichkeitsberechnung nicht wesentlich effektiver.</p>
<p>Variante 2: Fenster, beginnend mit <span class="math inline">\(w=2\)</span> solange erhöhen, bis die die Anzahl der durchschnittlich gefundenen Duplikate pro Vergleich eine Schwelle unterschreiten.</p>
<h4 id="canopy-clustering">Canopy Clustering</h4>
<p>Idee: Datensäzte mittels einfach zu berechnender Abstandsfunktion in überlappende Cluster partionieren (=Canopies). Datensätze eines Cluster werden miteinander verglichen.</p>
<p>Zur Generierung wird eine Kandidatenliste gebildet, welche inital als allen Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen Clusters gewählte und alle Datensätze innerhalb des Mindestabstandes <span class="math inline">\(d_1\)</span> zugewiesen. Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren Mindestabstandes <span class="math inline">\(d_2 &lt; d_1\)</span> aus der Kandidatenliste entfernt. Dieser Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die <em>Pair Completeness</em> hängt hierbei stark der gewählten Abstandsfunktion ab.</p>
<h4 id="mapping-basiertes-blocking">Mapping-basiertes Blocking</h4>
<p>Erweiterung des FastMap-Algorithmus. Datensätze werden anhand von Blockschlüsseln in einen mehrdimensionalen Euklidischen Raum abgebildet, welcher distanzerhaltend ist. Anschließend ähnlich wie beim Canopy Clustering in überlappende Partitionen teilen.</p>
<p>Die Art und Weise der Schlüsselgenerierung hat entscheidenden Einfluss auf die Qualität der ER-Workflows. Ist das Blocking-Kriterium zu &quot;scharf&quot;, werden Duplikate nicht gefunden, ist es zu &quot;lax&quot;, sind die resultierenden Cluster sehr groß und die Anzahl der Vergleiche steigt drastisch.</p>
<p>Zum Aufstellen von Regeln zur Generierung gibt es zwei Möglichkeite:</p>
<ul>
<li>Domänenexperten</li>
<li>Maschine-Learning Verfahren</li>
</ul>
<h3 id="ähnlichkeitsberechnung">Ähnlichkeitsberechnung</h3>
<p>Zur Berechnung der Ähnlichkeit zweier Datensätze, werden i.Allg. mehrere Ähnlichkeitsfunktionen auf die Attributewerte der Datensätze angewandt. Statt Ähnlichkeiten können auch Abstandsmaße genutzt werden, welche in eine Ähnlichkeit umgewandelt werden müssen.</p>
<p><strong>Field Matching</strong> beschreibt die Berechnung der Ähnlichkeit einzig anhander deren Attributewerte. Dies wird v.a. durch Zeichenkettenähnlichkeitsmaße durchgeführt. Alternative werden Tokenbasierte Maße genutzt. Hierbei wird die Zeichenkette in Token zerlegt und anschließend die Ähnlichkeit der Tokenmengen ermittelt. Verschiedene Maße eignen sich unterschiedlich für bestimmte Attributetypen. Da dies selbst für Domainexperten schwierig herauszufinden ist, werden oft Maschine-Learning Verfahren genutzt, um ein passendes Maß für ein Attributstyp zu finden.</p>
<p><strong>Kontexbasierte Verfahren</strong> nutzen zur Bestimmung der Ähnlichkeit assozierte Datenen. Im einfachsten Fall, werden diese einfach an die Datensätze angehängt. Weiter Beispiele sind XML, durch Ausnutzung der Struktur, Graphen, wo Assoziationen durch Kanten dargestellt werden, oder ontologische Strukturen, beispeilsweise OWL.</p>
<h3 id="klassifizierung">Klassifizierung</h3>
<p>Bestimmung der tatsächlichen Duplikate auf Basis von Ähnlichkeitsvektoren.</p>
<p><strong>Wahrscheinlichkeitsbasierte Verfahren</strong> beispeilsweise Bayes. Nicht mehr State of the Art.</p>
<p><strong>Schwellwertbasierte Verfahren</strong> aggregieren die Komponenten eines Ähnlichkeitsvektors, mittels einer einfachen oder gewichteten Summe. Ein Schwellwert wird genutzt, um Matches von Not-Matches zu unterscheiden. Eine erweiterte Form nutzt zwei Schwellwerte zur Unterscheidung von sicheren Matches und wahrscheinlichen, welche manuell überprüft werden müssen.</p>
<p><strong>Regelbasierte Verfahren</strong> treffen Entscheidungen anhand von Matching-Regeln in konjunktiver oder disjunktiver Normalform. Die Aufstellung der Regeln erfolgt durch Domänenexperten und ist ein aufwändiger, iterativer Prozess. Zusätzlich können Contraints genutzt werden, um domänspezifische Integritätsbedingungen sicherzustellen.</p>
<p><strong>Maschienelle Lernverfahren</strong> sind vorrangig überwachte Lernverfahren, die auf Basis der Ähnlichkeitsvektoren manuell gelabelter Trainingsdaten ein Klassifikationsmodell generieren, welches anschließend auf ungelabelte Ähnlichkeitsvektoren angewandt wird. Populäre Klassifikatoren sind:</p>
<ul>
<li>Entscheidungsbäume und</li>
<li>Support Vector Maschines (SVM)</li>
</ul>
<p>Die Qualität dieser Verfahren hängt von der Menge und Aussagekraft der manuellen Trainingsdaten ab. Zusätztlich können verschiedene Verfahren kombiniert werden, die unabhängig voneinander trainiert worden sind.</p>
<p><strong>Active Learning</strong> sind Verfahren die mit wesentlich weniger Trainingsdaten auskommen und kaum manuellen Aufwand benötigen. Hierbei wird ein initales Klassifikationsmodell aus einer kleinen Menge an Trainingsdaten erstellt, welches iterativ durch Nutzerfeedback verfeinert wird. Dabei müssen die Kandidatenpaare, die am schwierigsten zu klassifizieren waren manuell Klassifiziert werden.</p>
<h3 id="nachverarbeitung">Nachverarbeitung</h3>
<p>Berechnung der transitiven Hülle zur Beseitigung von Kontradiktionen der Menge der klassifizierten Duplikate und Nicht-Duplikate. Dazu ist ein sog. perfektes Match-Ergebnis erforderlicht, auch Gold Standard gennant. Dadurch iest es möglich die True Positives, False Positives, True Negatives und False Negatives zu ermitteln. Daraus werden dann die Kennzahlen <em>Precision</em>, <em>Recall</em> und <em>F-Measure</em> (harmonisches Mittel) abgeleitet.</p>
<h2 id="mapreduce-erweiterungen">MapReduce Erweiterungen</h2>
<p><strong>Lastbalancierung</strong> ohne Lastbalancierung stellt die Bearbeitung des größten Block, die untere Schranke der Bearbeitungszeit dar.</p>
<p><strong>Speicherengpässe</strong> zum Vergleich eines Blocks müssen alle Datensätze im Hauptspeicher gehalten werden. Unter Berücksichtigung aller Prozesse muss die maximale Blockgröße bestimmt und eingehalten werden.</p>
<p><strong>Redundante Ähnlichkeitsbestimmung</strong> entsteht durch Multi-Pass-Blocking Verfahren oder Verfahren mit überlappenden Clustern.</p>
<p><strong>Integration maschineller Lernverfahren</strong> beim Clustering, bei der Auswahl der Ähnlichkeitsfunktion für einen Attributswert und bei der Klassifizierung in Matches und Non-Matches.</p>
<p><strong>Iterative Berechung der transitiven Hülle</strong> zum Evaluieren des Matching Ergebnisses gegenüber dem Gold Standard.</p>
<h1 id="entity-resolution-approach-of-data-stream-management-systems">Entity Resolution Approach of Data Stream Management Systems</h1>
<p>This paper introduces real-time entity stream data processing architechture that performs entity resolution on heterogenous data.</p>
<p>The four components need by a DSMS to process heterogenous streaming data are:</p>
<ul>
<li>Scalablility</li>
<li>Low latency</li>
<li>Common protocol</li>
<li>Standardized format</li>
</ul>
<h2 id="architecture">Architecture</h2>
<p>The architecture consists of 5 main components.</p>
<p><strong>Stream receiver</strong>. Receives input stream from various channels. For example HTTP or ODBC. The received data is transfered to the stream filter.</p>
<p><strong>Stream filter</strong>. Uses a query language, for example CQL or Sparql, to perform a filtering on the incomming data stream. The filtering is done through pattern-based queries. In order to process events and filter them those need to be registered in the system.</p>
<p><strong>Adapter</strong>. Which can read heterogenous stream data with various formats, i.e. XML, JSON, CSV, ... . The read data is then transformed into standardized format. This step is comaparable to the pre-processing in standard entity resolution.</p>
<p><strong>Stream Entity Resolution Engine</strong>. Resolves ambigous entity in real-time against an existing database. Therefore a blocking index is create on the existing data where the blocking key has to be choosen by a domain expert. The comparator performs pairwise comparisons within the choosen block. The match or non-match decision is made through a threshold approach.</p>
<h2 id="conclusion-1">Conclusion</h2>
<p>The system uses the most simplistic ER mechanism a person can think of. For this system to be usable a lot more research has to be put in.</p>
<h1 id="evaluation-of-er-approaches-on-real-world-match-problems">Evaluation of ER approaches on Real-World-Match problems</h1>
<p>This paper evaluates non-learning and learning approaches against real-world matching Problems. Namely DBLP-ACM, DBLP-Scholar, Amazon-GoogleProducts and Abt-Buy (<source>-<target>). It differentiates between 1 or 2 attributes for matching. The Blocking stratagie is the same for all approaches. The evaluation considers both quality in F-Measure and execution time in seconds. Compared framworks are FEBRL, MARLIN, FEVER and a non disclosed comercial one.</p>
<p>The approaches differ only in the parameters <em>function</em> which determines the similarity between to data sets and <em>threshold</em> which identifies a match or non-match. The ML approaches are feed with the same training data which is provided in pairs 20, 50, 100 and 500. To ensure a decent quality of training data a ratio is applied that ensure a minimum of 40% of matches or non-matches.</p>
<h2 id="evaluation-1">Evaluation</h2>
<p><strong>Non-Learning based approaches</strong> show a high effectivness for the bibliographic match task with F-Measure results above 91% for most functions. The e-commerce match task turned out to be much more challanging with F-Measure result only at 62% for (Amazon-GoogleProducts) and 70% for (Abt-Buy). Interessting is that taking a second attribute into consideration turned out to reduce the overall qualtity of the matching result. Regarding execution times FERBL turned out to be much slower than the others. Using a second attribute slowed down the execution time by a factor of 2.</p>
<p><strong>Learning based approaches</strong> achive stable result for easy bibliographic matching with a small training size of 20. More challanging biblographic matching works best with SVM stratagies and combining several matchers reaching an F-Measure about 88-89%. All stratagies have simialar difficulties as non-learnears with e-commerce data. With smaller training sizes than 500 the results are substantially bad. In general performs a 2-Step learning better than 1-Step learning. Regarding the execution times the learning based approaches are significantly worse than the non-learning ones. Even worse is that combined approaches comparing two attributes are at least factor 2 slower than other learning-based approaches. Though the combined approach for learning-based approaches always improves the matching result.</p>
<h2 id="outlook">Outlook</h2>
<p>The good quality of learning based approaches with two attributes comes at expense of significantly higher execution times. Thus it is doubtful if these approaches do scale. Non-learning based single attribute matching outperform learning-based matching in both quality and exection time.</p>
<h1 id="similarity-aware-inverted-indexing-for-real-time-entity-resolution">Similarity Aware Inverted Indexing for Real-time Entity Resolution</h1>
<p>Collection of 3 papers which discuss similarity aware inverted indexing for real-time entity resolution and build upon each other:</p>
<ul>
<li>Towards Scalable Real-Time Entity Resolution using a Similarity-Aware Inverted Index Approach (2008)</li>
<li>Dynamic similarity-aware inverted indexing for real-time entity resolution</li>
</ul>
<ol start="2013" style="list-style-type: decimal">
<li></li>
</ol>
<ul>
<li>A Two Stage Similarity-aware Indexing for Real-time Large Scale Entity Resolution (2013)</li>
</ul>
<h2 id="towards-scalable-real-time-entity-resolution-using-a-similarity-aware-inverted-index-approach">Towards Scalable Real-time Entity Resolution Using a Similarity-aware Inverted Index Approach</h2>
<p>This paper dicusses the challanges for Entity Resolution in Real-time system, i.e. data or event streaming. Therefore the authors propose an Inverted Index algorithm with three variations which improve matching speed but a the cost of matching quality.</p>
<h3 id="requirements-real-time-er">Requirements Real-time ER</h3>
<p>The requirements for a Real-time ER framework is to process a stream of query records a fast a possible against a (large) data set of existing entities. The response time for such a system must be short (ideally sub-second). The result of a query is a ranked list with probabilities that a matched record refers to the same entity.</p>
<h3 id="default-inverted-index-er">Default Inverted Index ER</h3>
<p>Phase 1: Using a static database, which is assumed to be clean (=deduplicated), an Inverted Index is build using a record attribute or attribute encodings i.e. Soundex.</p>
<p>Phase 2: Build index is queried by a stream of records. For each record a list of top randked records is obtained from the index.</p>
<h3 id="similarity-aware-inverted-index">Similarity-Aware Inverted Index</h3>
<p>Main idea is to pre-calculate similarities between attributes in the same block.</p>
<p><strong>Build-time.</strong> Uses three Indexes using actual record values rather then encodings. For each unique attribute being used a blocking key a Record Index (RI) is build which stores all unique attributes and their associated record identifiers. Then the Block Index (BI) with an encoding of the attribute(s) is build. Which stores a list of blocking key values used in RI. Lastly the Similarity Index (SI) is build which has the same key as (RI) and holds the similarity values of each record in the same block. Inserting new records is quite efficient because only similarity values according the to new record have to be calculated.</p>
<p><strong>Query-time</strong>. In the <em>first</em> case a query record attribute is available as blocking key in RI. All records in this index are inserted into the <em>accumulator</em> with similarity 1.0. All other records from the same block are retrieved from SI correlated to RI and inserted with their similarity. In the second case when a query record attribute is not available as blocking key the encoding is calculated. Then for each record in the block the similarity is calculated and records and similarity values are added into the <em>accumulator</em>. The accumulator contains a list of possible matches which can be sorted by their similarity score.</p>
<h4 id="materialized-similarity-aware-inverted-index">Materialized Similarity-Aware Inverted Index</h4>
<p>This is an optimization which aims to improve retrival time at the cost of memory usage. Therefore the similarity values are stored in the Record Index (RI). Further the Record Index now contains all records within the same block with their according similarities. During query time the a record attribute found in the Record Index simply return its list which equals the accumulator.</p>
<h4 id="further-optimization">Further Optimization</h4>
<p>Only insert values into the accumulator that hold to a certain threshold.</p>
<h3 id="summary-1">Summary</h3>
<p>While the Inverted Index approach significantly outperforms Standard Blocking and achieves good results for records with minor, i.e. only one mistake, it fails short of finding duplicates with many (3+) mistakes. Also the required amount of memory for the materialized approach is at least 15 times higher than Standard Blocking.</p>
<h2 id="dynamic-similarity-aware-inverted-indexing-for-real-time-entity-resolution-1">Dynamic similarity-aware inverted indexing for real-time entity resolution</h2>
<p>This paper improves the Similarity-Aware Inverted Index approach from <span class="citation">[@CG:Scalable:08]</span> to allows dynamic additions to the index. Also it investigates the frequency-distribution of data and the effect of reducing the index to hold only frequent records.</p>
<p>In recent years dynamic entity resolution becomes more and more important as many businesses used databases that are not static and change over time. Therefore blocking techniques for Real-time data should allow to change as well. Meaning that if new records are inserted they have to be applied to the blocked/indexed data as well.</p>
<h3 id="dynamic-similarity-aware-inverted-index-approach-1">Dynamic Similarity-Aware Inverted Index approach</h3>
<p>As this approach is an extension to <span class="citation">[@CG:Scalable:08]</span> it has the same components. Namely a Record Index (RI) holding the reference identifiers for each record attribute, the Block Index (BI) storing the unique attribute values assosiated with the RI keys and a Similarity Index (SI) holding the pre-calculated similarities between attribute values in the same block.</p>
<p><strong>Build Time</strong>. Building the Indexes is equal to the original approach. Insert or append to RI then insert into BI or if appending to BI calculate the similarities insert into SI and add new similiarity from all entries of BI into SI.</p>
<p><strong>Query Time</strong>. Works the same as the original approach if the record attribute can be found in the RI. Then insert RI retrived list into accumulator with similarity 1 then insert all entries in SI with their identifiers from RI and similarity values into the accumulator. If record attribute is not available insert it into the indexed before executing the search. Thereby the extending the index with the new record.</p>
<h3 id="frequency-filtered-index-1">Frequency-Filtered Index</h3>
<p>As this index can become quite large it is wise to reduce the amount of indexed records to the most frequent. As many real world datasets follow zips law, meaning there are few frequent attribute and many infrequent ones, the index can be optimized to only index most x% frequent values. This requires an list of the most frequent attritbutes which needs to be provided. I.e. by analysing available dictionaries.</p>
<h3 id="evaluation-2">Evaluation</h3>
<p>On a dataset with 2.5 million records the authors show that time needed to insert a single records is almost constant and thereby independent of the dataset size. Times average around 0.1 msec. The query time is also not effected by the growing index and averages around 0.1 sec.</p>
<p>The result from the frequency filtering show that the amount of memory can almost be halfed if only the top 10% frequent values are index. With 30% there is only a 2% drop in recall.</p>
<h2 id="a-two-stage-similarity-aware-indexing-for-real-time-large-scale-entity-resolution">A Two Stage Similarity-aware Indexing for Real-time Large Scale Entity Resolution</h2>
<p>This paper improves the Similarity-Aware Inverted Index approach from <span class="citation">[@CG:Scalable:08]</span> with Local Sensitive Hashing (LSH). It also uses the idea introduced by <span class="citation">[@RCL+:Dynamic:13]</span> to dynamically extend the inverted index during query time.</p>
<h3 id="local-sensitive-hashing-lsh-with-minhash">Local Sensitive Hashing (LSH) with Minhash</h3>
<p>The idea of LSH is that neighborhood items in a high dimensional space are very likely to stay close after being projected on a lower dimensional space. Minhash is an efficient estimation of two set known as Jaccard similarity. Through LSH it is possible to reduce the search space by grouping similar items. Therefore for each item mulitple hashes are created. Each hash is a signature for the item and all hashes together form a signature matrix. One way to group similar items is to put items with the same hash value into the same bucket. The hope is that dissimilar items are not hashed into the same bucket. To avoid dissimilar items being put into the same bucket a signature matrix is divided in <span class="math inline">\(b\)</span> bands with <span class="math inline">\(r\)</span> rows. For example a signature matrix with 12 rows and 3 row per band is divided into 4 bands. While it is likely that dissimlilar items have common hashes it is very unlikely that they have common bands.</p>
<h3 id="two-stage-approach">Two Stage Approach</h3>
<p><strong>Build Time</strong>. For each MinHash band of each record a bucket is created and the record is put into the buckets. The bucket collection is called LSH Index (LI). Thereby the LI replaces the RI index in DySimII. Building the Block Index (BI) and Similarity Index (SI) is similar to the dynamic approach.</p>
<p><strong>Query Time</strong>. The first step is to obtain all records which are hashed into the same buckets from LI. Then for each record obtained the attributes are compared with the values stored in the similarity index (SI).</p>
<h3 id="summary-2">Summary</h3>
<p>The results show that a query is about 10 times faster then DySimII without obvious memory increase. However building the index takes about 3 times longer. Also the loss in recall is only marginal.</p>
<h1 id="towards-scalable-real-time-entity-resolution-using-a-similarity-aware-inverted-index-approach-1">Towards Scalable Real-time Entity Resolution Using a Similarity-aware Inverted Index Approach</h1>
<p>This paper dicusses the challanges for Entity Resolution in Real-time system, i.e. data or event streaming. Therefore the authors propose an Inverted Index algorithm with three variations which improve matching speed but a the cost of matching quality.</p>
<h2 id="requirements-real-time-er-1">Requirements Real-time ER</h2>
<p>The requirements for a Real-time ER framework is to process a stream of query records a fast a possible against a (large) data set of existing entities. The response time for such a system must be short (ideally sub-second). The result of a query is a ranked list with probabilities that a matched record refers to the same entity.</p>
<h2 id="default-inverted-index-er-1">Default Inverted Index ER</h2>
<p>Phase 1: Using a static database, which is assumed to be clean (=deduplicated), an Inverted Index is build using a record attribute or attribute encodings i.e. Soundex.</p>
<p>Phase 2: Build index is queried by a stream of records. For each record a list of top randked records is obtained from the index.</p>
<h2 id="similarity-aware-inverted-index-1">Similarity-Aware Inverted Index</h2>
<p>Main idea is to pre-calculate similarities between attributes in the same block.</p>
<p><strong>Build-time.</strong> Uses three Indexes using actual record values rather then encodings. For each unique attribute being used a blocking key a Record Index (RI) is build which stores all unique attributes and their associated record identifiers. Then the Block Index (BI) with an encoding of the attribute(s) is build. Which stores a list of blocking key values used in RI. Lastly the Similarity Index (SI) is build which has the same key as (RI) and holds the similarity values of each record in the same block. Inserting new records is quite efficient because only similarity values according the to new record have to be calculated.</p>
<p><strong>Query-time</strong>. In the <em>first</em> case a query record attribute is available as blocking key in RI. All records in this index are inserted into the <em>accumulator</em> with similarity 1.0. All other records from the same block are retrieved from SI correlated to RI and inserted with their similarity. In the second case when a query record attribute is not available as blocking key the encoding is calculated. Then for each record in the block the similarity is calculated and records and similarity values are added into the <em>accumulator</em>. The accumulator contains a list of possible matches which can be sorted by their similarity score.</p>
<h3 id="materialized-similarity-aware-inverted-index-1">Materialized Similarity-Aware Inverted Index</h3>
<p>This is an optimization which aims to improve retrival time at the cost of memory usage. Therefore the similarity values are stored in the Record Index (RI). Further the Record Index now contains all records within the same block with their according similarities. During query time the a record attribute found in the Record Index simply return its list which equals the accumulator.</p>
<h3 id="further-optimization-1">Further Optimization</h3>
<p>Only insert values into the accumulator that hold to a certain threshold.</p>
<h2 id="summary-3">Summary</h2>
<p>While the Inverted Index approach significantly outperforms Standard Blocking and achieves good results for records with minor, i.e. only one mistake, it fails short of finding duplicates with many (3+) mistakes. Also the required amount of memory for the materialized approach is at least 15 times higher than Standard Blocking.</p>
</body>
</html>
