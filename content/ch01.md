# Einleitung

Heutzutage werden von vielen Unternehmen riesige Mengen von Daten gesammelt.
Dabei verlassen sich die Unternehmen in Ihrer täglichen Arbeit auf diese
Informationen, deshalb hat die Qualität der Informationen maßgeblichen Einfluss
auf die Qualität des Produktes. Zur Verbesserung der Qualität der gesammelten
Daten wird in der Praxis eine Datenbereinigung (engl. data cleaning)
durchgeführt. Ein wichtiger Aspekt der Datenbereinigung ist, alle Datensätze zu
finden, welcher derselben Entität entsprechen. Diese Entitäten sind
beispielsweise Personen, Produkte, Veröffentlichungen oder jedes andere Objekt,
dass in der realen Welt existiert. Verfahren die Entitäten finden und verlinken
bzw. zusammenführen, werden üblicherweise Entity Resolution, Duplicate Detection
oder Record Linkage genannt. Die Relevanz dieser Verfahren besteht, da es bei
vielen Daten nicht möglich ist, einzigartige Attribute abzuleiten, anhand derer
Entitäten zugeordnet werden können. Hinzu kommen Verunreinigungen, etwa durch
Rechtschreibfehler. Beispiele wo mehrere Datensätze auf dieselbe Entität
verweisen sind, Patientenakten in einer Krankenhausdatenbank oder ein
Wählerverzeichnis, indem eine Person öfters eingetragen ist. Für den Fall, dass
diese Informationen nicht zusammengeführt oder verlinkt werden können, bestehen
teils schlimme Konsequenzen. Beispielsweise kann ein Arzt aufgrund
unvollständiger Informationen die falsche Entscheidung zur Behandlung eines
Patienten treffen oder ein Wahlberechtigter kann mehrere Stimmen abgeben, was zu
Wahlunstimmigkeiten führt. Weitere Einsatzbereiche sind die Betrugserkennung und
das Inkasso, hierbei sind die Konsequenzen finanzieller Art. Das Thema dieser
Masterarbeit wird für den Problembereich der UNIVERSUM Group in Frankfurt am
Main untersucht. Die UNIVERSUM Group bietet Online-Händlern an, die Einkäufe
ihrer Kunden zu versichern. Das bedeutet, dass nach Ablauf einer
Zahlungsperiode, bei Ausbleiben der Zahlung durch den Kunden, der Betrag durch
die Versicherung gezahlt wird. Die UNIVERSUM Group wird in diesem Fall zum
Gläubiger der Forderung und wird im Inkassoverfahren das Geld vom Kunden
einfordern. Beim Inkasso kommt ein klassisches Entity Resolution zum Einsatz,
bei dem periodisch, etwa jede Nacht, der Datenbestand auf Duplikate geprüft und
diese für den nächsten Tag zusammengeführt werden. Hierbei spielt hauptsächlich
die Qualität eine entscheidende Rolle, da unter anderem gesetzliche Regelungen
eingehalten werden müssen. Die Laufzeit ist lediglich durch die Periode
begrenzt. Viele Unternehmen bieten jedoch zunehmend Onlinedienste an, die die
Anforderungen an Entity Resolution ändern, sodass neben der Qualität auch die
Laufzeit, welche oft im Subsekundenbereich liegen muss, eine entscheidenden
Rolle einnimmt. Dies ist auch der Fall für die Betrugserkennung der UNIVERSUM
Group, beim Austellen einer Vericherung. Das Prüfen, ob eine Versicherung
ausgestellt wird oder nicht, geschieht während des Bestellungsabschlusses, dabei
ist die Entity Resolution nur ein Teil eines Gesamtprüfprozesses. Weshalb diese
entsprechend schnell Ergebnisse liefern muss. Technisch gesehen handelt es bei
solchen Onlinediensten um Stream Processing Systeme, bei welchen Anfragen als
Ereignisse auftauschen und in nahe Echtzeit bearbeitet werden müssen. Das
bedeutet auch, dass der zu prüfende Datenbestand dynamisch ist und sich mit
jedem Ereignis verändert. Zum einen die Laufzeitanforderungen und zum anderen
die dynamischen Daten, stellen Entity Resolution Verfahren für eine große
Herausforderung.

Im Rahmen dieser Thesis werden Entity Resolution Verfahren Event Stream
Processing Systeme betrachetet, analysiert, entwickelt und evaluiert. Unabhängig
von den eingesetzen Verfahren gibt es bei Entity Resolution stets die
Schwierigkeit, die Parameter der Verfahren auf die Domäne der Daten anzupassen.
Beispielsweise unterscheidet sich die Struktur eines Datensatzes mit
Personendaten gravierend von einer Produktdatenbank, weshalb gute Parameter
einer Domäne oft nicht übertragbar sind. Die Anpassung der Konfiguration ist ein
aufwändiger Prozess, der selbst Domänexperten vor eine großer Herausforderung
stellt. Das Hauptaugenmerk dieser Arbeit konzentriert sich deshalb darauf, die
Konfiguration, der Entity Resolution Verfahren für Event Stream Processing
Systeme, auf die Datendomäne möglichst zu automatisieren. Evaluiert werden
hierbei die Laufzeitanforderungen, die Qualität und die Effektivität der
Verfahren bei unterschiedlichen Konfigurationen. Zudem werden die Grenzen der
Übertragbarkeit der Konfiguration intradomän und interdomän getestet.
