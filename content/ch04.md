# Selbstkonfigurierendes System

Ein komplettes Entity Resolution System, wie in Kapitel 2 betrachtet, führt eine
Reihe von Schritten aus, um Datensätze, die auf dieselbe Entität beschreiben,
zu finden. Für ein statisches Entity Resolution System lassen sich diese
Mchritte grob in vier Phasen gliedern. Zunächst die Vorverarbeitung, um
offensichtliche Fehler zu korrigieren, gefolgt von der Blocking-Phase, welche
die Komplexität der Suche reduziert, der Matching Phase mit Paarvergleich und
Klassifizierung in Matches und Non-Matches und abschließend die Nacharbeitung, um
Gruppen von Duplikaten zu bilden. Für dynamische Entity Resolution trennt man
zunächst in Build-Phase, aufbauen eines Index zur effizienten Suche und
Query-Phase, Durchführung der Entity Resolution, auf einem Anfragestrom. In
beiden Phasen wird der Vorverarbeitungsschritt durchgeführt. Die Blocking-Phase
findet hauptsächlich in der Build-Phase statt und wird in der Query-Phase
ergänzt. Der Matching-Phase findet ausschließlich in der Query-Phase statt, da
jeweils nur ein Datensatz betrachtet wird, ist keine Nacharbeitung notwendig, da
das Ergebnis der aus mehreren Datensätzen automatisch die Gruppe von selben
Entitäten bildet.

Im weiteren Kapitel wird ein Entity Resolution System betrachtet, das sein
Hauptaugenmerk auf dynamische Datenquellen und die Anforderungen an die Laufzeit
der Anfragen legt. Neben der Wahl geeigneter Verfahren und Algorithmen zur
Entity Resolution, ist die größte Schwierigkeit, die vielen freien Parameter auf
die Datenquelle anzupassen. Insbesondere das Blocking Schema spielt hierbei eine
entscheidende Rolle, da es überhaupt beeinflusst, ob Entitäten gefunden werden
können. Werden beispielsweise die Parameter des DySimII Blocking Verfahren aus
@sec:dysimII betrachtet, so wird pro Attribute einen Blockschlüssel und eine
Ähnlichkeitsfunktion benötigt. Bei einem Datensatz mit fünf Attributen, sind
dies bereits 10 Parameter. Beim Matching kommen Parameter für die
Ähnlichkeitsfunktionen, beispielsweise die Kosten der Operationen (einfügen,
ersetzen, löschen) bei der Levenshtein Distanz, welche auf ein Attribut
optimiert werden. Bei einem Attribut pro Ähnlichkeitsfunktion und beispielsweise
abweichenden Werten für die Kosten der Einfügeoperation der Levenshtein Distanz,
kommen weitere fünf Parameter hinzu. Ebenfalls in der Matching-Phase wird ein
Klassifikator eingesetzt. Beispielsweise kann ein simpler
Schwellenwertklassifikator mit einer Schwelle genutzt werden. In diese
Konstellation (ohne Vorverarbeitung) mit Blocking Verfahren, verschiedenen
Ähnlichkeitsfunktionen und einem Klassifikator, kommt das ER-System bereits 16
Parameter. Diese Parameter manuell zu bestimmen, ist selbst mit einer cleveren
Strategie, die Parameter auszuprobieren, sehr zeit- und kostenintensiv.
Besonders bei großen Datenmengen kann dieses Trial and Error Verfahren sehr
lange dauern, da das Ausprobieren mehrere Stunden, wenn nicht Tage dauern kann.
Noch schwieriger wird es, wenn keine Ground Truth Daten vorhanden sind. Dadruch
entfällt größtenteils die Möglichkeit die eingestellten Parameter qualitativ zu
überprüfen.

In diesem Kapitel wird deshalb zunächst ein Design für ein sich
selbstkonfigurierendes Entity Resolution System für dynamische Datenquellen zur
Behandlung von Anfrageströme vorgestellt. Das Ziel ist es, mit und ohne Ground
Truth, die Einstellungen möglichst vieler (vorzugsweiser aller) Parameter
bestmöglichst auf die Eingabedatenmenge zu optimieren, ohne das der Benutzer
eine langwierige Trial and Error Phase durchlaufen muss. Damit sollen sich die
erlaubten Parameter auf Laufzeitoptimierungen beschränken, beispielsweise solche
die die Zeit der Selbstkonfiguration, auf Kosten der Qualität, um mehrere
Stunden bzw. Tage reduzieren können. Somit kann das ER System bei Bedarf
schneller in eine produktive Phase gebracht werden, wenn entsprechende
Anforderungen bestehen. Nach einem Überblick des Systems (genannt Engine) werden
die Phasen und die Rolle der Komponenten in den Phasen erklärt. Anschließend
werden in den weiteren Abschnitten die konkreten Komponeten und ihre Algorithmen
beschrieben.

## Engine

```{.plantuml #fig:engine
    caption="Engine des selbstkonfigurierenden Systems. Bestehend aus 8
    Komponenten. In Gelb sind der Ground Truth Generator, der Blocking Scheme
    Lerner, der Similarity Lerner und der Fusion-Lerner, welche für
    das Erlernen der Konfiguration (Fit-Phase) nötig sind. In Grün ist der Indexer, welcher
    aus anhand der gelernten Konfiguration gebaut wird und der Klassifikator.
    In Blau ist der Parser, um Daten einer Datenquelle zu laden und der
    Präprozessor, um die geladenen Daten für Entity Resolution zu manipulieren"}
@startuml
ditaa(--no-shadows, scale=5)
+-----------------------------------Engine--------------------------------------+
|+--------------+-------------------------+--------------------+---------------+|
||cBLU          |cYEL                     |cYEL                |cGRE           ||
|| Parser       | Label Generator         | Similarity Learner | Indexer       ||
||              |                         |                    |               ||
|+--------------+-------------------------+--------------------+---------------+|
||cBLU          |cYEL                     |cYEL                |cGRE           ||
|| Präprozessor | Blocking Scheme Learner | Fusion-Lerner      | Klassifikator ||
||              |                         |                    |               ||
|+--------------+-------------------------+--------------------+---------------+|
\-------------------------------------------------------------------------------/
 +----+               +----+           +----+
 |cBLU| Preprocessing |cYEL| Fit-Phase |cGRE| Build-/Query-Phase
 +----+               +----+           +----+
@enduml
```

Die Engine ist das Herzstück des selbstkonfigurierenden Systems und besteht aus
einzelnen Komponenten, welche wie in einem Steckkastensystem ausgetauscht
werden. Die Komponenten der Engine sind in @fig:engine dargestellt.

* **Parser**. Der Parser liest Datensätze aus einer Datenquelle in eine Menge
  von Tupel.
* **Präprozessor**. Der Präprozessor vorverarbeitet jedes Attribut in einer
  Pipeline, anhand einer Reihe von benutzerdefinierten Operationen, welche
  sequentiell angewendet werden, beispielsweise Rechtschreibprüfung.
* **Label Generator**. Der Label Generator erzeugt bestimmt eine geeignete
  Ground Truth zum Einstellen der Parameter in den folgenden Komponenten.
* **Blocking Schema Lerner**. Der Blocking Schema Lerner erzeugt eine Blocking
  Schema in distributiver Normalform nach [@KM:Unsupervised:13].
* **Similarity Lerner**. Der Similarity Lerner bestimmt für jedes Attribut eine
  geeignete Ähnlichkeitsfunktion.
* **Fusion-Lerner**. Der Fusion-Lerner lernt die besten
  Parameter für den verwendeten Klassifikator und trainiert das
  Klassifikationsmodell.
* **Indexer**. Der Indexer wendet ein Blocking Verfahren auf die Eingabedaten an
  und ermöglicht es diese anzufragen.
* **Klassifikator**. Der Klassifikator sortiert die Kandidatenmenge einer
  Indexanfrage in Matches und Non-Matches.

```{.plantuml #fig:engine_state
caption="Zustandsdiagramm der Engine. Lernen der Konfiguration versetzt die
Engine von unangepasst nach angepasst. Wurde der Index gebaut, ist die Engine im
Zustand gebaut und kann Anfrage entgegennehmen."}

[*] -> unangepasst
unangepasst -> angepasst : fitting
unangepasst -> angepasst : load config
angepasst -> angepasst : save config
angepasst -> gebaut : building
gebaut -> angepasst : re-fitting
gebaut -> gebaut : querying
gebaut -> gebaut : evalute (development)
```

Die Hauptaufgabe der Engine ist es die Interaktionen zwischen den Komponenten zu
steuern. Dazu werden im simpelsten Fall die Daten von einer Komponente zur
nächsten weitergereicht. Zum Teil muss die Engine allerdings zunächst die
Rückgabewerte für die nächste Komponente aufbereitet. Die Engine dient weiterhin
als die Schnittstelle für den Benutzer. Dabei kann der Benutzer die Engine in
drei Zustände versetzten (siehe @fig:engine_state). Eine neu erzeugte Engine ist
*unangepasst* und kann durch das Lernen der Konfiguration (engl. fitting) in den
Zustand *angepasst* wechseln. Alternativ kann der Zustandsübergang durch das
laden einer bereits gelernte Konfiguration durchgeführt werde durchgeführt
werden. Anhand dieser Konfiguration kann der Index auf den initialen Daten
(Datenbestand zum Zeitpunkt des Bauens) gebaut (engl. building) werden. Danach
befindet sich die Engine im Zustand *gebaut*. In diesem Zustand kann die Engine
mit Datensätzen angefragt (engl. querying) werden. Da die Möglichkeit besteht
jede Anfrage in den Datenbestand (den Index) aufzunehmen, liegen nach einer
gewissen Zeit genügend neue Daten vor, sodass sich auf Basis derer auch die
optimale Konfiguartion verändert haben kann. Während des erneuten lernens (engl.
refitting) können weiterhin Anfragen beantwortet werden, allderings ist dies
aufgrund des enormen Ressourcenverbrauch nicht ratsam. Vielmehr empfiehlt sich
die Engine seperat neu zu konfigurieren und die erlernte Konfiguration über
einen Neustart der Engine zu laden. Wenn Komponenten für die Engine entwickelt
werden, ist es notwendig deren Qualität und Effektivität auszuwerten. Weshalb
die Engine im Entwicklungsbetrieb entsprechende Metriken erheben und auswerten
kann. Die Auswertung erfolgt nachdem mindestens eine Anfrage durchgeführt wurde.
Die drei Phasen zum Wechseln der Zustände werden im Folgenden als *Fit-Phase*,
*Build-Phase* und *Query-Phase* bezeichnet. Alle drei Phasen haben den Schritt
der Vorverarbeitung gemeinsam.

### Vorverarbeitung

Die Vorverarbeitung der Daten ist in allen drei Phasen notwendig und macht die
Datensätze robuster gegenüber Missklassifikationen, indem offensichtliche Fehler
korrigiert und eventuelle, für die Identifikation von Entitäten irrelevante,
Varianzen bereinigt weren. In @fig:preprocessing sind die beteiligten
Komponenten Parser und Präprozesser (@fig:engine in blau) mit ihren Aktivitäten
visualisiert. Jede Phase beginnt mit der Auswahl des korrekten Parsers durch die
Engine.

```{.plantuml #fig:preprocessing
    caption="Aktivitätsdiagramm der Vorverarbeitung. Der Parser liest einen
    Datensatz, welcher vom Präprozessor transformieren wird. Der transformierte
    Datensatz wird von der Engine abgespeichert."}
|Engine|
start
:choose parser for Fit-,
Build- or Query-Phase;
|Parser|
:read dataset;
if (is Fit-Phase?) then (yes)
    :assign attribute datatypes;
endif
|Preprozessor|
:transform dataset;
|Engine|
:save transformed dataset;
:proceed with (Fit/Build/Query)-Phase;
```

**Parser**. Der Parser ist eine einfache Komponente, welche Datensätze aus einer
Datenquelle liest und eine Menge von Tupeln $D = {(t1_{a_1}, \dots, t1_{a_n}),
\dots, (tn_{a_1}, \dots, tn_{a_n})}$, mit mindestens einem Tupel, zurückgibt.
Dabei entspricht immer ein Tupel einem Datensatz. Je nach Phase kann die
Datenquelle ein beliebiges Format haben, weshalb für jede Phase ein eigener
Parser bestimmt werden kann. Für *Fit-* und *Build-Phase*, wo große Datenmengen
bearbeitet werden, liest der Parser beispielsweise aus einer CSV-Datei oder
selektiert die Datensätze aus einer Datenbank. Währenddessen in der
*Query-Phase* nur einzelne oder kleine Datenmengen gelesen werden, weshalb der
Parser hier aus einer Message Queue (MQ) Datensätze erhalten kann. Während der
*Fit-Phase* hat der Parser zudem dafür Sorge zu tragen, dass der Engine die
Attribute des Datensatzes, sowie deren Datentypen bekannt gemacht werden. Anhand
dieser des Datentyps können die Komponenten der Fit-Phase optimalere
Konfigurationen bestimmen. Wenn der Parser diese Information nicht bereitstellt,
werden alle Attribute als Zeichenketten behandelt.

**Präprozessor**. Der Präprozessor bzw. die Präprozessor-Pipeline besteht aus
einer Reihe von Funktionen, die nacheinander auf die Attribute aller Datensätze
angewandt werden, welche vom Parser zurückgegeben wurden. Je nach Datentyp des
Attributs wird dabei eine andere Pipeline verwendet. Der Präprozessor wird in
allen drei Phasen verwendet, um einen Datensatz für die Entity Resolution
vorzubereiten und robuster zu machen. Werden vom Benutzer keine Operationen
vorgegeben beschränkt sich die Pipeline auf generische Modifkationen. Die zum
Zeitpunkt dieser Thesis entwickelte Engine ist, zum Zwecke der Vorverarbeitung,
automatisiert lediglich in der Lage Strings in Kleinschreibweise zu konvertiert.
Andere Operationen wie das Entfernen von Stopwörtern (z.B. *und*, *oder*)
benötigen zum einen die Sprache der Attribute, welche zu erkennen durchaus eine
lösbare Aufgabe ist, zum anderen aber auch einen Datenbestand gegen den geprüft
wird. Ein komplexere domänenspezifische Anwendung hierfür ist, beispielsweise
die Überprüfung der postalischen Addresse, welche neben länderspezifischen Daten
benötigt und diesewp auch ständig auf dem aktuellen Stand halten muss. Neben
weiteren Funktionen muss der Benutzer auch die Reihenfolge der Funktionen
vorgeben. Beispielweise zunächst die Rechtspreibprüfung und anschließend die
Konvertierung in Kleinschreibweise, da durch die Rechtschreibprüfung diese
Konvertierung in Teilen wieder aufgehoben werden kann.

Die Menge der transformierten Tupel des Präprozessors wird abschließend von der
Engine im Hierarchical Data Format (HDF) gespeichert. Das HDF-Format ist eine
wissenschaftliches Datenformat, welches entwickelt worden ist, um große
Datenmengen effizient und hierachisch (vergleichbar mit UNIX-Filesystem) zu
persistieren. Um Kapazität zu sparen, werden die Daten mit der
Blosc-Komprimierung verkleinert. Dem Blosc-Komprimator liegt ein hochperformates
Design zugrunde, dass insbesondere den Datenaustausch für den CPU-Cache
optimiert. Obwohl es für Binärdaten entwickelt wurde, werden auch für Strings
gute Ergebnise erzielt.[^2]

[^2]: http://www.blosc.org/benchmarks-blosclz.html

### Fit-Phase

```{.plantuml #fig:fit_phase
    caption="Aktivitätsdiagramm der Fit-Phase. Die Engine kontrolliert den
    Datenfluss zwischen den Komponenten, speichert Konfigurationen und breitet
    Daten für Komponenten auf. Der Label Generator erzeugt die Ground Truth,
    durch welche ein DNF-Blocking Schema vom BS-Lerner erzeugt wird. Auf einer
    durch das Blocking Schema gefilterten Liste werden anschließend die
    Ähnlichkeitsfunktionen bestimmt. Anhand dieser Funktionen können
    Ähnlichkeitsvektoren auf der Ground Truth berechnet werden und vom
    Fusion-Lerner dadurch die Hyperparameter für den Klassifikator bestimmt,
    sowie abschließend das Klassifikationsmodell trainiert werden."}
|Engine|
start
:read transformed dataset;
|Label Generator|
:generate ground truth;
|Engine|
:save ground truth;
|BS-Learner|
:predict DNF Blocking Scheme;
|Engine|
:save DNF Blocking Scheme;
:filter ground truth;
|Sim-Learner|
:predict similarity functions;
|Engine|
:save similarity functions;
:calculate ground truth
similarity vectors;
|HP-Optimizer|
:predict hyperparameters;
:train model;
|Engine|
:save model;
stop
```

In der Fit-Phase nimmt die Engine die Konfiguration des Systems vor. Eine
Konfiguration ist ein Tupel $(GT, BS, S, M)$ bestehend aus der Grund Truth, dem
Blocking Schema, den Ähnlichkeitsfunktionen und dem Klassikationsmodell. Die
Ground Truth $GT = (P, N)$ ist ebenfalls ein Tupel, dass sich in die Menge der
positive Datensatzpaare, die tatsächlichen Matches (true positives), sowie die
Menge der negativen Datensatzpaare, die tatsächlichen Non-Matches (true
negatives) teilt. Ein Datensatzensatzpaar ist definiert als $p = (t_j, t_k), j
\neq k$, wobei $j$ und $k$ beliebige Tupel sein können. Weiterhin gilt $\forall
p \in P, p \notin N$ und umgekehrt $\forall p \in N, p \notin P$. Das Blocking
Schema entspricht der Definition aus @sec:scheme, $BS = (term_1 \land \dots
\land term_j) \lor \dots \lor (term_k \land \dots \land term_n)$. Die
Ähnlichkeitsfunktionen werden als Menge von Tupeln angegeben $S = {(f_1, sim),
\dots, (f_m, sim)}$, wobei $f$ das Datenfeld eines Datensatztupels und $m$ die
Anzahl der Attribute in einem Datensatztupel ist. Die Ähnlichkeitsfuntion $sim$
ist eine von $r$ möglichen Ähnlichkeitsfunktionen ${sim_1, \dots, sim_r}$, die
durch die Engine bereitgestellt werden. Das Klassifikationsmodell $M$ ist
spezifisch für den eingesetzten Klassifikator und entspricht bespielsweise einem
trainierten Entscheidungsbaum.

In @fig:engine sind die Komponenten für die Fit-Phase in gelb hervorgehoben. Bei
großen Datensätzen kann diese Phase sehr lange dauern, weshalb die Engine die
einzelnen Konfigurationen der Komponenten direkt sichert. Dadurch kann die
Fit-Phase im Falle eines Abbruchs, z.B. durch einen Systemneustart, fortgesetzt
werden und nur die unterbrochene Komponente muss wiederholt werden. Wurde die
Fit-Phase abgeschlossen, ist es möglich die ermittelte Konfiguration einzulesen.
Wodurch die Fit-Phase übersprungen wird. @fig:fit_phase zeigt das
Aktivitätsdiagramm der Fit-Phase, ohne existierende Konfiguratation.

**Label Generator**. Der Label Generator erzeugt, die für später in der
Fit-Phase folgenden Komponenten, nötige Ground Truth in Form von klassifizierten
Matches und Non-Matches. Dazu bekommt er von der Engine die, in der
Vorverarbeitung transformierten, Trainingsdaten und bildet Datensatzpaare
(@fig:fit_phase). Dabei gibt es zwei Ausprägungen. In der ersten Ausprägung
erhält der Label Generator eine Ground Truth für den gegebenen Datensatz.
Aufgrund der Verteilung von Matches und Non-Matches, die fast immer ein
deutliches Ungleichgewicht zugunsten der Non-Matches aufweist, werden für alle
öffentlich verfügbaren Datensätze mit Ground Truth, lediglich die Matches
angegeben. Dementsprechend sind alle Datensatzpaare, welche nicht in den Matches
der Ground Truht enthalten sind, als Non-Matches zu interpretieren. Über der
riesigen Menge an Non-Matches führt der Label Generator ein Sampling aus um eine
repräsentative Stichprobe zu erhalten. In der zweiten Ausprägung stehen dem
Label Generator keine vorklassifizierten Matches zur Verfügung. Weshalb die
Ground Truth vollständig automatisiert bestimmt werden muss (siehe
@sec:lblgen_nogt). Ein Label Generator kann beide Ausprägungen implementieren.
Falls nur die erste Ausprägung implementiert ist, kann die Engine, ohne
existierende Ground Truth, die Fit-Phase nicht durchführen. Sollte nur die
zweite Ausprägung vorhanden sein, werden die vorklassifizierten Matches
ignoriert.

Für die Entwicklung von Komponenten besitzt die Engine die Möglichkeit Metriken
zu messen und diese auszuwerten. Diese liefern ein wichtiges Indiz, wie gut eine
Komponente funktioniert. Des Weiteren ist es dadurch möglich das Zusammenspiel
der Komponenten untereinander zu bewerten, indem beispielsweise eine alternative
Komponente eingesetzt wird, um die Auswirkungen der neuen Komponente in den
Metriken zu überprüft werden. Von den Metriken, welche in @sec:measurements
beschrieben wurden, kann die Engine für das Blocking die Pairs Completeness,
Pairs Quality und Reduction Ratio aufzeichnen, sowie für den Klassifikator
Recall, Precision und F-measure messen. Des Weiteren werden die Daten zum
Zeichnen eines F-measure Graphen und einer Precision-Recall Kurve
bereitgestellt. Darüber hinaus kann die Engine messen, wie lange einzelne
Operationen einer Komponente benötigen. Beispielsweise wird gemessen, wie lange
es dauert einen Datensatz in den Index einzufügen bzw. zu einem Anfragedatensatz
die Kandidatenliste zu erhalten. Dadurch kann die Performanz, beispielsweise in
Anfragen pro Sekunde auf einer Testhardware angegeben werden. Alle Metriken
werden während der Query-Phase erhoben und können nach jeder Anfrage erhalten
werden.

**Blocking Schema Lerner**. Der Blocking Schema Lerner ermittelt ein Blocking
Schema in disjunktiver Normalform nach [@KM:Unsupervised:13], welches in
@sec:blk_scheme vorgestellt wurde. Dafür benötigt der Blocking Schema Lerner die
vorverarbeiteten Trainingsdaten, sowie die Ground Truth Daten des Label
Generators. Das Lernen eines Schemas ist ein rechenintensive, weshalb der
Benutzer über Laufzeitparameter die maximale Anzahl an Konjunktionen innerhalb
der Ausdrücke, sowie die maximale Anzahl an Disjunktionen von Ausdrücken angeben
kann.

**Similarity Lerner**. Der Similarity Lerner bestimmt aus den zur Verfügung
stehenden Ähnlichkeitsfunktionen für jedes Attribut die beste. Dazu werden die
Datensatzpaare der Grund Truth bewertet. Bevor die Ground Truth an den
Similarity Lerner übergeben wird, filtert die Engine die Datensatzpaare heraus,
welche nicht vom Blocking Schema erfasst werden. Das sind diejenigen
Datensatzpaare, welche in folgenden den Verarbeitungsschritten aus
Effizientgründen nicht weiter betrachtet werden.

**Fusion-Lerner**. Der Fusion-Lerner ermittelt für einen gegebenen Klassifikator
die Parameter, die das Modell mit der besten F-measure erzeugen. Bevor der
Fusion-Lerner aufgerufen werden kann, erzeugt die Engine für jedes gefilterte
Ground Truth Paar, anhand der Ähnlichkeitsfunktionen des Similarity Lerners,
einen Ähnlichkeitsvektor pro Paar. Die Ähnlichkeitsvektoren werden dann vom
Fusion-Lerner genutzt, um ein Modell mit gegebenen Parametern zu trainieren. Die
Parameter, die zur Optimierung in Frage kommen, müssen von der
Klassifikatorkomponente bereitgestellt werden, beispielsweise die maximale Tiefe
eines DecisionTree. Um einen optimalen Klassifikator für die Eingabedaten zu
bekommen ist es abgesehen von der Parameterliste möglich eine Liste von
verschiedenen Klassifikatoren anzugeben, beispielsweise einen DecisionTree und
eine SVM.


### Build-Phase

```{.plantuml #fig:build_phase width=60%
    caption="Aktivitätsdiagramm der Build-Phase. Der liest alle vorverarbeiteten
    Datensätze einer initalen Datensatzes ein und fügt diese seinem Index hinzu."}
|Engine|
start
:read transformed dataset;
repeat
    :get record from dataset;
    |Indexer|
    :insert record into index;
repeat while (more records?)
|Engine|
:save index;
stop
```

Die Build-Phase dient der Vorbearbeitung der Daten, bevor das
selbstkonfigurierte ER-System seinen Betrieb aufnehmen kann. Dazu wird der
komplette Datenbestand, in welchem Entitäten gesucht werden sollen, betrachtet.
Nachdem diese durch die Vorverarbeitung gelaufen sind, wird auf den Daten ein
Blocking-Verfahren durchgeführt. Der **Indexer** ist ein Blocking Mechanismus,
der zum einen mit dynamischen Daten umgehen können muss und zum anderen das
Blocking anhand des DNF-Blocking Schemas durchführt. In @fig:build_phase wird
die Build-Phase erläutert. Die Engine liest zunächst alle vorverarbeiteten
Datensätze ein. Anschließend werden die Datensätze einzel dem Indexer übergeben,
welcher diese zu seinem Index hinzufügt. Dabei besteht die Möglichkeit, dass der
Index während des Einfügens anhand der gelernten Ähnlichkeitsfunktionen
bestimmte Ähnlichkeiten vorausberechnet. Das Bauen des Index kann einige
Minuten, eventuell sogar Stunden, dauern. Deshalb wird der Index nach dem Bauen
gespeichert. Im Falle eines Neustarts der Engine müssen dann nur die Datensätze
eingefügt werden, welche während der letzten Query-Phase hinzugekommen sind.

### Query-Phase

```{.plantuml #fig:query_phase width=90%
    caption="Aktivitätsdiagramm der Query-Phase. Zunächst werden der
    transformierte Datensatz vom Präprozessor gelesen. Danach werden Datensätze
    einzeln entnommen und dem Indexer übergeben. Dieser liefert eine
    Kandidatenliste. Jeder Kandidat wird vom Klassifikator in Match bzw.
    Non-Match klassifiziert. Matches werden von der Engine gespeichert und
    Non-Matches verworfen. Am Schluss wird das Ergebnis aller Anfragen dem
    Benutzer übergeben."}
|Engine|
start
:read transformed dataset;
while (more queries?) is (yes)
    :get query record from dataset;
    |Indexer|
    :query candidates from index;
    |Engine|
    while (more candidates?) is (yes)
        |Engine|
        :get candidate record from candidate list;
        |Klassifier|
        :predict candidate class;
        |Engine|
        if (is candidate a match?) then (yes)
            |Engine|
            :save candidate as match;
        else (no)
            |Engine|
            :discard candidate;
        endif
    endwhile (no)
endwhile (no)
|Engine|
:pass results to user;
stop
```

In der Query-Phase (siehe @fig:query_phase) erhält die Engine von einem
Query-Parser eine Menge von Anfragedatensätzen. Nachdem diese vorverarbeitet
wurden, wird jeder Datensatz einzeln dem Indexer übergeben. Dieser erzeugt für
den übergebenen Datensatz eine Kandidatenmenge möglicher Matches. Diese
Kandidaten werden dem Klassifikator übergeben. Das Modell des **Klassifikators**
wurde während der *Fit-Phase* von dem Fusion-Lerner trainiert und kann nun in
der *Query-Phase* genutzt werden, um die Kandidaten in Matches und Non-Matches
zu klassifizieren. Das Ergebnis der Klassifikation speichert die Engine
zwischen, bis alle Datensätze verarbeitet wurden. Abschließend werden die
gesammelten Ergebnisse an den Benutzer übergeben.

## Komponenten

In diesem Abschnitt werden die konkreten Komponenten betrachtet und ihre
Funktionalität Algoritmis beschrieben. Die Details der Implementierung der
Komponenten werden in @sec:impl vorgenommen.

### Label Generator {#sec:weaklbl}

Für den Label Generator wurden beide Ausprägungen (mit und ohne Ground Truth)
umgesetzt. Zunächst wird die Variante ohne Ground Truth beschrieben und
anschließend die Variante mit Ground Truth, welche eine Modifikation der ersten
Ausprägung ist.

#### Ohne Ground Truth {#sec:lblgen_nogt}

Der Label Generator ohne Ground Truth implementiert den WeakLabel Algorithmus
vom Kejriwal & Miranker [@KM:Unsupervised:13] zur Erzeugung von schwachen
Labels. Der Algorithmus definiert zwei Schwellen, die obere Schwelle $ut$ und
die untere Schwelle $lt$. Damit vor allem die erzeugten Ground Truth
Non-Matches, der klassifizierten Paare, nicht beliebig groß werden, kann der
Anwender festlegen, wie viele Matches $max_p$ bzw. Non-Matches Paare $max_n$
maximal erzeugt werden sollen. Die vier Abschnitte Algorithmuses sind in
Algorithmus \ref{alg:weaklabels}). Zunächst wird die TF/IDF Statistik über $D$
erzeugt (Zeile \ref{alg:wl:tfidf}), welche für einen späteren Paarvergleich
benötigt wird.

```texalgo
#alg:weaklabels WeakTrainingSet w\textbackslash o Ground Truth
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Upper Threshold: $ut$
  \item Lower Threshold: $lt$
  \item Blocking Window Size: $c$
  \item Maximum Duplicate Pairs: $max_p$
  \item Maximum Non-Duplicate Pairs: $max_n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item A set of positive samples: $P$
  \item A set of negative samples: $N$
  \end{itemize}
}
\Statex
\State Initialize set $P = ()$, set $N = ()$
\State Initialize set of tuple pairs $C = ()$
\State Generate TFIDF statistics of $D$\label{alg:wl:tfidf}
\For{fields $f \in D$} \label{alg:wl:tks}
    \For{records $r \in D$}
        \State Tokenize $r_f$ into $BKV_f$
        \State Block $r$ on generate tokens for field $f$
    \EndFor
\EndFor \label{alg:wl:tke}
\For{block $B$ generate in previous step}\label{alg:wk:cs}
    \State  Slide a window of size c over tupels in $B$
    \StatexIndent[1] Generate all possible pairs within window and
    \StatexIndent[1] add to $C$
\EndFor \label{alg:wk:ce}
\For{pairs $(t_1, t_2) \in C$}
    \State Compute TFIDF similarity $sim$ of $(t_1, t_2)$
    \If{$sim \geq ut$}\label{alg:wk:ps}
        \If{$|P| < d$}
            \State add $(t_1, t_2)$ to $P$
        \ElsIf{$sim >$ lowest $sim$ in $P$}\label{alg:wk:pcks}
            \State Replace pair with lowest $sim$ in $P$ with $(t_1, t_2)$
        \EndIf\label{alg:wk:pcke}
    \EndIf\label{alg:wk:pe}
    \If{$sim < lt$}\label{alg:wk:ns}
        \If{$|N| < nd$}
            \State add $(t_1, t_2)$ to $N$
        \ElsIf{$sim >$ lowest $sim$ in $N$}\label{alg:wk:ncks}
            \State Replace pair with lowest $sim$ in $N$ with $(t_1, t_2)$
        \EndIf\label{alg:wk:ncke}
    \EndIf\label{alg:wk:ne}
\EndFor\label{alg:wk:sime}
\State Return $P$ and $N$

#alg:labels WeakTrainingSet with Ground Truth
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Ground Truth $GT$
  \item Blocking Window Size: $c$
  \end{itemize}
}
\Statex \Statex \Statex
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item A set of positive samples: $P$
  \item A set of negative samples: $N$
  \end{itemize}
}
\Statex {\color{gray}
\State Initialize set $P = ()$, set $N = ()$\label{alg:wk:inits}
\State Initialize set of tuple pairs $C = ()$
\State Generate TFIDF statistics of $D$
\For{fields $f \in D$}
    \For{records $r \in D$}
        \State Tokenize $r_f$ into $BKV_f$
        \State Block $r$ on generate tokens for field $f$
    \EndFor
\EndFor
\For{block $B$ generate in previous step}
    \State  Slide a window of size c over tupels in $B$
    \StatexIndent[1] Generate all possible pairs within window and
    \StatexIndent[1] add to $C$
\EndFor }\label{alg:wk:inite}
\For{pairs $(t_1, t_2) \in GT$}\label{alg:wk:gts}
    \State Add $(t_1, t_2)$ to $P$
    \If{$p \in C$}
        \State Remove $p$ from $C$
    \EndIf
\EndFor\label{alg:wk:gte}
\State Initialize dictionary $M = \{\}$
\For{pairs $(t_1, t_2) \in C$}\label{alg:wk:tfidfs}
    \State Compute TFIDF similarity $sim$ of $(t_1, t_2)$
    \State $M[(t_1, t_2)] = sim$
\EndFor\label{alg:wk:tfidfe}
\State Calculate probability distribution over $M$\label{alg:wk:hist}
\State $max_n = |GT| * 5$
\For{$i = 1$ \textbf{to} $max_n$}\label{alg:wk:ss}
    \State Choose a pair $p$ from $M$ based on probability distribution\label{alg:wk:smpl}
    \State Add $p$ to $N$
\EndFor\label{alg:wk:se}
\State \color{gray} Return $P$ and $N$
```

Anschließend wird ein Blocking der Daten per Standard Blocking und Sorted
Neighborhood durchgeführt. Jeder Datensatz wird (pro Attribute) in Token zerlegt
(Zeilen \ref{alg:wl:tks}-\ref{alg:wl:tke}). Anhand der Token wird das Standard
Blocking durchgeführt, wobei jeder Datensatz in mehreren Blöcken vertreten sein
kann. Die Menge von Blöcken sind jeweils nach Attributen gruppiert, sodass Token
unterschiedlicher Attribute nicht als Blockschüssel des selben Blockes genutzt
werden, da die Datensätze in den entsprecheden Blocken, trotz übereinstimmenden
Token, vermutlich wenig Ähnlichkeit haben. Danach wird die Kandidatenmenge $C$
möglicher Matches bzw. Non-Matches anhand der gruppierten Datensätze generiert
(Zeile \ref{alg:wk:cs}-\ref{alg:wk:ce}). Um innerhalb der Blöcke den
Paarvergleichsaufwand zu reduzieren, wird die Sorted Neighborhood verwendet und
ein Fenster der Größe $c$ über den Block geschoben. Nachdem das Fenster über
jeden Block geschoben wurde, steht die Menge möglicher Kandidatenpaare fest.
Diese Paare werden nun mit der TF/IDF-Ähnlichkeit $sim$ (aus Cohen
[@Coh:WHIRL:00]) verglichen (Zeilen \ref{alg:wk:ps}-\ref{alg:wk:pe}). Aufgrund
der, über die kompletten Daten erfassen, TF/IDF Statistik beträgt die
Komplexität des Vergleiches $O(1)$, da lediglich die entsprechenden TF und IDF
Werte, der Datensätze des Paares, nachgeschlagen werden müssen. Ist die
Ähnlichkeit $sim \geq ut$ wird das Paar als Match klassifiziert und zu $P$
hinzugefügt (Zeilen \ref{alg:wk:ps}-\ref{alg:wk:pe}). Analog, ist $sim < lt$
wird das Paar als Non-Match klassifiziert und zu $N$ hinzugefügt (Zeilen
\ref{alg:wk:ps}-\ref{alg:wk:pe}). Von allen Matches werden jeweils die $max_p$
mit der höchsten Ähnlichkeit $sim$ ausgewählt (Zeilen
\ref{alg:wk:pcks}-\ref{alg:wk:pcke}). Analog werden ebenfalls die $max_n$
Non-Matches mit der höchsten Ähnlichkeit $sim$ gewählt (Zeilen
\ref{alg:wk:ncks}-\ref{alg:wk:ncke}). Bei den Non-Matches soll dadurch
verhindern, dass lediglich Paare mit $sim \approx 0.0$ ausgewählt werden, da
diese für gewöhnlich zu niedrigen Klassifikationsraten führen. Die
Gesamtkomplexität des Algorithmus ist $O(n + nm + nm)$, welcher sich in die
Erzeugung der TF/IDF Statistik ($O(n)$) , die Erzeugung der Blöcke über $m$
Attribute ($O(nm)$) und die Erzeugung der Kandidatenpaare $O(nm)$ gliedert.
Kritisch bei diesem Algorithmus zu betrachten ist, dass ein Großteil der
Datensatzpaare, aufgrund der Lücke zwischen den Schwellen, nicht für die Ground
Truth ausgewählt werden kann. @fig:weaklbl_problem illustriert diese Lücke
zwischen einer unteren Schwelle bei 0.1 und oberen Schwelle bei 0.7. Für alle
Paare $p$ gilt, wenn $lt \leq sim(p) < ut$, dann folgt $p \notin P \cup N$.
Dadurch ist die generierte Repräsentation eines Datensatzes, durch die Ground
Truth, nicht sonderlich repräsentativ, da viele aussaugekräftige Paare
ausgeschlossen sind. Inwiefern diese Einschränkung eine Konfiguration
beeinflusst wird in Kapitel 6 überprüft.

```{.a2s #fig:weaklbl_problem
    caption="Darstellung der Lücke zwischen oberer und unterer Schwelle, des
    Algorithmuses des Label Generator ohne Ground Truth (GT), innerhalb welcher
    Paare nicht für die Ground Truth ausgewählt werden können."}
     lt=0.1                             ut=0.7
       |                                  |
       |    Pairs not available for GT    |
       |<-------------------------------->|
       :                                  :
 0     v                                  v                 1
 <---------------------------------------------------------->
                      TF/IDF Similarity
```

#### Mit Ground Truth

Der Algorithmus eines Label Generators mit Ground Truth, welcher in Algorithmus
\ref{alg:labels} beschrieben ist, modifiziert den Algorithmus
\ref{alg:weaklabels}. Gegeben ist die Ground Truth in Form von Matches. Davon
ausgehen soll eine representative Menge von Non-Matches aus dem Datensatz $D$
selektiert werden. Die ersten drei Schritte das generieren der TF/IDF Statistik,
das Blocken durch die Tokens und das erzeugen der Kandidatenmenge $C$ (Zeilen
\ref{alg:wk:inits}-\ref{alg:wk:inite} in grau), sind identisch zum
ursprünglichen Algorithmus. Nachdem die Kandidatenmenge erzeugt wurde, werden
zunächst alle Ground Truth Paare nach $P$ übernommen (Zeilen
\ref{alg:wk:gts}-\ref{alg:wk:gte}). Zusätzlich werden alle Matches aus der
Kandidatenmenge $C$ entfernt, sodass diese ausschließlich Non-Matches
beinhaltet. Anschließend werden ebenfalls die TF/IDF-Ähnlichkeit der Paare in
$C$ ermittelt und in $M$ zwischengespeichert (Zeilen
\ref{alg:wk:tfidfs}-\ref{alg:wk:tfidfe}). Anhand dieser wird die
Wahrscheinlichkeitsverteilung der Ähnlichkeiten in $M$, beispielsweise durch ein
Histogramm ermittelt (Zeile \ref{alg:wk:hist}). In @fig:label_sampling ist
beispielhaft eine Verteilung von Non-Matches (`-` Symbol) dargestellt. Die
X-Achse gibt den Ähnlichkeitswert der Paare an. Die Häufung, auf der Y-Achse,
illustiert, wie viele Paare eine entsprechende Ähnlichkeit haben. Eine Häufung
ist vor allen Dingen im unteren Ähnlichkeitsbereich zu erwarten. Durchaus
möglich sind allerdings auch größere Anhäufungen im mittleren Bereich, da durch
das Blocking der Großteil der Paare mit Ähnlichkeit 0.0 ausgeschlossen worden
ist. In Zeile \ref{alg:wk:smpl} werden nun Non-Matches, anhand der
Wahrscheinlichkeitsverteilung, zufällig aus $M$ gezogen, sodass Paare innerhalb
einer großen Anhäufung (z.B. unterer Bereich) häufiger ausgewählt werden, als
Paare in kleinen Anhäufungen (z.B. oberer Bereich). Damit wird erreicht, dass
die Menge, der für die Ground Truth gewählten Non-Matches, möglichst
representativ ist. Die Ziehen wird $max_n$ mal wiederholt bzw. solange bis keine
Paare mehr übrig sind (Zeilen \ref{alg:wk:ss}-\ref{alg:wk:se}). Zum Schluss wird
analog zum ursprünglichen Algorithmus die Grund Truth bestehend aus $P$ und $N$
an die Engine übergeben.

```{.a2s #fig:label_sampling
    caption="Beispielhalfte Verteilung von Non-Matches ('-' Symbol) auf der über
    die Ähnlichkeit (X-Achse) zwischen 0 und 1. Wie viele Non-Matches einen
    bestimmten Ähnlichkeitswert haben, wird durch die Häufung (Y-Achse)
    dargestellt."}
 Cluster-Size

      ^
      |     -
      |    - -
      |   - - -                         -
      |   -  - -                       - -
      |  - -  - -                      - - -
      |  - -  -  -            -      -   -  -
      |  - -  -   - -       -  -     -  - -  -            -
      |  - - -  - - -      -  -     -  -   - -           - - -
      | - - -  - -  - - -  - - -    - -  - - -  - -      - - -  - -                  - -
      +--------------------------------------------------------------------------------------------------->
      0                                            0.5                                                    1

                                              TF/IDF Similarity
```

### Blocking Schema Generator

#### Blocks DNF Generator

Der Blocks DNF Generator erzeugt ein Blocking Schema in disjunktiver Normalform,
welche für dynamische Entity Resolution Verfahren geeignet ist. Um die
Hintergründe des Blocks DNF Generator zu verstehen wird zunächst das Verfahren
von Kejriwal & Miranker [@KM:Unsupervised:13] untersucht, wessen Authoren das
DNF Blocking Schema entwickelt haben. Kejriwal & Miranker erzeugen Blocking
Schema, indem Ausdrücke über die Fisher-Score bewertet werden. Die berechnete
Fisher-Score drückt für einen Ausdruck $t$, in Abhängigkeit der Groud Truth $P$
und $N$, die Blockschlüsselabdeckung (engl. blocking key coverage) aus. Laut
Ramadan & Christen [@RC:Unsupervised:15] führt eine hohe Schlüsselabdeckung
dazu, dass viele true positive Matches in einem Block gruppiert werden, während
die Anzahl an negativen Matches gering gehalten wird. Durch dieses Verfahren
wird für einen Entity Resolution Workflow hochqualitative Blöcke erzeugt. Für
dynamische Verfahren, die Anfragen im Subsekundenbereich antworten und möglichst
gleiche Latenzen haben sollten, ist aufgrund der niedrigen Dichte von Duplikaten
in Datensätzen die Fisher-Score als Bewertungskriterum ungeeignet. Zwar werden
für die Duplikate Blöcke generiert, die es erlauben (möglichst) alle zur Anfrage
passenden Enitäten schnell und präzise zu erhalten, allerdings ist der Großteil
aller Anfragen ergebnislos. Ergebnislos in diesem Zusammenhang bedeutet, dass es
zu einer Anfrage keinen Datensatz gibt, der der selben Entität entspricht. Das
Problem ist, dass die Fisher-Score für diese Datensätze keine Aussage trifft,
weshalb die generierten Blöcke, in welchen sich keine Matches befinden, zum Teil
sehr groß werden können. Aufgrund der Menge dieser Anfragen, wird die
Effektivtät des ER Systems dramatisch reduziert.

```texalgo
#alg:dnf LearnOptimalBS($S$, $k$, $d$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Set of specific blocking predicates: $S$
  \item Maximum conjunctions per term: $k$
  \item Maximum disjunctions of terms: $d$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme: $BS$
  \end{itemize}
}
\Statex
\State Initialize set of blocking scheme candidates $BS_C = ()$
\State Initialize set of terms $T = ()$
\For{$i = 1$ \textbf{to} $k$}\label{alg:bs:dis}
    \State Generate combination of $S$ with cardinality $i$ and
    \StatexIndent[1] add to $T$
\EndFor\label{alg:bs:die}
\For{term $t \in T$}
    \State $fmeasure, y_{true}, y_{pred} = evaluateTerm(t)$\Comment
    \If{$fmeasure = thres$}
        \State Remove $t$ from $T$\label{alg:bs:del}
    \EndIf
\EndFor
\For{$i = 1$ \textbf{to} $d$}\label{alg:ref:cms}
    \State Generate combination $C$ of $T$ with cardinality $i$
    \State Add $C$ to $BS_C$\label{alg:ref:cme}
\EndFor
\For{Blocking scheme $bs \in BS_C$}
    \State Initialize array $y_{true}$ with length $|P \cup N|$
    \State Initialize array $y_{pred}$ with length $|P \cup N|$
    \For{term $t \in bs$}
        \State $y_{true} \lor t.y_{true}$\label{alg:bs:ort}
        \State $y_{pred} \lor t.y_{pred}$\label{alg:bs:ory}
    \EndFor
    \State Score $s = fmeasure(y_{true}, y_{pred})$\label{alg:bs:fm}
    \If{$s > top_score$}
        \State $BS = bs$
    \EndIf
\EndFor
\State return $BS$

#alg:dnf_eval EvaluateTerm($t$, $D$, $IX$, $P$, $N$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term: $t$
  \item Dataset: $D$
  \item Indexer: $IX$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item F-measure: $f$
  \item Labels: $y_{true}$
  \item Predictions: $y_{pred}$
  \end{itemize}
}
\Statex
\State Build Index $I$ over $D$ using Indexer $IX$ and Term $t$\label{alg:bs:idx}
\State Initialize set of pairs $C = ()$
\State Initialize $TP = 0, FP = 0, FN = 0$
\For{block $b \in I$}\label{alg:bs:feb}
    \State Generate pair combinations $pc$ for records in $b$\label{alg:bs:cmb}
    \StatexIndent[1] and add them to $C$
    \State According to $P$ and $N$ calculate number of true\label{alg:bs:cnt}
    \StatexIndent[1] positives, false positives and false negatives and
    \StatexIndent[1] sum them up with $TP$, $FP$ and $FN$.
\EndFor
\State Calculate F-measure $f$ according to $TP, FN, FP$\label{alg:bs:fmes}
\State Initialize array $y_{true}$ and $y_{pred}$ with length $|P \cup N|$\label{alg:bs:yarr}
\For{$i = 1$ \textbf{to} $|P|$}\label{alg:bs:p}
    \State Pair $p = P[i]$
    \State $y_{true} = True$\label{alg:bs:p1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:p2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:p3}
    \EndIf
\EndFor
\For{$i = |P| + 1$ \textbf{to} $|N|$}\label{alg:bs:n}
    \State Pair $p = N[i]$
    \State $y_{true} = False$\label{alg:bs:n1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:n2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:n3}
    \EndIf
\EndFor
\State return $f, y_{true}, y_{pred}$
```

Der Blocks DNF Generator erzeugt daher ein Blocking Schema, unter
Berücksichtigung aller erzeugten Blöcke. Dazu werden wie bei Kejriwal & Miranker
Kombinationen von konjugierten Ausdrücken, bespielsweise für einen
Publikationsdatensatz (`EnthältGemeinsamenToken`, `Autor`) $\land$
(`ExakteÜbereinstimmung`, `Konferenz`), gebildet. Diese werden neben der
Qualität, auch in ihrer Effektivität untersucht. Die Evaluierung eines Ausdrucks
ist in Algorithmus \ref{alg:dnf_eval} beschrieben. Zur Bewertung eines Ausdrucks
$t$ benötigt der Algorithmus den Datensatz $D$, sowie die Matches $P$ und die
Non-Matches $N$, der Ground Truth. Zudem wird, der durch die Engine
instanziierte Indexer $IX$ benötigt. Der Ausdruck $t$ wird dem Indexer $IX$
als Blocking Schema übergeben, woraus dieser seinen Index $I$ über $D$ zu baut
(Zeile \ref{alg:bs:idx}). Durch die Betrachtung des konkreten Index, kann der
Block DNF Generator eine DNF erzeugen, die auf den Indexer zugeschnitten ist.
Als nächstes werden alle generierten Blöcke aus $I$ betrachtet (Zeile
\ref{alg:bs:feb}). Der Indexer muss dazu eine entsprechende Blockliste
bereitstellen. Ein Block ist in diesem Zusammenhang, nicht zwangsweise ein
Gruppierung, welche über einen Blockschlüssel, gebildet wurde, sonderen jegliche
Anhäufungen von Datensätze, die bei einer Anfrage zusammen als Kandidatenmenge
ausgewählt werden. Die Details hierzu werden in @sec:indexer erläutert. Für
jeden Block werden zunächst die Paarkombinationen[^3], aller dem Block
zugehöriger Datensätze, ermittelt. Diese werden zu der Menge aller
Paarkombinationen aller Blöcke $C$ hinzugefügt (Zeile \ref{alg:bs:cmb}). Des
Weiteren wird für einen Block $b$ die Anzahl der Paare in den
Klassifikationskategorien

* true positives, wenn ein Paar $p \in b$ und $p \in P$
* false positives, wenn ein Paar $p \in b$ und $p \in N$
* true negatives, wenn ein Paar $p \notin b$ und $p \in P$

über die Grund Truth ermittelt und in $TP$, $FP$ und $FN$ aufsummiert (Zeile
\ref{alg:bs:cnt}). Anhand dieser $TP$, $FP$, $FN$ wird das F-measure zur
Bewertung des Ausdrucks $t$ bestimmt (Zeile \ref{alg:bs:fmes}). Bei der späteren
Disjunktion von Ausdrücken kann dieses F-measure allderdings nur zur Vorauswahl
der infragekommenden Audrücke genutzt werden, da sich die F-measure Werte
verschiedener Ausdrücke aus unterschiedlichen Paarkombinationen berechnen. Damit
die Ausdrücke effizient dijunktiert werden können, wird die Paarkombination auf
die Ground Truth abgebildet. Dazu werden zwei Arrays $y_{true}$ und $y_{pred}$
mit der Länge $|P \cup N|$ erzeugt (Zeile \ref{alg:bs:yarr}). Diese können beim
Zusammenfügen der DNF einfach verodert und daraus das F-measure bestimmt werden.
Für die Abbildung auf die Ground Truth müssen alles Matches $P$ (Zeile
\ref{alg:bs:p}) und alle Non-Matches (Zeile \ref{alg:bs:n}) betrachtet werden.
$y_{true}$ gibt an, welcher Klasse ein Paar $p$ angehört: Match, wenn $p \in P$
(Zeile \ref{alg:bs:p1}) oder Non-Match, wenn $p \in N$ (Zeile \ref{alg:bs:n1}).
$y_{pred}$ gibt an, ob ein Datensatzpaar einen gemeinsamen Block in $I$ hat
$y_{pred}[p] = True$ (Zeilen \ref{alg:bs:p2},\ref{alg:bs:n2}) oder nicht
$y_{pred}[p] = False$ (Zeilen \ref{alg:bs:p3},\ref{alg:bs:n3}). Die Werte für
F-measure, $y_{true}$ und $y_{pred}$ werden zum Schluss an den Aufrufer
zurückgegeben.

[^3]: 2-Tupel der Datensätze ohne festgelegte Reihenfolge

Der Algorithmus zur Bestimmung des optimalen Blocking Schemas ist in Algorithmus
\ref{alg:dnf} dargestellt. Der erste Parameter sind die spezifischen
Blockingprädikate $S$. Diese werden von der Engine je nach Attributstyp
bestimmt. Da die maximale Konjunktion der Blockingprädikate bzw. die maximale
Disjunktion potentiell unendlich groß ist, werden diese über die Parameter $k$
für die Konjunktionen und $d$ für die Disjunktionen begrenzt. Ein weiterer Grund
die Konjunktionen bzw. Disjunktionen nicht beliebig zu erhöhen ist, dass der
Indexer deutlich komplexere Blockschlüssel erzeugen muss. Denn für jedes
spezifische Blockingprädikat muss, beim Erzeugen der Blockschlüssel eines
Datensatzes, die Prädikatsfunktion aufgerufen werden. Demnach nimmt die
Effizienz der Blockschlüsselgenerierung ab, je mehr Konjunktionen bzw.
Disjunktionen ein Blocking Schema hat. Am Anfang werden die konjugierten
Ausdrücke erzeugt, indem alle Kombinationen der Menge spezifischer
Blockingprädikte $S$ bis zur Länge $k$ der maximalen Konjunktionen berechnet
werden (Zeilen \ref{alg:bs:dis}-\ref{alg:bs:die}), somit ist $$T =
\{\{\text{2-Tupel von S}\}, \{\text{3-Tupel von S}\}, \dots, \{\text{k-Tupel von
S}\}\}.$$ Anschließend wird jeder Ausdruck $t$ durch oben erklärten Algorithmus
\ref{alg:dnf_eval} evaluiert. Ist der F-measure Wert $f$ für $t$ kleiner einer
Schranke $thres$, indiziert dies einen ungeeigneten Block und der Ausdruck wird
entfernt (Zeile \ref{alg:bs:del}). Aus den noch in $T$ vorhandenen Ausdrücken,
werden durch Disjunktion bis zur Länge $d$, mögliche Kandidaten eines Blocking
Schemas generiert (Zeilen \ref{alg:ref:cms}-\ref{alg:ref:cme}). Ein Blocking
Schema wird ausgewählt, indem die Arrays $y_{pred}$ und $y_{true}$ der Ausdrücke
in jedem potentiellen Blocking Schema verodert werden (Zeilen
\ref{alg:bs:ort}-\ref{alg:bs:ory}) und daraus das F-measure berechnet wird
(Zeile \ref{alg:bs:fm}). Das potentielle Blocking Schema mit dem höchsten
F-measure wird abschließend ausgewählt und an die Engine zurückgegeben.

\TODO{Zu Implementierung hinzufügen} Laut Kejriwal & Miranker
[@KM:Unsupervised:13] bieten Werte $>3$ keine wesentliche Verbesserung.

\TODO{Ungeeignete Prädikatsfunktionen in Implementierung} Beispielsweise Monge &
Elkan, ungeordnete Q-Gramme.

### Indexer {#sec:indexer}

#### MDySimII

Der Multi-Dynamic Similarity-Aware Inverted Index (MDySimII) ist eine Anpassung
des ursprünglichen DySimII (vgl. @sec:dysimII) Verfahrens. Dabei steht das Multi
für einen Multi-pass Ansatz, bei welchem zu einem Datensatz mehrere
Blockschlüssel erzeugt werden. Dadurch können die Datensätze in mehrere Blöcke
eingeordnet werden, womit sich die Wahrscheinlichkeit erhöht ein Duplikat zu
finden. Die Idee des DySimII ist es, die Ähnlichkeit aller Attribute
vorauszuberechnen. Dafür wird für jedes Attribut eine Enkodierungsfunktion
genutzt, die einen Blockschlüssel erzeugt. Anhand dieser kann die Ähnlichkeit
für die Kandidatenmenge jedes Attributes vorausberechnet werden. Im Wesentlichen
handelt es sich bei DySimII bereits um eine Multi-pass Verfahren, da für jedes
Attribut der Datensatz in einen Block gruppiert wird. Bei $k$ Attributen wird
der Datensatz folglich zu $k$ Blöcken hinzugefügt. Die Generierung von
Blockschlüsseln ist jedoch stark eingeschränkt, da ein Schlüssel ausschließlich
auf einem Attribut berechnet wird. Darüber hinaus darf maximal ein Schlüssel pro
Attribut generiert werden. Aufgrund dessen wird der DySimII Ansatz nicht als
Multi-pass Ansatz betrachtet und es ist demzufolge auch nicht möglich das DNF
Blocking Schema nutzen. Der MDySimII hingegen implementiert einen DNF
kompatiblen Multi-pass Ansatz. Das bedeutet, dass Blockschlüssel über mehrere
Attribute generiert werden können, dass nicht jedes Attribut zur Generierung
genutzt werden muss und zudem kann eine Funktion mehrere Blockschlüssel erzeugen
darf. Da das DNF Blocking Schema allerdings nicht mehr garantiert, dass jedes
Attribut berücksichtigt wird, entfällt eine vollständige Vorausberechung der
Ähnlichkeiten. Im schlimmsten Fall besteht das Blocking Schema nur aus einem
einstelligen Ausdruck, wodurch das Blocking lediglich auf einem Attribut
durchgeführt. Dementsprechend muss bei einer Anfrage, zwischen den im Blocking
Schema nicht enthaltenen Attributen, immer die Ähnlichkeit ermittelt werden und
kann nicht im Index nachgeschlagen werden. Als Ergebnis einer Anfrage wurde vom
der ursprünglichen DySimII Implementierung eine Kandidatenliste mit der
aufsummierten Gesamtwahrscheinlichkeit der Kandidaten zurückgegeben. Der
MDySimII hingegen gibt für jeden Kandidaten einen Vektor mit den einzelnen
Attributsähnlichkeiten zurück, sodass für den Klassifikator möglich wenig
Informationen verloren gehen.

```{.texalgo #alg:bkvs caption="BlockingKeyValues(t, r)"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term from Blocking Schema $t$
  \item Record $r$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Key Values: $BKV$
  \end{itemize}
}
\Statex
\State Initialize list $BKV = []$
\For{specific blocking predicate $p \in t$}\label{alg:bkv:fep}
  \State field $f = p.field$
  \State attribute keys $ak = p.predicate(r.f)$\label{alg:bkv:pre}
  \If{$BKV$ is empty}
    \State $BKV = ak$\label{alg:bkv:f1}
  \Else
    \State $bkv$ = []
    \While{$BKV$ is not empty}\label{alg:bkv:as}
      \State Take key $x$ from BKV
      \For{key $y$ in $ak$}
        \State $xy = concatenate(x ,y)$\label{alg:bkv:cc}
        \State Append $xy$ to $BKV$
      \EndFor
    \EndWhile\label{alg:bkv:ae}
    \State $BKV = bkv$\label{alg:bkv:rpl}
  \EndIf
\EndFor
\State return $BKV$
```

Durch die erläuterten Anpassungen für den MDySimII ergeben einige Änderungen im
Ablauf. Zunächst wird in Algoritmus \ref{alg:bkvs} gezeigt, wie die
Blockschlüssel des DNF Blocking Schema erzeugt werden. Jeder Ausdruck $t$ der
Disjunktion erzeugt eine unabhängige Menge von Blockschlüsseln für einen
Datensatz $r$. Aus diesem Grund erzeugt der Algorithmus Blockschlüssel für die
Konjunktion von spezifischen Blockingprädikaten in $t$ über $r$. Die
Blockingprädikate werden dazu der Reihe nach betrachtet (Zeile
\ref{alg:bkv:fep}). Anhand des spezifische Blockingprädikat $p$ werden alle
Schlüssel für das verknüpfte Attribut $p.field$ generiert, beispielsweise
liefert das Prädikat `GemeinsamerToken` alle durch Leerzeichen getrennte Token
des Attributes $r.f$ (Zeile \ref{alg:bkv:pre}). Ist die BKV Liste zu diesem
Zeitpunkt leer, wird die Schlüsselliste $ak$ als $BKV$ Liste übernommen.
Existieren in $BKV$ allerdings schon Schlüssel wird zunächst eine temporäre
Liste $bkv$ erzeugt. Anschließend werden aus $BKV$ solange Schlüssel entnommen,
bis diese leer ist (Zeile \ref{alg:bkv:as}). Für jeden entnommenen Schlüssel $x$
werden $|ak|$ neue Schlüssel erzeugt. Dazu wird der neue Schlüssel $xy$
gebildet, indem ein Schlüssel $y$ aus $ak$ mit $x$ konkateniert wird (Zeile
\ref{alg:bkv:cc}). Alle Schlüssel $xy$ werden zu zu $bkv$ hinzugefügt. Wenn die
$BKV$ Liste leer ist, wird die temporäre Liste $bkv$ nach $BKV$ übernommen
(Zeile \ref{alg:bkv:rpl}). Nachdem alle Prädikate bearbeitet wurden, wird die
Liste der Blockschlüssel $BKV$ zurückgegeben.

```texalgo
#alg:mdysim_insert MDySimII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $RI, BI, SI$
  \end{itemize}
}
\Statex
\For{fields $f \in F$}\label{alg:dII:is}
  \State Initialize $RI_f = \{\}$, $BI_f = \{\}$, $SI_f = \{\}$
\EndFor\label{alg:dII:ie}
\For{records $r \in D$}
  \For{fields $f \in F$}
    \State insert $r.id$ into $RI_f[r.f]$
  \EndFor
  \For{terms $t \in BS$}
    \State $bkvs = BlockingKeyValues(t, r)$
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \State Append r.f to $BI_f[bkv]$
        \State Initialize inverted index list $si = ()$
        \For{attribute $a \in bi$}
          \If{$a \notin SI_f$}
            \State $sim = S_f(r.f, a)$
            \State Append $(r.f, sim)$ to $SI_f[a]$
            \State Append $(a, sim)$ to $si$
          \EndIf
        \EndFor
        \State $SI_f[r.f] = si$
      \EndFor
    \EndFor
  \EndFor
\EndFor

#alg:mdysimIII_insert MDySimIII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $BI, SI$
  \end{itemize}
}
\Statex
\For{fields $f \in F$}
  \State Initialize $BI_f = \{\}$, $SI_f = \{\}$
\EndFor
\For{records $r \in D$}
  \For{terms $t \in BS$}
    \State $bkvs = blocking\_key\_values(t, r)$
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \State $bi = BI_f[bkv]$
        \State Add r.f to $bi$
        \State Add r.id to $bi[r.f]$
        \State $BI_f[bkv] = bi$
        \State Initialize inverted index list $si = ()$
        \For{attribute $a \in bi$}
          \If{$a \notin SI_f$}
            \State $sim = S_f(r.f, a)$
            \State Append $(r.f, sim)$ to $SI_f[a]$
            \State Append $(a, sim)$ to $si$
          \EndIf
        \EndFor
        \State $SI_f[r.f] = si$
      \EndFor
    \EndFor
  \EndFor
\EndFor
```

In Algorithmus \ref{alg:mdysim_insert} ist die *Build-Phase* des MDySimII
beschrieben. Zunächst werden die Index Datenstrukturen Record Identifier Index
(RI), welcher alle Attribute speichert und diese ihren Datensätzen zuordnet,
Block Index (BI), welcher Attribute anhand der Blockschlüssel gruppiert und
Similarity Index (SI), welcher Attributsähnlichkeiten zwischen Attributen im
gleichen Block hält, für jedes in der DNF vorkommende Attribut erzeugt (Zeilen
\ref{alg:dII:is}-\ref{alg:dII:ie}). Die im Algorithmus genannten $fields$
beschreiben die Positionen der Attribute im Tupel eines Datensatz $r = [a_1,
\cdot, a_n]$. Anschließend werden alle Datensätze in $D$ nacheinander eingefügt.
Dazu wird zunächst der Datensatzidentifier, in alle oben erzeuge RIs, unter dem
entsprechenden Attribute eingefügt (Zeilen 5-7). Anschließend werden alle
Ausdrücke der DNF einzeln betrachtet. Zu jedem Ausdruck werden für den Datensatz
$r$ die Blockschlüsselwerte erzeugt (Zeile 9). Für jedes abgedeckte Attribut des
aktuellen Ausdrucks, werden die Attributswerte von $r$ unter dem Blockschlüssel
$bkv$ in den entsprechenden Block Index eingefügt (Zeilen 12-14). Nachdem ein
Attribut in einen Block eingefügt wurde, werden analog zum ursprünglichen
DySimII Verfahren die Ähnlichkeiten der Attribute ermittelt und gegenseitig im
Similarity Index ergänzt bzw. eingefügt (Zeilen 17-24).

Die *Query-Phase* des MDySimII wird in Algorithmus \ref{alg:mdysim_query}
beschrieben. Zunächst wird die Kandidatenliste $C$ initialisiert (Zeile
1). Dann wird der Anfragedatensatz nach Algorithmus \ref{alg:mdysim_insert} in
den Index eingefügt (Zeile 2). Sollte der Datensatz sich schon im Index befinden
wird dieser Schritt übersprungen. Für jedes Attribut aus $q$, das vom
dem Blocking Schema $BS$ abgedeckt wird, wird zunächst alle Kandidaten
mit übereinstimmenden Attribut aus dem RI geholt und mit dem Ähnlichkeitswert
1.0 in die Kandidatenliste übernommen (Zeilen 4-7). Anschließend werden für das
Attribute alle Kandidaten im selben Block mit ihrer vorberechneten Ähnlichkeit
aus dem SI geholt. Die Identifier der Attribute im selben Block werden über den
RI aufgelöst und anschließend mit dem Ähnlichkeitswert aus dem SI in die
Kandidatenlist übernommen (Zeilen 8-14).

#### MDySimIII

```{.plantuml #fig:mdysim_example
    caption="Ein MDySimIII-Index, welcher aus der Tabelle links erzeugt worden
    ist. Die Beispieldatensätze enthalten das Namensattribut eines Restaurants
    und die Art der Küche. B ist der Block Index, welcher aus dem Blocking
    Schema (CommonToken, Kitchen) erzeugt wurde. SI ist der Similarity Index."}
@startuml
ditaa(--no-shadows, --no-separation, scale=5)
+-----------+------+---------+    Blocking Scheme (CommonToken, Kitchen)
| Record ID | Name | Kitchen |
+-----------+------+---------+   BI  +-------+           +------+      SI  +------+     +------+
                               (name)| pizza |           | vegi |    (name)| tony |     | toni |
+-----------+------+---------+       +---+---+           +--+---+          +--+---+     +--+---+
|     r1    | tony | pizza   |           |                  |                 |            |
+-----------+------+---------+           v     RI           v     RI          v            v
|     r2    | toni | pizza   |       +---+---+  +--+--+  +--+---+  +--+    +--+---+---+ +--+---+---+
+-----------+------+---------+       | tony  +->+r1|r3|  | tony +->|r3|    | toni |0.9| | tony |0.9|
|     r3    | tony | pizza   |       +-------+  +--+--+  +------+  +--+    +------+---+ +------+---+
|           |      | vegi    |       | toni  +->|r2|
+-----------+------+---------+       +-------+  +--+
@enduml
```

Der MDySimIII ist eine Modifikation des MDySimII Algorithmus. Dieser Indexer
wurde dahingehend verändert, dass die Länge der Kandidatenliste durch ein
geeignetes Blocking Schema kontrolliert werden kann. Dazu wird der globale
Record Index (RI) in den Block Index (BI) verschoben. Der Similarity Index
bleibt unverändert und verlinkt Attribute eines Blockes mit ihren Ähnlichkeiten.
Der Block Index für das Attribute `Name` in @fig:mdysim_example, wurde durch das
spezifische Blockingprädikat `(CommonToken, Kitchen)` erzeugt und besteht daher
aus den Blockschlüsseln `pizza` und `vegi`. Innerhalb der Blöcke sind die
Attribute, welche zum entsprechenden Blockschlüssel gehören. Unterhalb der
Attribute befindet sich nun der Record Index, welcher alle Datensätze mit dem
selben Attribute und dem selben Blockschlüssel auflistet. Gibt es,
beispielsweise eine 1 mio. Datensätze mit dem Namen `tony`, kann durch ein
cleveres Blocking Schema, der **BI** so aufgebaut werden, dass es keinen
übermächtigen **RI** Einträg mit 1 mio. Datensatzidentifiern gibt.

```texalgo
#alg:mdysim_query DNF Similarity-Aware Index - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index
\For{fields $f \in F$}
    \State $ri = RI_f[q.f]$
    \For{$r.id \in ri$}
        \State $M[(r.id, f)] = 1.0$
    \EndFor
    \State $si = SI_f[q.f]$
    \For{$(r.f, sim) \in si$}
        \State $ri = RI_f[r.f]$
        \For{$r.id \in ri$}
            \State $M[(r.id, f)] = sim$
        \EndFor
    \EndFor
\EndFor

#alg:mdysimIII_query DNF Similarity-Aware Index - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index
\For{terms $t \in BS$}
  \State $bkvs = blocking\_key\_values(t, r)$
  \For{$bkv \in bkvs$}
    \For{fields $f \in t.fields$}
      \State $bi = BI_f[bkv]$
      \For{attribute $r.f \in bi$}
        \If{$q.f = r.f$}
          \For{identifier $r.id \in bi[q.f]$}
            \State $M[(r.id, f)] = 1.0$
          \EndFor
        \Else
          \State $si = SI_f[q.f]$
          \State $sim = si[r.f]$
          \For{identifier $r.id \in bi[q.f]$}
            \State $M[(r.id, f)] = sim$
          \EndFor
        \EndIf
      \EndFor
    \EndFor
  \EndFor
\EndFor
```
