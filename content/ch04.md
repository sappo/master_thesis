# Design eines selbstkonfigurierenden Systems \label{chap:design}

In diesem Kapitel werden zunächst die Prozesse eines sich selbst
konfigurierenden Entity Resolution System für dynamische Datenquellen, zur
Bearbeitung von Anfrageströme, beschrieben. Anschließend wird die zu lernende
Konfiguration formal definiert. Danach werden die Komponenten des Systems
vorgestellt und deren Schnittstellen beschrieben. In Bezug auf den jeweiligen
Prozess werden dann die Details zu den Komponenten erläutert, falls diese noch
nicht aus Kapitel 3 bekannt sind.

## Prozesssicht

```{.a2s #fig:engine_state
    caption="Zustandsdiagramm des selbstkonfigurierenden System. Lernen der
    Konfiguration versetzt das System von unangepasst nach angepasst. Wurde der
    Index gebaut, ist das System im Zustand gebaut und kann Anfrage
    entgegennehmen."}
        .------------.              .--------.  building    .-------.
        | not fitted |   fitting    | fitted +------------->| built |
 ●----->+------------+------------->+--------+  re-fitting  +-------+
        |            | load config  |        |<-------------+       |
        '------------'              '------+-'              '-----+-'
                                      ^    |                  ^   |
                                      '----'                  '---'
                                   save config              querying/
                                                            evalute
```

Die in Kapitel 2 und 3 vorgestellten Verfahren, für dynamische Entity
Resolution, trennen zwischen Build-Phase und Query-Phase. Diese Trennung wird
auch für das selbstkonfigurierende System aufrecht erhalten. Zusätzlich gibt es
noch eine weitere Phase zum Erlernen der Konfiguration, im Folgenden als
*Fit-Phase* bezeichnet. Je nach Phase befindet sich bzw. wechselt das System in
einen von drei Zuständen, die in @fig:engine_state dargestellt sind. Ein neu
erzeugtes System ist *unangepasst* und kann durch das Lernen der Konfiguration
(engl. fitting) in den Zustand *angepasst* wechseln. Alternativ kann der
Zustandsübergang durch das Laden einer bereits gelernte Konfiguration
durchgeführt werden. Anhand dieser Konfiguration kann der Index auf einem
initialen Datenbestand gebaut (engl. building) werden. Danach befindet sich das
System im Zustand *gebaut*. In diesem Zustand kann die eigentliche Entity
Resolution, durch stellen von Anfragen aus einem Datenstrom (engl. querying),
durchgeführt werden. Da die Möglichkeit besteht jede Anfrage in den Datenbestand
(den Index) aufzunehmen, liegen nach einer gewissen Zeit genügend neue Daten
vor, sodass sich auf Basis derer auch die optimale Konfiguartion verändert haben
kann. Während des erneuten Lernens (engl. refitting) können weiterhin Anfragen
beantwortet werden. Wenn der Lernvorgang abgeschlossen ist, muss der Index
erneut gebaut werden, bevor das System wieder anfragen entgegen nehmen kann.
Wenn Komponenten für das System entwickelt werden, ist es notwendig deren
Qualität und Effektivität auszuwerten. Weshalb das System im Entwicklungsbetrieb
entsprechende Metriken erheben und auswerten kann. Die Auswertung erfolgt
nachdem mindestens eine Anfrage durchgeführt wurde.

In der Fit-Phase nimmt die Engine die Konfiguration des Systems vor. Eine
Konfiguration ist ein Tupel $(BS, S, M)$ bestehend aus dem Blocking Schema, den
Ähnlichkeitsfunktionen und dem Klassikationsmodell. Die Teilkonfigurationen
werden anhand einer Ground Truth erlernt, diese ist definert als $GT = (P, N)$
und ist ebenfalls ein Tupel, dass sich in die Menge der positive Datensatzpaare,
die tatsächlichen Matches (true positives), sowie die Menge der negativen
Datensatzpaare, die tatsächlichen Non-Matches (true negatives) teilt. Ein
Datensatz wird definiert als $n$-Tupel, wobei $n$ die Anzahl der Attribute ist.
Ein Tupel hat die Form $t = (a_1, a_2, \dots, a_n)$. Ein Attribut hat eine feste
Position im Tupel, die als Feld oder Datenfeld $f$ bezeichnet wird. Ein
Datensatzensatzpaar ist definiert als $p = (t_j, t_k), j \neq k$, wobei $j$ und
$k$ zwei beliebige Tupel desselben Datensatzes sein können. Weiterhin gilt
$\forall p \in P, p \notin N$ und umgekehrt $\forall p \in N, p \notin P$. Das
Blocking Schema entspricht der Definition aus @sec:blk_scheme, $BS = (term_1
\land \dots \land term_j) \lor \dots \lor (term_k \land \dots \land term_n)$.
Eine Ähnlichkeitsfunktion wird während des Lernens der Konfiguration mit einem
Attribut verknüpft. Die Menge der gelernten Ähnlichkeitsfunktionen werden
entsprechend als Tupel angegeben $S = {(f_1, sim), \dots, (f_n, sim)}$. Die
Ähnlichkeitsfuntion $sim$ ist eine von $m$ möglichen Ähnlichkeitsfunktionen
${sim_1, \dots, sim_m}$, die vom System implementiert wurden. Das
Klassifikationsmodell $M$ ist spezifisch für den eingesetzten Klassifikator und
entspricht, bespielsweise einem trainierten Entscheidungsbaum.

## Komponentenmodell

```{.plantuml #fig:engine
    caption="Komponentenmodell des selbstkonfigurierenden Systems. Bestehend aus
    dem Ground Truth Generator, dem Blocking Scheme Lerner, dem Similarity
    Lerner und dem Fusion-Lerner, welche für das Erlernen der Konfiguration
    (Fit-Phase) nötig sind, dem Indexer, welcher anhand der gelernten
    Konfiguration gebaut wird und dem Klassifikator. Der Parser, um Daten einer
    Datenquelle zu laden und der Präprozessor, um die geladenen Daten für Entity
    Resolution zu manipulieren"}
skinparam componentStyle uml2
component "Engine" {
    [Parser] - D
    D )-- [Preprocessor]
    [Preprocessor] - PD

    package "Fit-Phase" {
        PD )-down- [Blocking Scheme Learner]
        PD )-down- [Label Generator]
        PD )-down- [Similarity Learner]
        PD )-down- [Fusion-Learner]
        PD )-down- [Label Filter]
        GT )-up- [Label Filter]
        GT  -up- [Label Generator]
        FGT -up- [Label Filter]
        FGT )-up- [Blocking Scheme Learner]
        FGT )-up- [Fusion-Learner]
        FGT )-up- [Similarity Learner]
        BS -up- [Blocking Scheme Learner]
        BS )-up- [Label Filter]
        BS )-up- [Similarity Learner]
        BS )-up- [Fusion-Learner]
        M -up- [Fusion-Learner]
        S -up- [Similarity Learner]
        S )-up- [Fusion-Learner]
        B )-up- [Blocking Scheme Learner]
        PG )-up- [Fusion-Learner]
    }

    package "Build-/Query-Phase" {
        B -up- [Indexer]
        BS )-down- [Indexer]
        PD )-down- [Indexer]
        S )-down- [Indexer]

        C -left- [Indexer]
        PG -down- [Klassifier]
        M )-down- [Klassifier]
        C )-down- [Klassifier]
        R -left- [Klassifier]
    }
}
```

Die Engine ist das Herzstück des selbstkonfigurierenden Systems und besteht aus
einzelnen Komponenten, die bei Schnittstellenkompatibilität beliebig
ausgetauscht werden. Die Komponenten und Schnittstellen der Engine sind in
@fig:engine dargestellt.

* **Parser**. Der Parser liest Datensätze aus einer Datenquelle bietet eine
  Menge von Tupeln $D$ an.
* **Präprozessor**. Der Präprozessor vorverarbeitet jedes Attribut jedes
  Datensatzes aus $D$ in einer Pipeline, anhand einer Reihe von
  benutzerdefinierten Operationen, welche sequentiell angewendet werden,
  beispielsweise Rechtschreibprüfung, das Ergebnis ist die vorverarbeitete Menge
  an Tupeln $PD$.
* **Label Generator**. Der Label Generator erzeugt eine geeignete Ground Truth
  $GT$ zum Einstellen der Parameter in den folgenden Komponenten. Er konsumiert
  dazu die vorverbeiteten Tupel $PD$.
* **Blocking Schema Lerner**. Der Blocking Schema Lerner erzeugt eine Blocking
  Schema $BS$ in distributiver Normalform nach [@KM:Unsupervised:13]. Zur
  Bewertung eines Ausdrucks, konsumiert er die generierten Blöcke $B$ eines
  Indexers.
* **Label Filter**. Der Label Filter ist fester Bestandteil der Engine und
  modifiziert die Ground Truth $GT$, indem nur Paare durchgelassen werden, die
  zum Blocking Schema $BS$ passen. Das Ergebnis ist die gefilterte Ground Truth
  $FGT$.
* **Similarity Lerner**. Der Similarity Lerner bestimmt für jedes Attribut eine
  geeignete Ähnlichkeitsfunktion $S$.
* **Fusion-Lerner**. Der Fusion-Lerner ermittelt die besten Parameter für den
  verwendeten Klassifikator. Von diesem erhält der Fusion-Lerner mögliche
  Parameter $PG$, anhand welcher das Klassifikationsmodell $M$ trainert wird.
* **Indexer**. Der Indexer wendet ein Blocking Verfahren auf die
  vorverarbeiteten Daten $PD$ an und bietet bei einer Anfrage eine
  Kandidatenliste $C$ mit möglichen Duplikaten an.
* **Klassifikator**. Der Klassifikator ordnet die Kandidate aus $C$ in Matches
  und Non-Matches, die Menge an Matches $R$, ist das Ergebnis einer Anfrage.

Die Hauptaufgabe der Engine ist es, die Interaktionen zwischen den Komponenten
zu steuern. Dazu werden im simpelsten Fall die Daten von einer Komponente zur
nächsten weitergereicht. Zum Teil muss die Engine zunächst jedoch die
Rückgabewerte für die nächste Komponente aufbereitet (vgl. Label Filter). Die
Engine dient weiterhin als Schnittstelle für den Benutzer. Alle drei Phasen
haben den Schritt der Vorverarbeitung gemeinsam.

### Vorverarbeitung

Die Vorverarbeitung der Daten ist in allen drei Phasen notwendig und macht die
Datensätze robuster gegenüber Missklassifikationen, indem offensichtliche Fehler
korrigiert und eventuelle, für die Identifikation von Entitäten irrelevante,
Varianzen bereinigt weren. In @fig:preprocessing sind die beteiligten
Komponenten Parser und Präprozesser mit ihren Aktivitäten visualisiert. Jede
Phase beginnt mit der Auswahl des korrekten Parsers durch die Engine.

```{.plantuml #fig:preprocessing
    caption="Aktivitätsdiagramm der Vorverarbeitung. Der Parser liest einen
    Datensatz, welcher vom Präprozessor transformieren wird. Der transformierte
    Datensatz wird von der Engine abgespeichert."}
|Engine|
start
:choose parser for Fit-,
Build- or Query-Phase;
|Parser|
:read dataset into D;
if (is Fit-Phase?) then (yes)
    :assign attribute datatypes;
endif
|Preprozessor|
:transform dataset into PD;
|Engine|
:save transformed dataset PD;
:proceed with (Fit/Build/Query)-Phase;
```

**Parser**. Der Parser ist eine einfache Komponente, welche Datensätze aus einer
Datenquelle liest und eine Menge von Tupeln $D$ an die Engine übergibt. Je nach
Phase kann die Datenquelle ein beliebiges Format haben, weshalb für jede Phase
ein eigener Parser bestimmt werden kann. Für *Fit-* und *Build-Phase*, wo große
Datenmengen bearbeitet werden, liest der Parser beispielsweise aus einer
CSV-Datei oder selektiert die Datensätze aus einer Datenbank. Währenddessen in
der *Query-Phase* nur einzelne oder kleine Datenmengen gelesen werden, weshalb
der Parser hier aus einer Message Queue (MQ) Datensätze erhalten könnte. Während
der *Fit-Phase* hat der Parser zudem dafür Sorge zu tragen, dass der Engine die
Attribute des Datensatzes, sowie deren Datentypen bekannt gemacht werden. Anhand
der Datentypen können die Komponenten der Fit-Phase effizienter eine gute
Konfigurationen bestimmen. Wenn der Parser diese Information nicht bereitstellt,
werden alle Attribute als Zeichenketten behandelt.

**Präprozessor**. Der Präprozessor bzw. die Präprozessor-Pipeline besteht aus
einer Reihe von Funktionen, die nacheinander auf alle Tupel aus $D$ angewandt
werden, um diese für die Entity Resolution vorzubereiten und robuster zu machen.
Je nach Datentyp des Attributs wird dabei eine andere Pipeline verwendet.
Dieselbe Präprozessor-Pipeline muss in allen drei Phasen verwendet, damit die
vorverarbeiteten Datenmenge $PD$ stets die gleichen Charakteristiken aufweist.
Werden vom Benutzer keine Operationen vorgegeben, beschränkt sich die Pipeline
auf generische Modifkationen. Die zum Zeitpunkt dieser Thesis entwickelte Engine
ist, zum Zwecke der Vorverarbeitung, automatisiert lediglich in der Lage Strings
in Kleinschreibweise zu konvertiert. Andere Operationen wie das Entfernen von
Stopwörtern (z.B. *und*, *oder*) benötigen zum einen die Sprache der Attribute,
welche zu erkennen durchaus eine lösbare Aufgabe ist, zum anderen aber auch
einen Datenbestand gegen den geprüft wird. Ein komplexere domänenspezifische
Anwendung hierfür ist, beispielsweise die Überprüfung der postalischen Addresse,
welche zum einen länderspezifische Daten benötigt und zum anderen auch ständig
auf dem aktuellen Stand gehalten werden muss. Neben zusätzlichen Funktionen muss
der Benutzer auch die Reihenfolge der Funktionen vorgeben. Beispielweise
zunächst die Rechtspreibprüfung und anschließend die Konvertierung in
Kleinschreibweise, da durch die Rechtschreibprüfung diese Konvertierung in
Teilen wieder aufgehoben wird.

Die vorverarbeiteten Tupel $PD$ des Präprozessors, werden abschließend
abgespeichert, um zu einem späteren Zeitpunkt geladen zu werden.

### Fit-Phase

```{.plantuml #fig:fit_phase
    caption="Aktivitätsdiagramm der Fit-Phase. Die Engine kontrolliert den
    Datenfluss zwischen den Komponenten, speichert Konfigurationen und breitet
    Daten für Komponenten auf. Der Label Generator erzeugt die Ground Truth,
    durch welche ein DNF-Blocking Schema vom BS-Lerner erzeugt wird. Auf einer
    durch das Blocking Schema gefilterten Liste werden anschließend die
    Ähnlichkeitsfunktionen bestimmt. Anhand dieser Funktionen können
    Ähnlichkeitsvektoren auf der Ground Truth berechnet werden und vom
    Fusion-Lerner dadurch die Hyperparameter für den Klassifikator bestimmt,
    sowie abschließend das Klassifikationsmodell trainiert werden."}
|Engine|
start
:read transformed dataset PD;
|Label Generator|
:generate ground truth GT;
|Engine|
:save ground truth GT;
|BS-Learner|
:predict DNF Blocking Scheme BS;
|Engine|
:save DNF Blocking Scheme BS;
:filter GT through BS into FGT;
|Sim-Learner|
:predict similarity functions S;
|Engine|
:save similarity functions S;
:calculate ground truth
similarity vectors;
|HP-Optimizer|
:predict hyperparameters;
:train model M;
|Engine|
:save model M;
stop
```

Die für die Fit-Phase relevanten Komponenten und Schnittstellen sind in
@fig:engine in der Box "Fit-Phase" gruppiert. Bei großen Datensätzen kann diese
Phase sehr lange dauern, weshalb die Engine die Teilkonfigurationen der
Komponenten direkt sichert. Dadurch kann die Fit-Phase im Falle eines Abbruchs,
z.B. durch einen Systemneustart, fortgesetzt werden und nur die unterbrochene
Komponente muss wiederholt werden. Wurde die Fit-Phase abgeschlossen, ist es
möglich die ermittelte Konfiguration einzulesen, wodurch die Fit-Phase
übersprungen wird. @fig:fit_phase zeigt das Aktivitätsdiagramm der Fit-Phase,
ohne existierende Konfiguratation.

**Label Generator**. Der Label Generator erzeugt die Ground Truth $GT$, in Form
von klassifizierten Matches und Non-Matches, für später in der Fit-Phase
folgenden Komponenten. Dazu nutzt der Label Generator, die vorverarbeiteten
Tupel $PD$ und bildet Datensatzpaare (@fig:fit_phase). Dabei gibt es zwei
Ausprägungen. In der ersten Ausprägung erhält der Label Generator
vorklassifizierte Matches, für den vom Parser eingelesenen Datensatz (vgl.
@sec:ana_lbl). In der zweiten Ausprägung stehen dem Label Generator keine
vorklassifizierten Matches zur Verfügung. Weshalb die Ground Truth vollständig
automatisiert bestimmt werden muss (vgl. @sec:lblgen_nogt). Ein Label Generator
kann beide Ausprägungen implementieren. Falls nur die erste Ausprägung
implementiert ist, kann die Engine, ohne existierende Ground Truth, die
Fit-Phase nicht durchführen. Sollte nur die zweite Ausprägung vorhanden sein,
werden die vorklassifizierten Matches ignoriert.

**Blocking Schema Lerner**. Der Blocking Schema Lerner ermittelt ein Blocking
Schema in disjunktiver Normalform nach [@KM:Unsupervised:13], welches in
@sec:ana_bs vorgestellt wurde. Dafür benötigt der Blocking Schema Lerner die
vorverarbeiteten Tupel $PD$, sowie die Ground Truth Daten $GT$ des Label
Generators. Zudem werden die Blöcke des genutzten Indexers $IX$ zu jedem zu
analysierenden Audrück benötigt. Daraus wird schlussendlich ein DNF Blocking
Schema $BS$ gebildet.

```texalgo
#alg:gt_filter FilterGT($BS$, $D$, $P$, $N$, $AN$, $max_n$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme: $BS$
  \item Dataset: $D$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \item Set of all generated negative pairs: $AN$
  \item Maximum Non-Duplicate Pairs: $max_n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Set of filtered positive pairs: $fP$
  \item Set of filtered negative pairs: $fN$
  \item Predictions: $y_{pred}$
  \end{itemize}
}
\Statex
\State Initialize empty sets $fP = (), fN = ()$
\For{pair $(p_1.id, p_2.id) \in P$}\label{alg:fgt:p}
    \If{$HasCommonBlock(BS, D, (p_1.id, p_2.id))$}
        \State Append $(p_1.id, p_2.id)$ to $fP$\label{alg:fgt:ap}
    \EndIf
\EndFor
\For{pair $(p1, p2) \in N$}\label{alg:fgt:n}
    \If{$HasCommonBlock(BS, D, (p_1.id, p_2.id))$}
        \State Append $(p_1.id, p_2.id)$ to $fN$\label{alg:fgt:an}
    \EndIf
\EndFor
\While{$|fN| < max_n$ and $|AN| > 0$}\label{alg:fgt:wan}
    \State Draw pair $(p_1.id, p_2.id)$ from $AN$
    \If{$HasCommonBlock(BS, D, (p_1.id, p_2.id))$}\label{alg:fgt:cb}
        \State Append $(p_1.id, p_2.id)$ to $fN$\label{alg:fgt:aan}
    \EndIf
\EndWhile
\State return $fP, fN$

#alg:common_block HasCommonBlock($BS$, $D$, $p$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme: $BS$
  \item Dataset: $D$
  \item Pair: $p = (p_1.id, p_2.id)$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item True if $p$ has common block, false otherwise
  \end{itemize}
}
\Statex
\State Initialize empty sets $p_{1_{bkvs}} = (), p_{2_{bkvs}} = ()$
\For{term $t \in BS$}
    \State $r_1 = D[p_1.id], r_2 = D[p_2.id]$
    \State Add $BlockingKeyValues(t, r_1)$ to $p1_{bkvs}$\label{alg:ct:bkv1}
    \State Add $BlockingKeyValues(t, r_2)$ to $p2_{bkvs}$\label{alg:ct:bkv2}
\EndFor
\If{$p1_{bkvs} \cup p2_{bkvs} \neq \emptyset$}
    \State return True\label{alg:ct:t}
\Else
    \State return False\label{alg:ct:f}
\EndIf
```

**Label Filter.** Anhand des Blocking Schema werden die Kandidaten, welche für
die Entity Resolution infrage kommen, eingeschränkt. Damit der Similarity Lerner
und der Fusion-Lerner ihre Konfiguration lediglich auf relevanten Paaren
ermitteln, muss die Ground Truth durch das Blocking Schema gefiltert werden. Das
Filtern der Ground Truth auf Basis des ermittelten Blocking Schema, wird von der
Engine durchgeführt, bevor der Similarity Lerner aufgerufen wird (siehe
Algorithmus \ref{alg:gt_filter}). Dabei werden nacheinander alle Matches und
Non-Matches der Ground Truth betrachet (Zeilen \ref{alg:fgt:p},
\ref{alg:fgt:n}). In Algorithmus \ref{alg:common_block} werden für jedes Paar
$(p_1.id, p_2.id)$ die Blockschüssel, anhand des gegeben Blocking Schema,
generiert (Zeilen \ref{alg:ct:bkv1}, \ref{alg:ct:bkv2}). Gibt es dabei eine
Überlappung, dann gibt es für mindestens ein Attribut einen gemeinsamen Block,
in welchem das Paar zusammen vorkommt. In diesem Fall gibt der Algorithmus Wahr
zurück (Zeile \ref{alg:ct:t}). Gibt es keine Überlappung wird Falsch
zurückgegeben (Zeile \ref{alg:ct:f}). Wurde durch Algorithmus
\ref{alg:common_block} festgestellt, dass ein Match bzw. ein Non-Match einen
gemeinsamen Blockschlüssel hat, dann werden dieses zur gefilterten Ground Truth
$fP$ oder $fN$ hinzugefügt (Zeilen \ref{alg:fgt:ap}, \ref{alg:fgt:an}). Da das
Ziel des Blocking Schema ist, möglichst nur gleiche Entitäten zu gruppieren,
werden beim Filtern sehr viele Non-Machtes, im schlimmsten Fall alle,
herausgefiltert. Dadurch ist die gefilterte Ground Truth zugunsten der Matches
unbalanciert. Damit der Similarity Lerner und der Fusion-Lerner dennoch eine
sinnvolle und aussagekräftige Konfiguration ermitteln können, werden die
Non-Matches künstlich angereichert. Dazu werden die vom Label Generator zuvor
verworfenen Non-Matches $AN$ benötigt. Diese werden nun versucht zur Ground
Truth hinzuzufügen, indem wie davor eine Überlappung der Blockschlüssel gesucht
wird (Zeile \ref{alg:fgt:cb}). Gibt es eine Überlappung wird das Paar zu $fN$
hinzugefügt (Zeile \ref{alg:fgt:aan}). Dies wird solange wiederholt, bis die
Ground Truth $max_n$ Non-Matches beinhaltet oder die gesamte Menge der
Non-Matches des Label Generators erschöpft sind (Zeile \ref{alg:fgt:wan}).

**Similarity Lerner**. Der Similarity Lerner bestimmt aus einer Menge von
Ähnlichkeitsfunktion für jedes Attribut die geeigneste, nach dem Verfahren aus
@anasim. Dabei werden nur Attribute betrachtet die einen gemeinsamen
Blockschlüssel durch das Blocking Schema $BS$ erhalten. Zur Bewertung werden die
Datensatzpaare der gefilterten Grund Truth $FGT$ genutzt. Das Ergebnis ist die
Tupelliste $S$, welche jeweils ein Ähnlichkeitmaß mit Datenfeld verknüpft.

**Fusion-Lerner**. Der Fusion-Lerner ermittelt für einen gegebenen Klassifikator
die Parameter, die das Modell mit der besten Bewertung erzeugen. Bevor der
Fusion-Lerner aufgerufen werden kann, erzeugt die Engine für jedes Paar der
gefilterte Ground Truth $FGT$, anhand der Ähnlichkeitsfunktionen des Similarity
Lerners $S$, einen Ähnlichkeitsvektor pro Paar. Die Ähnlichkeitsvektoren sind
dabei nur an den Stellen besetzt, in welches die Attribute des Paares einen
gemeinsamen Block im Blocking Schema $BS$ haben. Die Ähnlichkeitsvektoren werden
dann vom Fusion-Lerner genutzt, um ein Modell mit gegebenen Parametern zu
trainieren. Das Parameternetz $PG$ muss von der Klassifikatorkomponente
bereitgestellt werden, beispielsweise die maximale Tiefe eines Decision Tree. Um
einen optimalen Klassifikator für die Eingabedaten zu bekommen, ist es abgesehen
von der Parameterliste möglich, eine Liste von verschiedenen Klassifikatoren
anzugeben, beispielsweise einen Decision Tree und eine SVM. Das Ergebnis des
Fusion-Lerners ist das Modell $M$ eines Klassifikator mit den Parameter, die die
beste Bewertung erzielt haben.

### Build-Phase

```{.plantuml #fig:build_phase width=60%
    caption="Aktivitätsdiagramm der Build-Phase. Der liest alle vorverarbeiteten
    Datensätze einer initalen Datensatzes ein und fügt diese seinem Index hinzu."}
|Engine|
start
:read transformed dataset;
repeat
    :get record from dataset;
    |Indexer|
    :insert record into index;
repeat while (more records?)
|Engine|
:save index;
stop
```

Die Build-Phase dient der Vorbearbeitung der Daten, bevor das
selbstkonfigurierte ER-System seinen Betrieb aufnehmen kann. Dazu wird der
komplette Datenbestand, in welchem Entitäten gesucht werden sollen, betrachtet.
Nachdem diese durch die Vorverarbeitung gelaufen sind, wird auf den Daten ein
Blocking-Verfahren durchgeführt. Der **Indexer** ist ein Blocking Mechanismus,
der zum einen mit dynamischen Daten umgehen können muss und zum anderen das
Blocking anhand des DNF Blocking Schema $BS$ durchführt. In @fig:build_phase
wird die Build-Phase erläutert. Die Engine liest zunächst alle vorverarbeiteten
Datensätze $PD$ ein. Anschließend werden die Datensätze einzel dem Indexer
übergeben, welcher diese zu seinem Index hinzufügt. Dabei besteht die
Möglichkeit, dass der Index während des Einfügens anhand der gelernten
Ähnlichkeitsfunktionen $S$ bestimmte Ähnlichkeiten vorausberechnet. Das Bauen
des Index kann einige Minuten, eventuell sogar Stunden, dauern. Deshalb wird der
Index nach dem Bauen gespeichert. Im Falle eines Neustarts der Engine müssen
dann nur die Datensätze eingefügt werden, welche während der Query-Phase
hinzugekommen sind.

### Query-Phase

```{.plantuml #fig:query_phase width=90%
    caption="Aktivitätsdiagramm der Query-Phase. Zunächst werden der
    transformierte Datensatz vom Präprozessor gelesen. Danach werden Datensätze
    einzeln entnommen und dem Indexer übergeben. Dieser liefert eine
    Kandidatenliste. Jeder Kandidat wird vom Klassifikator in Match bzw.
    Non-Match klassifiziert. Matches werden von der Engine gespeichert und
    Non-Matches verworfen. Am Schluss wird das Ergebnis aller Anfragen dem
    Benutzer übergeben."}
|Engine|
start
:read transformed dataset;
while (more queries?) is (yes)
    :get query record from dataset;
    |Indexer|
    :query candidates from index;
    |Engine|
    while (more candidates?) is (yes)
        |Engine|
        :get candidate record from candidate list;
        |Klassifier|
        :predict candidate class;
        |Engine|
        if (is candidate a match?) then (yes)
            |Engine|
            :save candidate as match;
        else (no)
            |Engine|
            :discard candidate;
        endif
    endwhile (no)
endwhile (no)
|Engine|
:pass results to user;
stop
```

In der Query-Phase (siehe @fig:query_phase) erhält die Engine von einem
Query-Parser eine Menge von Anfragedatensätzen. Nachdem diese vorverarbeitet
wurden, wird jeder Datensatz einzeln dem Indexer übergeben. Dieser erzeugt für
den übergebenen Datensatz eine Kandidatenmenge $C$ möglicher Matches. Diese
Kandidaten werden dem Klassifikator übergeben. Das Modell des **Klassifikators**
wurde während der *Fit-Phase* von dem Fusion-Lerner trainiert und kann nun in
der *Query-Phase* genutzt werden, um die Kandidaten in Matches und Non-Matches
zu klassifizieren. Das Ergebnis der Klassifikation speichert die Engine
zwischen, bis alle Datensätze verarbeitet wurden. Abschließend werden die
gesammelten Ergebnisse $R$ an den Benutzer übergeben.

### Auswertung

Für die Entwicklung von Komponenten besitzt die Engine die Möglichkeit Metriken
zu messen und diese auszuwerten. Diese liefern ein wichtiges Indiz, wie gut eine
Komponente funktioniert. Des Weiteren ist es dadurch möglich das Zusammenspiel
der Komponenten untereinander zu bewerten, indem beispielsweise eine alternative
Komponente eingesetzt wird oder verschiedene freie Parameter gewählt werden. Von
den Metriken, welche in @sec:measurements beschrieben wurden, kann die Engine
für das Blocking die Pairs Completeness, Pairs Quality und Reduction Ratio
aufzeichnen, sowie für den Klassifikator Recall, Precision, F-measure und
Average Precision messen. Des Weiteren werden die Daten zum Zeichnen eines
F-measure Graphen und einer Precision-Recall Kurve bereitgestellt. Darüber
hinaus kann die Engine messen, wie lange einzelne Operationen einer Komponente
benötigen. Beispielsweise wird gemessen, wie lange es dauert einen Datensatz in
den Index einzufügen bzw. zu einem Anfragedatensatz die Kandidatenliste zu
erhalten. Dadurch kann die Performanz, beispielsweise in Anfragen pro Sekunde
auf einer Testhardware angegeben werden. Alle Metriken werden während der
Query-Phase erhoben und können nach jeder Anfrage abgefragt werden.

# Evaluierung der Implementierung

In diesem Kapitel wird die Programmierumgebung, die Implementierung der Engine
und die Implementierung der Komponenten evaluiert. Eine große Herausforderung
bei der Umsetzung der Algorithmen war es, diese für die begrenzten Ressourcen,
insbesondere Arbeitsspeicher und Rechenzeit, zu optimieren.

## Programmierumgebung

Als Programmiersprache für die Implementierung wurde Python und C eingesetzt.
Wobei C lediglich zur Implementierung der Ähnlichkeitsberechnung eingesetzt
wurde, alle anderen Teile wurden mit Python umgesetzt. Python hat den Vorteil,
dass es sehr einfach und schnell möglich ist, einen Prototypen eines
Algorithmuses zu entwickeln und zu testen. Zudem gibt es eine Vielzahl von
Qualitativ hochwertigen Paketen, die komkompfortable Standardfunktionalitäten
bereitstellen, beispielsweise das Einlesen und das Schreiben von großen
CSV-Dateien oder das Plotten von Graphen. Des Weiteren wird Python im Maschine
Learning Bereich oft genutzt, was dazu führt, dass es eine Vielzahl von
effizienten, ausgereiften und umfangreichen Frameworks gibt, um verschiedenste
Lernaufgaben zu behandeln. Vor allem der Fusion-Lerner und der Klassifikator
profitieren hiervon.

Der große Nachteil von Python ist das Global Interpreter Lock (GIL). Dieses
verhindert, dass Python-Code in mehreren Threads gleichzeitig ausgeführt werden
kann. Die Multithreading Bibliothek von Python ist daher lediglich geeignet, um
Programme mit hoher E/A-Last zu beschleunigen, da Schreib- bzw. Lesezugriffe das
GIL freigeben. Der Grund warum in Python ein GIL einzusetzen wird ist, dass
dadurch die Single-Thread Ausführung optimiert wird. Multithreading, im Sinne
von Gleichzeitigausführung, d.h. ein Prozess mit mehreren Threads, die auf
verschiedenen Prozessorkernen, zur selben Zeit ausgeführt werden, wird dadurch
allerdings komplett unterbunden. Um denoch Python zu paralelisieren gibt es zwei
beliebte Möglichkeiten. Die erste Möglichkeit ist, statt Multithreading,
Multiprocessing einzusetzen. Das hat allderdings den Nachteil, dass Daten
zwischen Prozessen ausgetauscht werden müssen. Das lohnt sich offensichtlich nur
für rechenintensive Aufgaben, wo der Overhead des Datenaustausches keine Rolle
spielt. Die zweite Möglichkeit ist das Multithreading in einer anderen
Programmiersprache umzusetzen, beispielsweise in C. Dies ist möglich, da das GIL
lediglich die Mehrfachausführung von Python-Code verhindert. Allerdings erweist
sich dies oft als relativ schwierig, da selbst einfache Datenklassen,
beispielsweise `set` oder `dict`, keine Entsprechung in C haben und daher
manuell, in beide Richtungen Python zu C und C zu Python, z.T. aufwendig
konvertiert werden müssen.

Aufgrund der genannten Nachteile von Python wurde die Engine und sämtliche
Komponenten lediglich in einem Thread ausgeführt. Ideen dies zu optimieren
konnten nicht im Zeitrahmen der Thesis umgesetzt werden. Dabei kann vor allen in
der Fit-Phase durch Multi-Threading und Parallel Programming Laufzeit eingespart
werden. Die längste Laufzeit haben dabei der DNF Blocks Lerner und der
Fusion-Lerner.

## Label Generator

Für den Label Generator wurden beide Ausprägungen (mit und ohne Ground Truth)
umgesetzt. Zunächst wird die Variante ohne Ground Truth beschrieben und
anschließend die Variante mit Ground Truth, welche eine Modifikation der ersten
Ausprägung ist.

Der Label Generator wurde gegenüber dem Algorithmus \ref{alg:weaklabels} von
Kejriwal & Mirankern [@KM:Unsupervised:13] und dessen Anpassung mit Ground Truth
Matches in Algorithmus \ref{alg:labels} in zwei Punkten modifiziert. Zunächst
werden die Datensätze in den Blöcken alphabetisch sortieren. Damit ist es
möglich deterministische Ergebnisse zu bekommen und daraufbasierend geeignete
Testfälle zu schreiben. Des Weiteren werden wie beim klassischen Sorted
Neighborhood Verfahren, dadurch ähnliche Datensätze näher zusammengebracht, was
die Wahrscheinlichkeit erhöht aussagekräftige Paare zu selektieren. Die zweite
Anpassung ist sowohl eine Laufzeit-, als auch ein Arbeitsspeicheroptimierung.
Ähnlich zum Record Identifier Index der Similarity-Aware Inverted-Index
Verfahren, kann es durch das Blocking auf Basis der Token ebenfalls dazu kommen,
dass rießige Blöcke erzeugt werden. Selbst wenn ähnliche Attribute durch
Sortierung näher zueinander sortiert wurden, ist in diesen Blöcken ein größes
Fenster nötig, um aussagekräftig Paare zu finden. Dies wiederum führt zu einer
Explosion der Kandidatenmenge und damit des Arbeitsspeichers und der Laufzeit.
Zur Optimierung wird ein Blockfilter eingeführt, sodass lediglich Kandidaten in
Blöcken generiert werden, deren Anzahl an Datensätzen kleiner einer Schwelle $z$
sind.

## Blocking Schema Lerner {#sec:eval_dnflearner}

Die Algorithmen des DNF Blocks Lerners haben bei der Implementierung das
Problem, das nur eine bestimmte Menge an Arbeitsspeicher zur Verfügung steht.
Der kritische Teil des Algorithmus ist die Erzeugung der Paarkombinationen, für
jeden Block. Angenommen die beiden Datensatzidentifier eines Paares $(p1.id,
p2.id)$ sind Integerwerte und der Datensatz hat nicht mehr als 2^30 Einträge,
dann benötigt ein Integerwert 28 Bytes. Um möglichst effizient auf die Paare
zuzugreifen, ist die Menge von Paarkombinationen als `set` implementiert. Damit
ein `set` $s$ ein Zugriffkomplexität von $O(1)$ ermöglichen kann, wird für jedes
Element in der Menge ein Hashwert berechnet. Auf einem 64-bit System beträgt die
Größe dieses Hashwertes $h$ 8 Bytes. Somit benötigt ein Eintrag $(h_j, p1_j.id,
p2_j.id) \in s$ 64 Bytes. Angenommen es werden für einen Block mit 1.000
Einträgen Paarkombinationen erzeugt. Bei Attributen mit wenigen möglichen Werten
können Blöcke entstehen, die sehr viele Datensätze enthalten. Beispielsweise hat
ein Block mit 10.000 Einträgen 49.995.000 Paare und benötigt 2.9 GB an
Arbeitsspeicher. Somit kann bereits ein rießiger Block den zur Verfügung
stehenden Arbeitsspeicher sprengen und führt damit zum Abbruch des Programmes.
Aus diesem Grund wurde der Algorithmus dahingehend erweitert, dass die Erzeugung
der Paare bei Ausdrücken, die zu viele Paare erzeugen würden, unterbunden wird
und dies Ausdrücke mit der niedrigsten Wert der Bewertungsskala bewertet werden.

Zur genaueren Analyse des Problems, wird die Verteilung der Blöcke, anhand ihrer
Größe (Anzahl von Datensätzen), betrachtet. Um die Verteilungen in Gute,
benötiget weniger Arbeitsspeicher als zur Verfügung steht und Schlechte,
benötiget mehr Arbeitsspeicher als zur Verfügung steht, zu kategorisieren, wurde
eine Schwelle $t$ eingeführt. Anhand dieser Schwelle wird ein Block $B$ bei $|B|
< t$ als guter Block und bei $|B| > t$ als schlechter Block bewertet. Daraus
kann für jede Verteilung berechnet werden, wie viel Prozent gute bzw. schlechte
Blöcke es gibt. Dadurch ist es möglich bei Audrücken mit einer höheren
schlechten Blockrate von $b$, beispielsweise $b=0.1$, die Erzeugung der
Blockpaare zu verhindern und die weitere Verarbeitung abzubrechen. Da aber
bereits ein einziger schlechter Block, mit genügend Einträgen, den
Arbeitsspeicher überfüllen kann, wird mit der Schwelle $b$ lediglich eine
Vorauswahl, besonders schlechter Ausdrücke, getroffen. Für den Fall, dass es nur
wenige schlechte Blöcke gibt, bestehen deren Blockschlüssel meistens aus
Stopwörtern, beispielsweise bei Strassennamen `Strasse`, `Weg`, oder `Platz`.
Dieses Problem kann folglich durch eine bessere Vorverarbeitung der Daten gelöst
werden. Da es das Ziel ein selbstkonfigurierendes System ist, muss die Engine,
die auf diese Weise gefundenen Stopwörter nutzen und den Lernvorgang mit der
erweiterten Vorverarbeitung der Daten wiederholen. Dieser Prozess sorgt
allerdings dafür, dass das Lernen der Konfiguration deutlich länger dauert. Eine
einfacherere Möglichkeit ist, für jeden Ausdruck eine Liste mit verbotenen
Blockschlüsseln anzulegen und die Blockschlüssel schlechter Blöcke dort
hinzuzufügen. In der Build- und Query-Phase dürfen diese Blöckschlüssel vom
Indexer demnach nicht genutzt werden.

Trotz dieser Optimierungen hat der DNF Blocks Generator immer noch hohe
Arbeitsspeicheranforderungen, welche verhindern das Multithreading oder
Multiprocessing auf einem Rechner zur Laufzeitoptimierung eingesetzt werden
können. Denkbar ist aber die Verteilung auf ein Cluster von Rechnern,
beispielsweise per Hadoop, wodurch deutlich mehr Prädikate in kürzerer Zeit
überprüft werden können.
