# Evaluierung der Qualtiät und Effektivität

TBW

## Berechnung der Metriken für dynamisches Enity Resolution

Während im statischen Entity Resolution, die Metriken (vgl. @sec:measurements)
am Ende des Verfahrens einmalig berechnet werden können, ist dies im dynamischen
Falle nicht möglich, da es theoretisch kein Ende gibt. Das bedeutet, die
Metriken müssen inkrementell mit jeder Anfrage $q$ erhoben werden. Für Anfragen
ohne tatsächliche Matches werden keine Metriken erhoben, da diese ansonsten das
Ergenis verfälschen, weil die Metriken keine Aussagekraft für diese Anfragen
haben. Die Ausnahme ist das Reduction Ratio, welches für alle Anfragen gemessen
wird. Zur Berechnung der Effektivitätsmaße Pairs Completeness, Pairs Quality und
Reduction Ratio werden die tatsächlichen Matches $n_M$, die tatsächlichen
Non-Matches $n_N$, die Matches in der Kandidatenmenge $s_M$ und Non-Matches in
der Kandidatenmenge $s_N$ benötigt. Die Kandidatenmenge wird mit $C$ bezeichnet,
die tatsächlichen Matches mit $P$ und die Menge der Datensätze des Indexer mit
$IX$. Für jede Anfrage werden diese Metriken folgendermaßen berechnet:

$$\begin{aligned}
s_M &= \forall c \in C: \sum |(q_{id}, c) \cap P| + |(c, q_{id}) \cap P| \\
s_N &= |C| - s_M \\
n_M &= \forall (p_1, p_2) \in P: \sum |(q_{id}, p_2) \cap P| + |(p_1, q_{id}) \cap P| \\
n_N &= (|IX \setminus q_{id}|) - n_M
\end{aligned}$$

$s_M$ ist die Anzahl der Matches zur Anfrage $q$ in $C$, $s_N$ ist die Anzahl
der Non-Matches zu $q$ in $C$, $n_M$ ist die Gesamtanzahl der Matches zu $q$ in
den Matches $P$ und $n_N$ ist die Gesamtanzahl an Non-Matches zu $q$ in $IX$.
Für $n_N$ muss der Anfragedatensatz von der Gesamtmenge abgezogen werden, da
dieser zu Begin jeder Anfrage vom Indexer in den Datenbestand aufgenommen wird
bzw. wenn er dort schon vorhanden ist keine Rolle für die Enity Resolution
spielt, da er herausgefiltert wird. Mit jeder Anfrage werden $s_M, s_N$ und
$n_M$ mit den vorherigen Werten aufsummiert, sodass die Effektivtätsmaße Bezug
auf alle bisher gestellten Anfragen nehmen. Die Anzahl $n_N$ nimmt Bezug auf
eine wachsende Menge, sodass beim Aufsummieren die frühen Anfragen abgewertet
werden. $n_M$ wird zur Berechnung des Reduction Ratio benötigt, sodass dieses
für jede Anfrage berechnet und bei Bedarf gemittelt wird.

Die Qualitätsmaße Recall, Precision, F-measure und Average Precision werden über
die True Positives (TP), False Positives (FP) und False Negatives (FN) bestimmt.
Deren Berechnung ist identisch zu den Werten der Effektivitätsmaßen mit der
Abweichung, dass diese auf der Ergebnismenge $R$ gemessen werden. Die True
Negatives werden nicht berechnet, da diese in den Metriken nicht benötigt
werden. Auch hier werden die Werte für jede Anfrage summiert, sodass die aus der
Summe berechneten Metriken alle bisherigen Anfragen berücksichtigen.

## Experimenteller Aufbau

![Aufteilung der Datensätze in Validierungsmenge, Trainingsmenge und Testmenge.
Tupel in den Mengen sind durch Punkte markiert und Duplikate durch eine Line
zwischen zwei Tupeln. Die farbigen Linen zeigen, wie die jeweilige Untermenge
gebildet wird. ](./images/testsets.svg){#fig:testsets}

Für die Durchführung der Evaluierung wurden die Datensätze in vier disjunkte
Teildatensätze gesplittet. Diese Aufteilung ist in @fit:testsets dargestellt.
Die Hälfte der Datensätze befindet sich in der Base, die andere Hälfte ist zu
gleichen Teilen in Validierung, Training und Testing aufgeteilt. Datensätze in
den Mengen sind durch schwarze Punkte markiert. Matches sind durch Linien
verbunden. In der Build-Phase wird der initiale Index stets aus den Datensätzen
der Base gebaut. Der Anfragestrom, in der Query-Phase, wird durch Datensätze aus
Validierung, Training oder Testing zusammengestellt. Durch die Verteilung der
Matches ist sichergestellt, dass dadurch für jedes Match eine Query durchgeführt
wird, in welcher das jeweilige andere in der Base gefunden werden kann. In der
Fit-Phase werden die Duplikate zusammen benötigt, weshalb jeweils Validierung,
Training und Testing mit der Base zusammengefasst werden, wie durch die rote,
grüne bzw. blaue Umrandung dargestellt ist.

![Vorgehen zur Aufteilung eines Datensatzes in vier Teilmengen. Datensätze
werden in drei Kategorien zugeordnet: Non-Matches (rot), Matches (grün),
Matchcliquen (blaugrün). Diese werden seperat in Teil 2, 3 und 4 aufgeteilt.
](./images/testsetssplitting.svg){#fig:testsetssplit}

Das Vorgehen zum Teilen eines Datensatzes ist in @fig:testsetssplit dargestellt.
Dazu werden die Datensätze in drei Kategorien eingeteilt (Teil 1): Non-Matches
(rot), Matches (grün) und Cliquen von Matches (blaugrün). Zuerst werden die
Cliquen auf die Mengen verteilt (Teil 2). Dafür wird jeweils ein Datensatz
bestimmt, der der Base zugewiesen wird, hier `1` und `16`. Anschließend werden
die restlichen Datensätze der Cliquen per Round-Robin auf Validation, Training
und Testing verteilt. Für die erste Clique bedeutet das `1` in die Base, `2` in
Validation und `3` in Training. Bei der zweiten Clique kommt `16` in die Base,
`17` in Testing, da das Round-Robin der vorherigen Clique vorgesetzt wird, `18`
in Validation und `19` in Training. Danach werden die einfachen Matches
ebenfalls über Round-Robin aufgeteilt (Teil 3). Jeweils ein Datensatz der Paare
wird der Base zugewiesen (`7`, `9`, `11`) und der andere wird auf Validation,
Training und Testing verteilt, wobei der Round-Robin Mechanismus unabhängig von
dem der Cliquen ist. Zum Schluss werden die Non-Matches aufgeteilt (Teil 4).
Dazu werden jeweils drei Datensätze der Base zugewiesen und anschließend drei
per Round-Robin auf Validation, Training und Testing verteilt. Dieser
Round-Robin Mechanismus ist ebenfalls unabhängig von den beiden anderen. Durch
die drei unabhängigen Round-Robin Aufteilungen, kann es dazu kommen, dass der
Testing Datensatz bis zu drei Datensätze weniger hat als Validation oder
Testing. Bei tausenden bzw. Millionen von Datensätzen ist dies jedoch nicht
ausschlaggebend.

## Datensätze {#sec:datasets}

| Datensatz             |  Einträge | Duplikatspaare | Attribute |
|-----------------------+----------:+---------------:+----------:|
| Abt-Buy               |     2.171 |          1.096 |         4 |
| Amazon-GoogleProducts |     4.587 |          1.299 |         4 |
| Cora                  |     1.879 |         64.577 |         5 |
| DBLP-ACM              |     4.908 |          2.223 |         4 |
| DBLP-Scholar          |    66.877 |          5.346 |         4 |
| NCVR                  | 8.261.839 |        155.470 |        17 |
| Restaurant            |       864 |            112 |         4 |

: Überblick der Datensätze aus wissenschaftlichen Veröffentlichungen.
{#tbl:datasets_overview}

### CORA

Der CORA Datensatz beinhaltet 1879 bibliographische Einträge über
wissenschaftliche Veröffentlichungen aus dem Maschine Learning Bereich. Die
Einträge bestehen aus Autoren, Titel, Publikationsjahr und Konferenz bzw.
Journal. Insgesamt beinhaltet dieser Datensatz 64.577 Duplikate. Dieser
Datensatz ist besonders schwierig zu Deduplizieren, da teilweise nur Initialen
der Autoren vorhanden sind bzw. Attribute zusammengefügt oder getauscht wurden.

### Abt-Buy & Amazon-GoogleProducts

Diese beiden Datensätze beinhalten Produkte aus dem Onlinehandel verschiedener
Plattformen mit Name, Beschreibung, Hersteller und Preis. Der Abt-Buy Datensatz
beinhaltet 2171 Einträge mit 1096 Duplikaten. Im Amazon-GoogleProducts Datensatz
sind es 4587 Einträge mit 1299 Duplikaten.

### DBLP-ACM & DBLP-Scholar

Diese beiden Datensätze beinhalten bibliografische Einträge mit Titel,
Autor(en), Konferenz, und Jahr. Der DBLP-ACM Datensatz beinhaltet 4908 Einträge
und 2223 Duplikate. Im DBLP-Scholar Datensatz sind sind 66877 Einträge mit 5346
Duplikaten. Dabei ist zu beachten, dass der DBLP-ACM Datensatz einfach zu
klassifizieren ist, da ein Großteil der Daten durch eine Instanz gepflegt wird.

### Restaurant

Der Restaurant Datensatz ist ein kleiner mit lediglich 864 Einträgen, welche aus
Restaurantname, Adresse, Telefonnummer und der Küchenart bestehen. Es gibt
insgesamt 112 Restaurantduplikate, welche doppelt vorkommen.

### NCVoter

Der NC Voter Registration (NCVoter) Datensatz beinhaltet ca. 8 mio Datensätze aus
dem Wählerverzeichnis des Bundestates North Carolina in den USA. Eine genaue
Analyse des Datensatzes wurde von Christen [@Chr:Preparation:13] durchgeführt.
Der Datensatz beinhaltet ca. 145.000 Duplikate zwischen zwei Einträgen, sowie
3.500 zwischen drei und mehr Einträgen. Die Zuordnung der Duplikate wurde dabei
über die Wählerregistriernummer getätigt. Weitere Attribute sind Namenspräfix,
Vorname, Zweiter, Vorname, Nachname, Namenssuffix, Alter, Geschlecht,
Rassenziffer, Ethnizitätsziffer, Strasse + Hausnummer, Stadt, Bundesland,
Postleitzahl, Telefonnummer, Geburtsort und Registrierdatum.

### Febrl

Die Febrl-Datensätze wurden synthetisch durch den Febrlgenerator erzeugt. Die
Attributsdaten dafür liefert ein australisches Telefonbuch. Die generierten
Einträge haben folgende Attribute: Kultur, Geschlecht, Alter, Geburtsdatum,
Titel, Vorname, Nachname, Bundesland, Vorort, Postleitzahl, Hausnummer, Straße
und Telefonnummer.

* Febrl-4k-1k: 5.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-9k-1k: 10.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-90k-10k: 100.000 Einträge mit 10.000 Duplikaten zwischen zwei Datensätzen

## Auswahl der Komponenten

Die Komponenten des selbstkonfigurierenden Systems wurden in der Analyse in
Kapitel 3 und im Design in Kapitel 4 vorgestellt. Dabei ist der Label Generator
(@sec:ana_lbl), der Blocking Schema Lerner (@sec:ana_bs), und der Similarity
Lerner (@#sec:fit_comp) fix. Alternativen gibt es für jeweils für Parser,
Präprozessor, Fusion-Lerner, Klassifikator und Indexer.

Die Datensätze aus @sec:datasets liegen alle Samt im CSV-Format vor. Weshalb
auch die gesplitteten Datensätze ins CSV-Format geschrieben wurden. Deshalb ist
der Parser ein CSV-Parser. Da während der Thesis nicht genügend Zeit war, um die
Datentypen der Attribute zuverlässig zu erkennen, werden alle Attribute als
Strings behandelt, was unter anderem die Wahl geeigneter Prädikate für den
Blocking Schema Lerner vereinfacht.

Weiterhin sind alle Datensätze aus @sec:datasets in englischer Sprache, weshalb
der Präprozessor bekannt englische Stopwörter herausfiltert und anschließend
alle Attribute in kleinschreibweise konvertiert. Weitere Attributs- bzw.
Datensatzspezifische Anpassungen werden nicht durchgeführt.

Um die Hyperparameter der Klassifikatoren zu erlernen muss der
Fusion-Lerner insbesondere Wissen, wie deren API Schnittstelle ist, damit er die
Modelle trainieren und auswerten kann. Aufgrunddessen und weil die
Implementierung von verschiedenen Lernverfahren und Klassifikatoren nicht
Schwerpunkt der Thesis ist, wurde für die Umsetzung die Python Maschine Learning
Bibliothek Scikit-learn [@PVG.EA:Scikitlearn:11] eingesetzt. Diese bietet ein
breites Spektrum an Funktionen:

* Klassifikation, bestimmen zu welcher Klasse ein Objekt gehört.
* Regression, einen fortlaufenden Wert eines Objektes vorhersagen.
* Clustering, automatisches gruppieren von Objekten.
* Dimensionsreduktion, reduzieren der Anzahl zu betrachtender zufälliger
  Variablen.
* Modellauswahl, vergleichen, validieren und auswählen von Parametern und
  Modellen.
* Vorverarbeitung, Eingabetransformation und Normalisierung.
* Evaluation, berechnen der Effizienz und Qualität von Modellen.

Für den Fusion-Lerner sind dabei das Module zur Modellauswahl und Evaluation
interessant. Dieser ist zudem die einzige Komponente, die ihre Aufgabe
parallelisieren kann, da dies in Scikit-learn transparent implementiert ist. Zur
Auswahl stehen ein Grid Search und eine zufällige Suche mit begrenzter Tiefe.
Tests mit dem größten Datensatz (NCVoter) haben ergeben, dass die Zeit, die eine
Grid Search benötigt (2-4 Stunden), für die Evaluation vertretbar ist. Deshalb
wird die Klasse `GridSearchCV` verwendet. Für die Kreuzvalidierung wird der
Stratified K-Fold (implementiert in `StratifiedKFold`) verwendet, da dieser das
Verhältnis der Ground Truth beibehält.

Die als Klassifikator nutzbaren Komponenten müssen zur Grid Search kompatibel
sein. Das Scikit-learn Klassifikationsmodul beinhaltet dazu SVMs, DecisionTrees,
neuronale Netze und mehr. Diese Implementierungen können ohne Anpassungen mit
der Scikit-learn `GridSearchCV` verwendet werden. Die vorgestellten
Klassifikatoren in @sec:autolearn verwenden hauptsächlich DecisionTrees und
Support Vector Machines. Deshalb werden diese beiden für die Evaluation
eingesetzt. Die entsprechenden Scikit-learn Klassen sind
`DecisionTreeClassifier`, `SVC` für SVM mit RBF-Kernel und `LinearSVC` für SVM
mit Linearkernel.

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_index_bt.pdf}
        \caption{MDySimII vs MDySimIII - Bauzeit}
        \label{fig:IIvsIIIbt}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_memusg.pdf}
        \caption{MDySimII vs MDySimIII - Speicherverbrauch}
        \label{fig:IIvsIIImem}
    \end{minipage}
\end{figure}
```

Der Indexer kann durch den MDySimII oder den MDySimIII aus @sec:anaindxer
besetzt werden. Beide Indexer wurden auf dem ferbl-9k-1k-1 Datensatz mit Ground
Truth gegeneinander getestet, weil das MDySimII auf dem NCVoter-Datensatz nicht
in angemessener Zeit durchgeführt werden konnte. In @fig:IIvsIIIbt sind die
Bauzeiten der beiden Indexer verglichen. Aufgrund der komplexeren Struktur
schneidet der MDySimIII hier erwartungsgemäß leicht schlechter ab.
@fig:IIvsIIImem zeigt den Speicherbedarf beider Indexer. Überraschend ist, dass
der MDySimII fast das doppelte an Speicher benötigt als der MDySimIII. Durch die
komplexere Struktur war die Erwartung, dass der MDySimIII gegenüber dem MDySimII
schlechter abschneidet. Der Grund dafür kann in der Precision-Recall Kurve in
@fig:IIvsIIIprc abgelesen werden. Zwar erreicht der MDySimII einen Recall von
über 80 %, doch die Precision ist nahezu 0 %. Im Gegensatz dazu errreicht der
MDySimIII lediglich knapp 60 % Recall, dafür ist die Precision nahe 100 %. Die
schlechte Precision des MDySimII macht sich direkt in den Anfragezeiten in
@fig:IIvsIIIqry bemerkbar. Die durchschnittliche Anfragezeit für den MDySimII
liegt bei 10^-2^ und ist damit um 10^-2^ dramatisch schlechter als die des
MDySimIII mit 10^-4^. Der Datensatz ist mit 10.000 Einträgen relativ klein,
dennoch macht sich bereits hier ein deutlicher Unterschied bemerkbar, sowohl in
der Qualität als auch der Effektiviät. Aufgrund der schlechten Precision
skaliert der MDySimII nicht und ist daher für größerere Datensätze ungeeignet.
Deswegen wird in der weiteren Evaluation der MDySimIII als Indexer genutzt.

```tex
\begin{figure}
    \centering
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_prc.pdf}
        \caption{MDySimII vs MDySimIII - Precision-Recall Kurve}
        \label{fig:IIvsIIIprc}
    \end{minipage} \\
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_tc_query.pdf}
        \caption{MDySimII vs MDySimIII - Anfragezeiten}
        \label{fig:IIvsIIIqry}
    \end{minipage}
\end{figure}
```

## Auswahl der Freien Parameter {#sec:free_params}

Auf der Validierungsmenge wurden robuste Parameter für die freien Parameter zur
die Evaluierung gewählt. Robust bedeutet, dass diese nicht optimal für jeden
Datensatz sind, sondern gute Ergebnisse für alle Datensätze liefern und
gleichzeitig verhindern, dass die Entity Resolution katastrophal versagt. Um die
freien Parameter zu bestimmen, wurden diese anhand des NCVoter Datensatzes
ausprobiert und ausgewertet. Für jeden Datensatz wurden dazu die folgenden acht
Attribute genutzt: Geburtsdatum, Vorname, Nachname, Bundesstaat, Ort, PLZ,
Straße und Telefonnummer. Zunächst werden die Parameter des Blocking Schema
Lerners bestimmt, da das Blocking Schema zur Bewertung der Label Generator und
der Fusion-Lerner Parameter benötigt wird.

### Blocking Schema Lerner

Für den Blocking Schema Lerner aus @sec:ana_bs müssen folgende freie Parameter
bestimmt werden:

* Blockschlüsselgenerator
* Blockingprädikate
* Maximale Konjunktion und Disjunktion
* Blockfilter (Größe/Ratio)

#### Blockschlüsselgenerator

Von den drei unterschiedlichen Möglichkeiten zur Erzeugung zusammengesetzer
Blockschlüssel aus @sec:bkv_gen wurde im Rahmen dieser Thesis nur der
vorgestellte Algorithmus \ref{alg:bkvs} implementiert. Weshalb auch dieser für
die Evaluation genutzt wird.

#### Geeignete Prädikate

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_memusg.pdf}
        \caption{Arbeitsspeicherverbrauch}
        \label{fig:pred_mem}
    \end{minipage}\hfill
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_index_ips.pdf}
        \caption{Einfügeoperation pro Sekunde}
        \label{fig:pred_ips}
    \end{minipage}\\
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_query_ips.pdf}
        \caption{Anfragen pro Sekunde} \label{fig:pred_qps}
    \end{minipage}
\end{figure}
```

Die Blockingprädikate sind Hauptbestandteil eines Blocking Schema und haben
deswegen den größten Einfluss auf die Qualität und Effektivtät des
Gesamtsystems. Um geeignete auszuwählen, wurden folgende 6 Prädikate betrachtet.

* Identität eines Attributes (ID)
* Token eines Attributes die durch Leerzeichen getrennt sind (Tok)
* Prefixe eines Attributes, der Längen 2-4 (Pre)
* Suffixe eines Attributes, der Längen 2-4 (Suf)
* Bigram eines Attributes, sind n-Gramme der Länge 2 (Bi)
* Trigram eines Attributes, sind n-Gramme der Länge 3 (Tri)

Da die Blockschlüsselerzeugung über ID offensichtlich am Effektivsten ist,
wurden alle Kombinationen, der anderen Prädikate, mit ID getestet. Zusätzlich
wurden (Prefix, Sufix), (Bigram, Trigram) und (ID, Token, Prefix, Suffix,
Bigram, Trigram) getestet. Da einige dieser Kombinationen einen deutlichen
Einfluss auf die Effektivtät haben wurde aus Zeitgründen der Febrl-4k-1k
Datensatz genutzt.

In @fig:pred_mem ist der Arbeitsspeicherverbrauch des gebauten Indexes
dargestellt. Das speicherhungriste Blocking Schema wurde durch (ID, Trigram)
erzeugt, welches lediglich aus Trigrammen besteht. Das speichersparenste Schema
wird durch (ID, Token) erzeugt, welches auch aus beiden Prädikaten besteht. In
[@fig:pred_ips, @git:pred_qps] sind die Einfügeoperationen und Anfragen pro
Sekunde dargestellt. Dabei zeigt sich, dass (ID, Token) mit deutlichem Abstand
in beiden Abbildungen dominiert. Dies ist zum einen auf die geringerer Anzahl
der Blockschlüssel zurückzuführen, die pro Attribute erzeugt werden und zum
anderen auf die deutlich bessere Pairs Quality (PQ), welche in @tbl:pred_qual zu
sehen ist. Hierbei erreicht (ID, Token) einen Wert von 0.96, die nächstbeste
Kombination (Bi-Tri) kommt lediglich auf 0.33. Die hohe Pairs Quality kommt
allerdings auf Kosten der Pairs Completeness (PC), welche von allen Kombinationen
mit 0.83 die schlechteste ist. Mit anderen Kombination sind hier bis zu 0.99 PC
möglich, beispielsweise über (Prefix, Suffix). Für die weitere Evaluation werden
die beiden Prädikate ID und Token ausgewählt, da trotz leicht schlechterer
Qualität, die Effizienz deutlich besser ist.

|    | Bi-Tri | ID-Bi | ID-Pre | ID-Tok-Pre-Suf-Bi-Tri | ID-Suf | ID-Tok | ID-Tri | Pre-Suf |
|----+-------:+------:+-------:+----------------------:+-------:+-------:+-------:+--------:|
| PC |   0.97 |  0.90 |   0.99 |                  0.99 |   0.97 |   0.83 |   0.95 |    0.99 |
| PQ |   0.33 |  0.09 |   0.33 |                  0.14 |   0.09 |   0.96 |   0.28 |    0.18 |

: Pairs Completeness und Pairs Quality der Prädikatkombinationen
{#tbl:pred_qual}

#### Maximale Konjunktion/Disjunktion

![Bauzeiten des Indexers bei unterschiedlicher maximaler Länge der Konjunktion
der Ausdrücke des Blocking Schema.
](./images/dnf/MDySimIII_index_bt.pdf){#fig:dnf_bt width=50%}

Die maximale Konjunktion $max_k$ von spezifischen Blockingprädikaten zu
Ausdrücken und die maximale Disjunktion $max_d$ von Ausdrückenkönnen
entscheidend sein, um ein gutes Blocking Schema zu bilden. Um einen Ausdruck zu
bewerten, müssen durch den Indexer die Blöcke gebaut werden, was relativ
zeitintensiv ist, dahingehend werden für die Disjunktion der Ausdrücke nur
boolesche Vektoren verodert und miteinander verglichen, was deutlich effizienter
ist. In [@KM:Unsupervised:13] geben Kejriwal & Miranker an, in der Evaluation
ihres DNF Blocking Schema Lerners, keine Verbesserung für $max_k > 2$ gemessen
zu haben. Im Gegensatz dazu wird $max_d$ von Ihnen nicht beschränkt, über die
maximal gemessene Disjunktion keine Aussage gemacht. Für die Evaluation des
Algorithmus aus @sec:ana_bs wurde $max_d$ für alle Durchläufe auf 5 gesetzt,
um diesem etwas Spielraum zu gewähren. Für $max_k$ wurden die Werte 1, 2 und 3
getestet. Dazu wurden Ausdrücke auf 8 Attributen des NCVoter Datensatzes mit
jeweils 2 spezifischen Blockingprädikaten gebildet. Die Anzahl der gebildeten
Ausdrücke beträgt für $max_k = 1$ gleich 16, für $max_k = 2$ gleich 136, dass
sind 8.5 Mal mehr Ausdrücke und für $max_k = 3$ gleich 696 Ausdrücke, was
nochmals 5 Mal soviel sind. Dabei ist zu beachten, dass $max_k$ jeweils die
Ausdrücke von $max_k - 1$ beinhaltet. Die Anzahl der zu prüfenden Ausdrücke, hat
deutlichen Einfluss auf die Lernzeit. Bei $max_k = 1$ dauert das Lernen nur 22
Minuten, bei $max_k = 2$ dauert es 5.5 Mal solange mit 2 Stunden und bei $max_k
= 3$ dauert es nochmal 12.5 Mal solange mit einem Tag und einer Stunde. Während
von $max_k = 2$ auf $max_k = 3$ die Anzahl der Ausdruck um das fünffache wächst,
ist dies bei der Lernzeit über das 12fache. Dies lässt sich mit den Bauzeiten
des Indexers erklären. In @fig:dnf_bt sind die Bauzeiten des jeweils besten
Blocking Schema in dem jeweiligen Durchlauf dargestellt. Dabei besteht das beste
Blocking Schema in allen drei Fällen stets aus Ausdrücken der Länge $max_k$. Die
Bauzeiten verraten, dass je länger die Ausdrücke werden, desto länger benötigt
der Indexer zum erzeugen der Blockschlüssel, wodurch sich die Bauzeit
verlängert. Da die Blockschlüssel auch für jede Anfrage erzeugt werden,
verlängern sich diese ebenfalls. Während bei $max_k = 1$ noch 50k Anfragen/s
beantwortet werden, sind es bei $max_k = 2$ nur noch 22k Anfragen/s und bei
$max_k = 3$ lediglich noch 13k Anfragen/s. Neben der Effizienz muss allerdings
auch die Qualität überzeugen, was bei $max_k = 1$ nicht der Fall ist. Die Pairs
Completeness beträgt lediglich 0.16 und die Pairs Qualtity 0.1. Für $max_k = 2$
liegt der Recall dafür bei guten 0.95, auch die Precision ist mit 0.13 leicht
besser. Für $max_k = 3$ kann noch ein wenig auf 0.98 verbessern, die Precision
bleibt jedoch mit 0.13 gleich. Sowohl $max_k = 2$ als auch $max_k = 3$ bieten
einen guten Recall, da $max_k = 2$ jedoch deutlich effizienter ist, wird dieser
Wert für die Evaluation genutzt. Die maximale Disjunktion lag bei 3 in
Verbindung mit $max_k = 3$, weshalb für $max_d$ der Wert 3, für die Evaluation
ausreichend erscheint.

<!-- 1k - 22min 6s (1326) - 16 - 52969 Queries/s - 0.158681/0.100261 (R/P) -->
<!-- 2k - 2h 1m 18s (7278) x 5.5 - 136 x 8.5 - 22117 Queries/s - 0.954298/0.126349 (R/P) -->
<!-- 3k - 1d 1h 4m 38s (90278) x 12.4 - 696 x 5.1 -  13203 Queries/s - 0.984595/0.126973 (R/P) -->

#### Blockfilter

![Precision-Recall Kurve
](./images/fp_dnf/MDySimIII_ncvoter_block_filters_prc.pdf){#fig:tvsg_prc
width=50%}

In Abschnitt @sec:eval_dnflearner wurden zwei Filter für den Blocking Schema
Lerner eingeführt, die dafür sorgen, dass offensichtlich schlechte Ausdrücke und
schlechte Blockschlüssel nicht im Detail betrachtet werden, da dies zur
Überlastung der Arbeitsspeicherkapazitäten führen kann. Dazu gibt es eine
Schwelle $t$, ab welcher ein Block mit mehr Einträgen als schlechter Block
behandelt wird. Ist die Anzahl der Einträge in guten Blöcken prozentual kleiner
als $g$ (minimale gute Blockrate) wird der komplette Ausdruck verworfen.
Andernfalls werden Blockschlüssel größer $t$ verboten und nur die Blöcke kleiner
$t$ benutzt. Bei den meisten schlechten Blockschlüsseln handelt es sich um
Stoppwörter, die in der Vorverarbeitung nicht korrekt aussortiert wurden. In der
Evaluation wurde $t$ für die Werte 25, 50, 100, 200, 500 und 1000 jeweils auf
$g$ 0.75, 0.8, 0.85, 0.9, 0.95 und 1 angewandt. @tbl:tvsg_pr zeigt Recall und
Presion für alle Kombinationen von $g$ und $t$. Für $t$ gleich 500 und 1000
konnten keine Ergebnisse ausgewertet werden, da diese zu Abbruchen aufgrund zu
hoher Speicheranforderungen geführt haben. Weitere Abbrüche gab es für $t$
gleich 200 in Verbindung mit $g$ gleich 0.75 und 0.85. Zudem Fallen die Werte
mit $g$ gleich 1.0 aus der Reihe, da offensichtlich nicht genügend Blöcke diesem
Verhältnis genüge tragen können. Die übrigen Paarungen beschränken sich auf zwei
Blocking Schemata. Eines mit schlechtem Recall und mittelmäßiger Precision und
eines mit gutem Recall und schlechter Precision. Mit zunehmender minmaler guter
Blockrate verliert das zweite Blocking Schema im F-measure gegenüber dem Ersten.
Über einen Klassifikator kann die Precision noch verbessert werden, der Recall
jedoch nicht. Deswegen ist das zweite Blocking Schema mit höherem Recall zu
bevorzugen. In @fig:tvsg_prc ist die Precision-Recall Kurve für das zweite
Blocking Schema mit steigender maximaler Blockgröße abgebildet. Je größer $t$
desto größer ist entsprechend auch der Recall, da weniger Blockschlüssel
verboten werden. Allerdings fällt gleichzeitig die Precision. Als maximale
Blockgröße, die auch für alle gewählten $g$ funktioniert, bietet sich daher 100
an. Für die minimale gute Blockrate wird 0.85 gewählt, da dort jeweils nach
unten und oben noch Puffer ist, in welchem das präferierte Blocking Schema
ebenfalls ausgewählt wurde. Für deutlich größere Datensätze als der NCVoter,
wird die Blockgröße von 100 vermutlich zu Verschlechterungen der Effizienz
führen, da allerdings in der Evaluation kein größerer Datensatz genutzt wird,
ist die gewählte Blockgröße vertretbar.

| t\\g | 0.75      | 0.80      | 0.85      | 0.90      | 0.95      | 1.0       |
|------+-----------+-----------+-----------+-----------+-----------+-----------|
| 25   | 0.87/0.27 | 0.16/0.47 | 0.16/0.47 | 0.16/0.47 | 0.16/0.47 | 0.06/0.57 |
| 50   | 0.92/0.18 | 0.92/0.18 | 0.92/0.18 | 0.16/0.44 | 0.16/0.44 | 0.06/0.57 |
| 100  | 0.95/0.13 | 0.95/0.13 | 0.95/0.13 | 0.95/0.13 | 0.16/0.41 | 0.14/0.25 |
| 200  |           |           | 0.98/0.09 | 0.98/0.09 | 0.98/0.09 | 0.14/0.25 |
| 500  |           |           |           |           |           |           |
| 1000 |           |           |           |           |           |           |

: Tabelle mit Recall und Presion für unterschiedliche maximale Blockgrößen und
minimale gute Blockrate. {#tbl:tvsg_pr}

### Label Generator

Die freien Parameter des Labelgenerators sind die Fenstergröße, die untere
und obere Schwelle, sowie die maximalen Matches und Non-Matches. Dabei werden
die Schwellen nur benötigt falls keine Ground Truth existiert und diese vom
Label Generator selbstständig erzeugt wird.

#### Fenstergröße

Zur Bestimmung einer geeigneten Fenstergröße wurde der Label Generator ohne
Matches betrachtet (vgl. @sec:ana_lbl). Diese Variante reagiert, im Vergleich
zur Variante mit Matches, deutlich empfindlicher auf die unterschiedlichen
Parameter, wodurch der Effekt der unterschiedlichen Fenstergrößen $w$ einfacher
ausgewertet werden kann. Dazu wurde die untere Schwelle $lt$ und die obere
Schwelle $ut$ in 0.1 Schritten bis 0.5 erhöht und mit den Fenstergrößen 2, 5, 10
und 20 verglichen. Die maximalen Matches wurden mit 10 % der Gesamtmenge und die
maximalen Non-Matches mit 25 % der Gesamtmenge bestimmt. Bei dem genutzten
NCVoter Datensatz sind die maximalen Matches bei 551k und die maximalen
Non-Matches bei 1.337k. In den [@tbl:w2;@tbl:w5;@tbl:w10;@tbl:w20;] sind die
Ergebnisse für die unterschiedlichen Fenstergrößen dargestellt. Die obere
Schwelle $ut$ hatte dabei keine entscheidende Auswirkung, sodass lediglich die
untere Schwelle $lt$ betrachtet wird. Für jede Schwelle wurden Matches (P),
Non-Matches (N), Pairs Completeness (PC) und Pairs Qualtity (PQ) analysiert. Die
Pairs Quality liefert keinen entscheidenden Hinweis auf ein geeignetes Fenster,
da diese sich lediglich zwischen 1 % und 11 % hin und her bewegt. Dies bedeutet
zwar einen Performanzunterschied, welcher jedoch nicht ausschlaggebend
signifikant ist. Beim Blick auf die Pairs Completeness zeigt sich, dass diese
sich zwischen 13 % und 16 % für alle Fenstergrößen bewegt, mit Außnahme von
$w=2$. Dort ist die Pairs Completeness für $lt \leq 0.3$ mit 95 % deutlich
besser. Für $w=2$ werden am wenigsten Paare gebildet, allerdings sind die Paare
die gebildet werden, die mit der höchsten Ähnlichkeit zueinander, da nur
Blocknachbarn betrachtet werden. Aufgrund dessen werden viele deutlich
verschiedene Paare ausgeschlossen, wie sich bei $lt=0.1$ bemerkbar macht, da
hier lediglich 8k Non-Matches generiert wurden. Bei $lt=0.2$ gibt es mit 655k
dann allerdings schon eine große Auswahl an Non-Matches und mit $lt=0.3$ wurden
bereits mehr Paare generiert als das Maximum. Aufgrund dieser Ergebnisse wird
die Fenstergröße für die weitere Evaluation mit 2 bestimmt.

```tex
\begin{table}
\centering
\begin{minipage}{.4\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=2) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 8k     & 0.95 & 0.11 \\
0.2      & 551k & 655k   & 0.95 & 0.11 \\
0.3      & 551k & 1,377k & 0.95 & 0.11 \\
0.4      & 433k & 1,377k & 0.15 & 0.10 \\
0.5      & 433k & 1,377k & 0.16 & 0.10 \\ \bottomrule
\end{tabular}
\caption{Ergebnis mit Fenstergröße 2}\label{tbl:w2}
\end{minipage}\hfill
\begin{minipage}{.4\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=5) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 32k    & 0.15 & 0.01 \\
0.2      & 551k & 1,377k & 0.15 & 0.01 \\
0.3      & 551k & 1,377k & 0.15 & 0.01 \\
0.4      & 551k & 1,377k & 0.16 & 0.11 \\
0.5      & 551k & 1,377k & 0.16 & 0.11 \\ \bottomrule
\end{tabular}
\caption{Ergebnis mit Fenstergröße 5}\label{tbl:w5}
\end{minipage}\\
\begin{minipage}{.4\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=10) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 66k    & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.13 & 0.13 \\ \bottomrule
\end{tabular}
\caption{Ergebnis mit Fenstergröße 10}\label{tbl:w10}
\end{minipage}\hfill
\begin{minipage}{.4\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=20) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 119k   & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.13 & 0.12 \\ \bottomrule
\end{tabular}
\caption{Ergebnis mit Fenstergröße 20}\label{tbl:w20}
\end{minipage}\hfill
\end{table}
```

#### Untere und Obere Schwelle

![Ähnlichkeitsverteilung der TF/IDF Ähnlichkeiten der Ground Truth des NCVoter
Datensatzes, welche anhand der tatsächlichen Matches erzeugt wurde. Die
Datensätze Matches bzw. Non-Matches wurden in 5 % Schritten nach Ähnlichkeit
zusammengefasst.](./images/ncvoter_matches_histo.pdf){#fig:match_histo}

Die untere Schwelle $lt$ legt fest, bis zu welchem Ähnlichkeitswert Paare als
Non-Matches betrachtet werden und die obere Schwelle $ut$ legt fest, ab welchem
Ähnlichkeitswert Paare als Matches betrachtet werden, dabei gilt stets $lt \leq
ut$. In einem Experiment wurden $lt$ und $ut$ in 0.1 Schritten betrachtet und so
alle Konfigurationen bis 1.0, auf dem NCVoter-Datensatz ausprobiert. Für das
Fenster wurde der bereits bestimmte Wert von 2 gesetzt. Zur Auswertung wurden
Pairs Completeness, Pairs Quality und die Ground Truth analysiert. Anhand der
Pairs Completeness und Pairs Qualtity kann betrachtet werden, wie gut ein
Blocking Verfahren auf der generierte Ground Truth funktioniert. Durch die
gefilterte Ground Truth hingegen kann herausgefunden werden, wie viele Ground
Truth Paare für den Fusion-Lerner zur Verfügung stehen. Die [@tbl:recall;
@tbl:fp; @tbl:fn] betrachten nacheinander die Pairs Completeness, die Matches
und die Non-Matches. Die Pairs Quality ist uninteressant, da deren Werte relativ
konstant bei 0.1 liegen, mit einer Varianz von 0.03. In @tbl:recall ist gut zu
sehen, dass die Pairs Quality zwischen einer $ut$ von 0.1 und 0.4 immer eine
gute Pairs Qualtity von 95 % erzeugt. Der Blick auf das erlernte Blocking Schema
zeigt, dass dieses auch immer dasselbe ist. Dies trifft auch noch teilweise für
$ut=0.5$ zu, allerdings nur für $lt \leq 0.3$. Für alle $ut > 0.5$ variieren die
Blocking Schemata, wobei unabhängig von $lt$ keines über 17 % Pairs Completeness
kommt. In @fig:match_histo sind ist die Ähnlichkeitsverteilung der Ground Truth,
die aus den tatsächlichen Matches erzeugt wurde, für Matches und Non-Matches
dargestellt. Der Großteil der Matches hat eine Ähnlichkeit zwischen 0.2 und 0.5,
wohingegen sich der Großteil der Non-Matches zwischen 0.1 und 0.3 befindet. Für
$ut > 0.5$ fällt der Recall dramatisch ab, weil ein Großteil der Matches nicht
mehr erfasst wird. Für $ut = 0.5$ sind die Recallwerte für $lt \leq 0.3$ noch
gut. Wird $lt$ weiter erhöht fällt der Recall, da sich nun zu viele tatsächliche
Matches in den Non-Matches befinden.

Deshalb werden in den [@tbl:fp; @tbl:fn] lediglich $ut$-Werte kleiner 0.6
betrachtet. Die maximalen Matches, die der Label Generator erzeugen darf, liegen
bei 10 % der Gesamtmenge und betragen 551k. Bei den Non-Matches ist das Limit 25
% und damit 1,3 mio. Für die künstliche Anreicherung der gefilterten
Non-Matches, stehen jedoch alle erzeugten Non-Matches zur Verfügung,
dementsprechend je höher $lt$ desto mehr Non-Matches und umgekehrt für $ut$. In
@tbl:fp ist zu sehen, dass für $ut \leq 0.4$ die Ausgangsmenge der Matches auf
das Maximum beschränkt wurde. Da jeweils die Matches mit der höchsten
Ähnlichkeit genutzt werden, sind diese Mengen identisch, weshalb auch die
gefilterte Mengen mit jeweils 300k Datensätzen, aufgrund desselben Blocking
Schema, identisch sind. Für $ut=0.5$ ist die Ausgangsmenge kleiner als das
Maximum. Die gefilterte Menge beträgt in diesem Fall 288k. In jedem Fall sind
nach dem Filtern genügend Matches vorhanden, um einen Klassifikator zu
trainieren, auch wenn diese 6-Mal soviele Matches beinhaltet, wie die
tatsächlichen Matches.

In @tbl:fn wird die Anzahl der Non-Matches dargestellt. Für $lt=0.1$ sind
insgesamt nur 8k Paare erzeugt worden, weil das TF/IDF Blocking die meisten der
sehr unähnlichen Paare ausschließt. Nach dem Filtern durch das Blocking Schema
sind keine Non-Matches mehr vorhanden, da das Blocking Schema verhindert, dass
diese offensichtlichen Non-Matches zusammen gruppiert werden. Für $lt=0.2$ gibt
es ein ähnliches Bild. Zwar ist die Anzahl der Ausgangsmenge mit 655k deutlich
höher, dennoch werden lediglich 66 Paare gefunden, die zusammen gruppiert und
damit nicht ausgefiltert wurden. Interessanter wird es erst ab $lt=0.3$. Hier
wird erste Mal das Maximum der Ausgangsmenge mit 1377k erreicht. Die gefilterten
Non-Matches betragen 2430, was im Vergleich zu den Matches immer noch sehr wenig
ist, aber durchaus genügt um einen Klassifikator zu trainieren. Mit $lt=0.4$
erhöht sich diese Anzahl nochmals, um das 5-fache. Ein Blick auf
@fig:match_histo zeigt, dass sich dadurch der Großteil der Matches in den
Non-Matches befindet. Die Linien für Matches und Non-Matches schneidet sich in
im Bereich 0.1 - 0.5 bei 0.28. Unterhalb gehen viele Non-Matches verloren und
oberhalb viele Matches. Deshalb wird sowohl $lt$ als auch $ut$ auf 0.3
festgelegt, da ab hier auch genügend Paare zum Trainieren eines Klassifikators
zur Verfügung stehen.

```tex
\begin{table}
\centering
\begin{tabular}{rrrrrrrrrrr}\toprule
PC  & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & 0.8  & 0.9  & 1.0  \\ \midrule
0.1 & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.2 &      & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.3 &      &      & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.16 & 0.13 & 0.13 \\
0.4 &      &      &      & 0.95 & 0.15 & 0.16 & 0.16 & 0.16 & 0.14 & 0.14 \\
0.5 &      &      &      &      & 0.16 & 0.16 & 0.13 & 0.14 & 0.14 & 0.06 \\
0.6 &      &      &      &      &      & 0.13 & 0.14 & 0.14 & 0.16 & 0.06 \\
0.7 &      &      &      &      &      &      & 0.14 & 0.10 & 0.16 & 0.06 \\
0.8 &      &      &      &      &      &      &      & 0.10 & 0.16 & 0.06 \\
0.9 &      &      &      &      &      &      &      &      & 0.14 & 0.06 \\
1.0 &      &      &      &      &      &      &      &      &      & 0.06 \\ \bottomrule
\end{tabular}
\caption{Pairs Completeness}\label{tbl:recall}
\begin{tabular}{rrrrrrr}\toprule
Matches & 0.1       & 0.2       & 0.3       & 0.4       & 0.5   \\ \midrule
0.1 & 551k/300k & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.2 &           & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.3 &           &           & 551k/300k & 551k/300k & 443k/288k \\
0.4 &           &           &           & 551k/300k & 443k/287k \\
0.5 &           &           &           &           & 443k/273k \\ \bottomrule
\end{tabular}
\caption{Matches}\label{tbl:fp}
\begin{tabular}{rrrrrrr}\toprule
Non-Matches & 0.1     & 0.2        & 0.3         & 0.4         & 0.5        \\ \midrule
0.1         & 8k/0    & 8k/0       & 8k/0        & 8k/0        & 8k/0       \\
0.2         &         & 655k/66    & 655k/66     & 655k/66     & 655k/66    \\
0.3         &         &            & 1377k/2430  & 1377k/2430  & 1377k/2430 \\
0.4         &         &            &             & 1377k/13916 & 1377k/11k  \\
0.5         &         &            &             &             & 1377k/8k   \\ \bottomrule
\end{tabular}
\caption{Non-Matches}\label{tbl:fn}
\end{table}
```

#### Maximale Paare der Ground Truth

Sowohl mit, also auch ohne Ground Truth Match werden dem Label Generator
mitgeteilt, wie viele Matches $max_p$ bzw. Non-Matches $max_n$ in der Ground
Truth enthalten sein dürfen. Im Fall ohne Ground Truth Matches werden jeweils
die $max_p$ Matches bzw. $max_n$ Non-Matches ausgewählt, die die höchste
Ähnlichkeit haben. Im Fall mit Ground Truth Matches werden sowohl Matches als
auch Non-Matches nach ihrer Ähnlichkeitsverteilung ausgewählt. Mithilfe dieser
Ground Truth sucht der Blocks DNF Generator nach dem besten Blocking Schema.
Anschließend wird die Ground Truth, anhand des Blocking Schema, gefiltert,
sodass diese nur Paare enthält, die einen gemeinsamen Blockschlüssel haben. In
diesem Schritt werden sehr viele Non-Matches herausgefiltert, da das der primäre
Zweck des Blocking Schema ist. Damit für den Fusion-Lerner genügend Non-Matches
zur Verfügung stehen, werden die Non-Matches mit allen generierten Non-Matches
des Label Generators angereichert. Hierbei spielt $max_n$ keine Rolle. Die
Auswirkungen dieser Parameter wurden auf dem NCVoter Datensatz mit Ground Truth
und ohne Ground Truth mit Fenstergröße $w=2$ und Schwellen $lt=ut=0.3$
evaluiert. Ausgangssituation ist (0.1, 0.25) mit $max_p$ 10 % und $max_n$ 25 %
der Gesamtmenge, welche bereits zur Ermittlung der Fenstergröße und der
Schwellen genutzt wurden. Bei allen getesteten Paarungen ist das Limit der
Non-Matches höher als das der Matches, um das Verhältnis im Datensatz zu
repräsentieren. Getestet wurden zunächst größere Werte mit (0.5, 2.5), (1, 5),
(5, 25), (10, 50). Dabei wuchsen die Matches leicht auf 650k und die Non-Matches
bis stark auf 19 Mio. Auf das Blocking Schema hat die vergrößerte Ground Truth
in keinem Fall einen Einfluss. Die Betrachtung der Matches im größten Fall (10,
50) ergibt, dass diese sich lediglich um ca. 100k verändert hat. Die neu
hinzugekommen Matches wurden durch die Filterung auf ein paar Hundert reduziert.
Das bedeutet, dass die erweiterten Matches die Ausdrücke des Blocking Schema, im
Bezug auf Pairs Completeness, abwerten und zwar je mehr desto größer die Anzahl
der Matches. Allerdings haben sich die Non-Matches mit 19 mio. dramatisch
erhöhtet und durch die Filterung bleiben lediglich die bekannten 2k übrig,
sodass die erhöhte Menge sich positiv auf das Reduction Ratio und die Pairs
Qualtity auswirkt. Damit wird der Negativeffekt auf die Pairs Completeness
aufgehoben und das Blocking Schema bleibt dasselbe. Neben größeren Werten wurden
auch kleinere getestet (0.0001, 0.00025), (0.001, 0.0025), (0.01, 0.025). Für
(0.01, 0.25) werden in etwa so viele Matches ausgewählt wie tatsächlich
enthalten sind. Recall und Precision sind allerdings mit 15 % und 0.1 % sehr
schlecht. Dasselbe Ergebnis zeigt sich bei den noch kleineren Paaren. Daraus
folgt, dass das Ausgangspaar (0.1, 0.25) bereits gut gewählt war und auch
robuste Werte liefert.

### Fusion-Lerner {#sec:fp_fusion}

Für den Fusion-Lerner müssen drei Freie Parameter bestimmt werden. Zunächst die
Suchverfahren Grid Search oder Randomized Search. Spezialisierte Verfahren
werden nicht näher betrachtet. Anschließend die Kreuzvalidierung, aus
Effizienzgründen eine nicht-vollständige, K-Fold oder Stratified K-Fold. Sowie
das Qualitätsmaß zur Bewertung der trainierten Modelle.

* Tests mit beiden Suchverfahren haben ergeben, dass die Lernzeit der Randomized
  Search gegenüber der Grid Search, auf dem NCVoter Datensatz, kleiner einer
  Stunde ist. Dieser Mehraufwand ist für die Evalution vertretbar, da dadurch
  stets die besten Parameter gefunden werden.
* Bei der Kreuzvalidierung sollte der Datensatz bestmöglich representiert sein.
  Dazu muss allerdings das Verhältnis von Matches zu Non-Matches aus der Ground
  Truth eingehalten werden. Deshalb wird hierfür das Stratified K-Fold Verfahren
  genutzt. Die Anzahl für $K$ wird mit 3 bestimmt, was der Standardwert in
  Scikit-learn ist.
* Ein Qualitätsmaß muss sowohl Recall als auch Precision berücksichtigen,
  deshalb wurden hierzu das F-measure und die Average Precision betrachtet. In
  mehreren Durchläufen wurden die beiden Maße jeweils für einen Klassifikator
  genutzt. Dabei wurde zunächst festgestellt werden, dass Recall und Precision
  immer gleich sind, unabhängig davon welcher Klassifikator und welche Parameter
  durch die Grid Search bestimmt wurden. Beim Vergleich der Ergebnisse gegen
  eine Baseline, bei welcher die Kandidatenmenge $C$ gleich Ergebnissmenge $R$
  entspricht, wurde festgestellt, dass die Precision dieselbe ist und der Recall
  um 13 % niedriger. Zweck des Klassifikator ist es jedoch, möglichst ohne
  Verlust des Recalls, die Precision so nahe wie möglich an 1 zu bringen.
  Aufgrund dieses Ergenisses, kann keine endgültige Entscheidung über das
  Qualitätsmaß getroffen werden. Dazu wird in @sec:base_par_full genau
  analysiert, worin das Problem besteht. Da für diese Untersuchung jedoch ein
  Qualitätsmaß benötigt wird, wird zunächst das F-measure gewählt, welches
  bereits für das Blocking Verfahren gute Dienste leistet.

## Baseline vs Teilvektor vs Vollvektor {#sec:base_par_full}

![PRC
](./images/basevsparvsfull/full-svm-linear_full-dtree_par-dtree_par-svm-rbf_full-svm-rbf_par-svm-linear_prc.pdf
){#fig:baseparfull_prc}

Der Grund für das Versagen der Klassifikatoren, bei der Auswahl der freien
Parameter des Klassifikators, liegt vermutlich an der reduzierten Menge von
Ähnlichkeitswerten, die der MDySimIII für Kandidatenpaare zurückgibt, da nur
Ähnlichkeiten für Attribute angegeben werden, die einen gemeinsamen Block haben.
Beim Blick auf die Kandidatenmenge, wurde festgestellt, dass dies für fast alle
Paare lediglich ein Attribut ist. Eine modifizierte Variante des MDySimIII
Indexer berechnet deshalb, während der Anfrage, die fehlenden Ähnlichkeiten
zwischen den Paaren in der Kandidatenmenge $C$. Die folgenden vier Konfiguration
werden hierzu evaluiert:

* Teilbaseline, MDySimIII ohne Klassifikator mit vorausberechneten Ähnlichkeiten
* Vollbaseline, MDySimIII ohne Klassifikator mit voller Ähnlichkeitsberechnung
* Teilvektor, MDySimIII mit Klassifikator mit vorausberechneten Ähnlichkeiten
* Vollvektor, MDySimIII mit Klassifikator mit voller Ähnlichkeitsberechnung

Alle vier Konfigurationen nutzen dazu eine Ground Truth mit vorklassifizierten
Matches und die vom Similarity Lerner bestimmten Ähnlichkeitsmetriken sind
identisch Konfigurationen. Des Weiteren wurden Teilvektor und Vollvektor jeweils
mit Decision Tree (dtree), SVM mit RBF-Kernel (svm-rbf) und SVM mit Linearkernel
(svm-linear) getestet.

![Anfragen pro Sekunde
](./images/basevsparvsfull/MDySimIII_query_ips.pdf){#fig:baseparfull_qps
width=60%}

@fig:baseparfull_prc zeigt die Precision-Recall Kurven für die Teilvektoren
(Abk. par) und Vollvektoren (Abk. full). Die Precision-Recall Kurven wurden
erstellt, indem die Teilähnlichkeiten aufsummiert wurden. Die Kurven der
Teilvektoren sind dabei identisch. Es gibt zum einen Punkte in der linken oberen
Ecke und der rechten unteren, wobei ein Punkt einem Ähnlichkeitswert entspricht.
Haben mehrere Paare denselben Ähnlichkeitswert, werden diese in einem Punkt
zusammengefasst. Dementsprechend gibt es Paare mit sehr niedriger Ähnlichkeit
(links oben) und zum anderen Paare mit sehr hoher Ähnlichkeit (rechts unten),
allerdings nichts dazwischen. Dementsprechend hat der Klassifikator fast
auschließlich nur ein Attribut und einen binären Ähnlichkeitswert, um eine
Entscheidung zu treffen, was die schlechten Werte aus @sec:fp_fusion erklärt. Im
Gegensatz dazu bietet der Vollvektor dem Klassifikator deutlich mehr
Unterscheidungsmerkmale für die Paare. Dies lässt sich in den Precision-Recall
Kurven daran erkennen, dass diese vollständig mit Punkten besetzt ist. Der
Klassifikator, der mit den vollbesetzen Vektoren am besten abschneidet, ist die
SVM mit Linearkernel. Dennoch verliert selbst dieser mit 73 % einen großen
Anteil an Recall, gegenüber 95 % bei der Baseline. Dafür ist die Precsion mit 55
% deutlich besser als die 12 % der Baseline.

In @fig:baseparfull_qps werden die Konsequenzen auf die Effektivität in Anfragen
pro Sekunde, der verschieden Konfiguration dargestellt. Mit deutlichem Abstand
am meisten Anfragen pro Sekunde können durch die Baseline ohne Klassifikator und
mit vorausberechneten Ähnlichkeiten erzeilt werden. Bei der Baseline mit voller
Ähnlichkeitsberechnung können knapp 80% weniger Anfragen beantwortet werden.
Aber nicht ausschließlich die Ähnlichkeitsberechnung benötigt viel Zeit. Wie bei
den Teilvektorkonfigurationen zu sehen bremst die Klassifikation ebenfalls
erheblich, sogar stärker als die Ähnlichkeitsberechnung. Klassifikation zusammen
mit voller Ähnlichkeitberechnung sorgt dafür, dass die Anfragen pro Sekunde
sogar über 90 % sinken. Gegenüber den Teilvektoren sind die Vollvektoren
zwischen 28 % und 49 % weniger effizient. Wobei der Decision Tree am schnellsten
und die SVM mit RBF-Kernel am langsamsten ist. Aufgrund der gemessenen Qualität
ist die Teilvektorkonfiguration ungeeignet, da hier ohne Klassifikator bessere
Ergebnisse erzielt werden. Für den weiteren Verlauf wird deshalb der MDySimIII
mit voller Ähnlichkeitsberechung genutzt, trotz der schlechteren Effizienz.

Beim Auswerten der Daten gab es noch eine Unstimmigkeit. Und zwar wurden als
Ähnlichkeitmaß für alle Attribute die Bag-Distanz ausgewählt. Ein genauer Blick
auf die vom Similarity Lerner berechnete Average Precision zeigt, dass diese
denselben Wert pro Attribut für alle getesteten Ähnlichkeitsmaße hat. Die
Bag-Distanz wurde dementsprechend nur ausgewählt, weil diese zuerst getestet
wurde. Dieses Verhalten des Similarity Lerners wird in @sec:simlearnvsmanual, in
Bezug auf den MDySimIII mit Klassifikator und voller Ähnlichkeitsberechung,
analysiert.

## Einfluss der Ähnlichkeitmetriken {#sec:esim_metrik}

![PRC SVM Linearkernel](./images/simtest/linear_prc.pdf){#fig:sim_prc}

Die Auswertung in @sec:base_par_full, in Bezug auf den Similarity Lerner,
erweckt den Eindruck, dass die Ähnlichkeiten keinen Einfluss haben, da alle
dieselbe Average Precision bekommen. Deshalb werden jeweils für die drei
Klassifikatoren unterschiedliche Ähnlichkeitsmaße getestet, indem ein
Ähnlichkeitsmaß pro Durchlauf für jedes AtTribut vorgegeben wird. Getestet
wurden folgende Ähnlichkeitsmaße

* Bag-Distanz
* Levenshtein-Distanz
* Jaro-Distanz
* Jaccard-Koeffizent

In @fig:sim_prc sind die Precision-Recall Kurven der Ähnlichkeitsmaße mit der
SVM mit Linearkernel dargestellt. Darauf ist deutlich zu erkennen, dass das
ausgewählte Ähnlichkeitsmaß deutlichen Einfluss auf die Qualtität des
Klassifikator hat.

=> TBD

## Human Baseline

* Train/Train
* Train/Test

## Grund Truth vs No Ground Truth

* Train/Train
* Train/Test
