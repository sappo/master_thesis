# Evaluierung der Qualtiät und Effektivität

## Berechnung der Metriken für dynamisches Enity Resolution

Während im statischen Entity Resolution, die Metriken (vgl. @sec:measurements)
am Ende des Verfahrens einmalig berechnet werden können, ist das im dynamischen
Falle nicht möglich, da es theoretisch kein Ende gibt. Das bedeutet, die
Metriken müssen inkrementell mit jeder Anfrage $q$ erhoben werden. Zur
Berechnung der Effektivitätsmaße Pairs Completeness, Pairs Quality und Reduction
Ratio werden die tatsächlichen Matches $n_M$, die tatsächlichen Non-Matches
$n_N$, die Matches in der Kandidatenmenge $s_M$ und Non-Matches in der
Kandidatenmenge $s_N$ benötigt. Die Kandidatenmenge wird mit $C$ bezeichnet und
die Menge der Datensätze des Indexer mit $IX$. Für jede Anfrage werden diese
Metriken folgendermaßen berechnet:

$$\begin{aligned}
s_M &= \forall c \in C: \sum |(q_{id}, c) \cap P| + |(c, q_{id}) \cap P| \\
s_N &= |C| - s_M \\
n_M &= \forall (p_1, p_2) \in P: \sum |(q_{id}, p_2) \cap P| + |(p_1, q_{id}) \cap P| \\
n_N &= (|IX \setminus q_{id}|) - n_M
\end{aligned}$$

$s_M$ ist die Anzahl der Matches zur Anfrage $q$ in der Kandidatenmenge $C$,
$s_N$ ist die Anzahl der Non-Matches zu $q$ in $C$, $n_M$ ist die Gesamtanzahl
der Matches zu $q$ in den Matches $P$ und $n_N$ ist die Gesamtanzahl an
Non-Matches zu $q$ in $IX$. Für $n_N$ muss der Anfragedatensatz von der
Gesamtmenge abgezogen werden, da dieser zu Begin jeder Anfrage vom Indexer in
den Datenbestand aufgenommen wird bzw. wenn er dort schon vorhanden ist keine
Rolle für die Enity Resolution spielt, da er herausgefiltert wird. Mit jeder
Anfrage werden diese vier Werte mit den vorherigen aufsummiert, sodass die
Effektivtätsmaße Bezug auf alle bisher gestellten Anfragen nehmen. Die
Qualitätsmaße Recall, Precision, F-measure und Average Precision werden über
die True Positives (TP), False Positives (FP) und False Negatives (FN)
bestimmt. Deren Berechnung ist identisch zu den Werten der Effektivitätsmaßen
mit der Abweichung, dass diese auf der Ergebnismenge $R$, welche die durch den
Klassifikator klassifizierten Matches aus $C$ enhählt.

$$\begin{aligned}
TP &= \forall r \in R: \sum |(q_{id}, r) \cap P| + |(r, q_{id}) \cap P| \\
FP &= |R| - TP \\
FN &= \forall (p_1, p_2) \in P: \sum |(q_{id}, p_2) \cap P| + |(p_1, q_{id}) \cap P| \\
\end{aligned}$$

Die True Negatives werden nicht berechnet, da diese in den Metriken nicht
benötigt werden. Auch hier werden die Werte für jede Anfrage summiert, sodass
die aus der Summe berechneten Metriken alle bisherigen Anfragen berücksichtigen.

## Experimenteller Aufbau

![Aufteilung der Datensätze in Validierungsmenge, Trainingsmenge und Testmenge.
Tupel in den Mengen sind durch Punkte markiert und Duplikate durch eine Line
zwischen zwei Tupeln. Die farbigen Linen zeigen, wie die jeweilige Untermenge
gebildet wird. ](./images/testsets.svg){#fig:testsets}

Für die Durchführung der Evaluierung wurden die Datensätze in vier disjunkte
Teildatensätze gesplittet. Diese Aufteilung ist in @fit:testsets dargestellt.
Die Hälfte der Datensätze befindet sich in der Base, die andere Hälfte ist zu
gleichen Teilen in Validierung, Training und Testing aufgeteilt. Datensätze in
den Mengen sind durch schwarze Punkte markiert. Matches sind durch Linien
verbunden. In der Build-Phase wird der initiale Index stets aus den Datensätzen
der Base gebaut. Der Anfragestrom, in der Query-Phase, wird durch Datensätze aus
Validierung, Training oder Testing zusammengestellt. Durch die Verteilung der
Matches ist sichergestellt, dass dadurch für jedes Match eine Query durchgeführt
wird, in welcher das jeweilige andere in der Base gefunden werden kann. In der
Fit-Phase werden die Duplikate zusammen benötigt, weshalb jeweils Validierung,
Training und Testing mit der Base zusammengefasst werden, wie durch die rote,
grüne bzw. blaue Umrandung dargestellt ist.

```{.texalgo #alg:split caption="SplitDataset(D, GT)"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Ground Truth Matches: $P$
  \end{itemize}
}
\Statex
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Base Dataset: $D_B$
  \item Validation Dataset: $D_V$, Validation Ground Truth: $P_V$
  \item Training Dataset: $D_{Tr}$, Training Ground Truth: $P_{Tr}$
  \item Testing Dataset: $D_{Te}$, Testing Ground Truth: $P_{Te}$
  \end{itemize}
}
\Statex
\State Initialize sets $D_B = (), D_V = (), D_{Tr} = (), D_{Te} = ()$
\State Initialize sets $P_V = (), P_{Tr} = (), P_{Te} = ()$
\State Initialize sets $P_B = ()$, set $P_Q = ()$
\State Initialize dictionary $B = \{\}, Q = \{\}$
\For{pairs $(p_1, p_2) \in P$}\label{alg:sp:1}
  \State Append $p_1$ to $P_B$
  \State Append $p_2$ to $P_Q$
  \State Append $p_2$ to $B[p_1]$
\EndFor\label{alg:sp:2}
\State $P_I = P_B \cup P_Q$\label{alg:sp:2.1}
\State $P_B = P_B \setminus P_I$
\State Remove $P_I$ from $B$\label{alg:sp:2.2}
\State Initialize $rr = 0$
\For{keys $p_1 \in B$}\label{alg:sp:3}
  \If{$|B[p_1]| > 1$}
    \For{$p_2 \in B[p_1]$}
      \State $Q[p_2] = rr \% 3$
      \State Increment $rr$
    \EndFor
  \EndIf
\EndFor\label{alg:sp:4}
\State Initialize $rr_m = 0, rr_{nm} = 0, rr_s = 0$
\State Initialize $ID_B = (), ID_V = (), ID_{Tr} = (), ID_{Te} = ()$
\For{record $r \in D$}\label{alg:sp:5}
  \If{$r.id \in P_B$}\label{alg:sp:6}
    \State Append $r$ to $D_B$ and append $r.id$ to $ID_B$
  \ElsIf{$r.id \in P_Q$}\label{alg:sp:7}
    \If{$r.id \in Q$}\label{alg:sp:8}
      \State $rr_s = Q[r.id]$
    \Else\label{alg:sp:9}
      \State $rr_s = rr_m$
      \State $rr_m = rr_m + 1 \% 3$
    \EndIf
    \If{$rr_s = 0$}\label{alg:sp:10}
      \State Append $r$ to $D_V$ and append $r.id$ to $ID_V$
    \ElsIf{$rr_s = 1$}
      \State Append $r$ to $D_{Tr}$ and append $r.id$ to $ID_{Tr}$
    \ElsIf{$rr_s = 2$}
      \State Append $r$ to $D_{Te}$ and append $r.id$ to $ID_{Te}$
    \EndIf\label{alg:sp:11}
  \Else\label{alg:sp:12}
    \If{$rr_{nm} > 2$}\label{alg:sp:13}
      \State Append $r$ to $D_B$ and append $r.id$ to $ID_B$
    \ElsIf{$rr_{nm} = 0$}\label{alg:sp:14}
      \State Append $r$ to $D_V$ and append $r.id$ to $ID_V$
    \ElsIf{$rr_{nm} = 1$}\label{alg:sp:15}
      \State Append $r$ to $D_{Tr}$ and append $r.id$ to $ID_{Tr}$
    \ElsIf{$rr_{nm} = 2$}\label{alg:sp:16}
      \State Append $r$ to $D_{Te}$ and append $r.id$ to $ID_{Te}$
    \EndIf\label{alg:sp:17}
    \State $rr_{nm} = rr_{nm} + 1 \% 6$
  \EndIf\label{alg:sp:18}
\EndFor\label{alg:sp:19}
\For{pairs $(p_1, p_2) \in P$}\label{alg:sp:20}
  \If{$p_1 \in ID_B and p_2 \in ID_V$}
    \State Append $(p_1, p_2)$ to $P_V$
  \ElsIf{$p_1 \in ID_B and p_2 \in ID_{Tr}$}
    \State Append $(p_1, p_2)$ to $P_{Tr}$
  \ElsIf{$p_1 \in ID_B and p_2 \in ID_{Te}$}
    \State Append $(p_1, p_2)$ to $P_{Te}$
  \EndIf
\EndFor\label{alg:sp:21}
\State Return $D_B, D_V, P_V, D_{Tr}, P_{Tr}, D_{Te}, P_{Te}$
```

Das Vorgehen zum Teilen eines Datensatzes ist in Algorithmus \ref{alg:split}
erläutert. Der Algorithmus bekommt dazu den Datensatz $D$ und die Matches der
Ground Truth $P$ übergeben. Das erwartete Ergebnis sind vier disjunkte Teilmenge
von $D$ für die Base $D_B$, Validierung $D_V$, Training $D_{Tr}$ und Testing
$D_{Te}$. Zudem die jeweiligen Ground Truth Matches für Validierung $P_V$,
Training $P_{Tr}$ und Testing $P_{Te}$. Im ersten Verarbeitungsschritt werden
die Paare $(p_1, p_2)$ der Ground Truth gelesen, dabei wird jeweils der erste
Datensatz des Tupels $p_1$ zur Menge der Matches $P_B$, die der Base zugeordnet
werden sollen, hinzugefügt. Der zweite Datensatz $p_2$ wird zur Menge der
Matches $P_Q$, auf welchen später die Query gestellt werden, hinzugefügt.
Zusätzlich werden in $B$ alle $p_1$ Datensätze mit $p_2$ verlinkt, sodass
Gruppen von Duplikaten gefunden werden (Zeilen \ref{alg:sp:1}-\ref{alg:sp:2}).
Danach wird die gemeinsamen Datensatzidentifier in $B$ und $Q$ ermittelt, was
bei Gruppen von Duplikaten vorkommen kann. Diese werden anschließend aus $P_B$
und $B$ entfernt, wodurch Matches zwischen den Anfragemengen (Validierung,
Training und Testing) entfernt werden (Zeilen
\ref{alg:sp:2.1}-\ref{alg:sp:2.2}). Diese sind für die Evaluierung nicht
relevant, da lediglich Matches zwischen der Base und der jeweiligen Anfragemenge
betrachtet werden. Im nächsten Schritt wird für Gruppen von Duplikaten in $B$
eine Verteilung bestimmt (Zeilen \ref{alg:sp:3}-\ref{alg:sp:4}). Für jedes $p_1$
in $B$, dass mehr als einen Matchpartner $p_2$ hat, wird $p_2$ zu $Q$
hinzugefügt und über einen Schlüssel (0 = Validierung, 1 = Training, 2 =
Testing) auf die Anfragemengen per Round-Robin verteilt. Dadurch ist
sichergestellt, dass Gruppen von Duplikaten fair aufgeteilt werden. Anschließend
erfolgt die Zuweisung der Datensätze, ebenfalls per Round-Robin und zwar erstens
für die Matches $rr_m$ und zweitens für die Non-Matches $rr_{nm}$ (Zeilen
\ref{alg:sp:5}-\ref{alg:sp:19}). Für jeden Datensatz $r$ wird geprüft, ob der
Datensatzidentifier von $r.id$ in $P_B$ ist, dann wird der Datensatz zu $D_B$
hinzugefügt. Ist $r.id$ in $P_Q$ und in $Q$, dann wird aus $Q$ die vorher
bestimmte Menge nach $rr_s$ ausgewählt. Ist $r.id$ in $P_Q$, aber nicht in $Q$
wird die Menge über $rr_m$ nach $rr_s$ ausgewählt und $rr_m$ danach
inkrementiert, damit dieser auf die nächste Menge zeigt, da es drei
Anfragemengen gibt wird $rr_m$ zusätzlich modulo 3 genommen. Anhand von $rr_s$
wird $r$ zu $D_V$, $D_{Tr}$ oder $D_{Te}$ hinzugefügt. Ist $r.id$ weder in $P_B$
noch in $P_Q$, dann wird über $rr_{nm}$ die Menge bestimmt, wobei 0-2 die
Anfragemengen sind und 3-5 die Base, da diese die Hälfte der Datensätze
beinhaltet Zeilen(\ref{alg:sp:12}-\ref{alg:sp:18}}). Dementsprechend wird
$rr_{nm}$ nach der Inkrementierung modulo 6 genommen. Zum Schluss werden die
Ground Truth Matches $P$ aufgeteilt. Ist der erste Datensatz $p_1$ eines Paares
$(p_1, p_2)$ in der Base und der zweite $p_2$, entweder in der
Validierungsmenge, Trainingsmenge oder Testingmenge, dann wird das Paar zu den
entsprechenden Matches $P_V$, $P_{Tr}$ oder $P_{Te}$ hinzugefügt (Zeilen
\ref{alg:sp:20}-\ref{alg:sp:21}). Durch diese Auswahl wird verhindert, dass
Matches zwischen $D_V$, $D_{Tr}$ oder $D_{Te}$ fälschlicherweise in $P_V$,
$P_{Tr}$ oder $P_{Te}$ aufgenommen werden.

* Welche Komponenten werden genutzt?

## Freie Parameter {#sec:free_params}

Auf der Validierungsmenge wurden robuste Parameter für die freien Parameter zur
die Evaluierung gewählt. Robust bedeutet, dass diese nicht optimal für jeden
Datensatz sind, sondern gute Ergebnisse für alle Datensätze liefern und
gleichzeitig verhindern, dass die Entity Resolution katastrophal versagt.

### Labelgenerator

Die freien Parameter des Labelgenerators sind die Fenstergröße, die untere
und obere Schwelle, sowie die maximalen Matches und Non-Matches. Dabei werden
die Schwellen nur benötigt falls keine Ground Truth existiert und diese vom
Label Generator selbstständig erzeugt wird.

#### Fenstergröße

Zur Bestimmung einer geeigneten Fenstergröße wurde der Label Generator ohne
Matches betrachtet, da diese Variante deutlich empfindlicher auf die
unterschiedlicher Parameter reagiert und dadurch der Effekt unterschiedlicher
Fenstergrößen $w$ einfacher ausgewertet werden kann. Dazu wurde die untere
Schwelle $lt$ und die obere Schwelle $ut$ in 0.1 Schritten bis 0.5 erhöht und
mit den Fenstergrößen 2, 5, 10 und 20 verglichen. Die maximalen Matches wurden
mit 10 % der Gesamtmenge und die maximalen Non-Matches mit 25 % der Gesamtmenge
bestimmt. Bei dem genutzen NCVoter Datensatz sind die maximalen Matches bei 551k
und die maximalen Non-Matches bei 1.337k. In den [@tbl:w2; @tbl:w5; @tbl:w10;
@tbl:w20;] sind die Ergebnisse für die unterschiedlichen Fenstergrößen
dargestellt. Die obere Schwelle $ut$ hatte dabei keine entscheidende Auswirkung,
sodass lediglich die untere Schwelle $lt$ betrachtet wird. Für jede Schwelle
wurden Matches (P), Non-Matches (N), Pairs Completeness (PC) und Pairs Qualtity
(PQ) analysiert. Die Pairs Quality liefert keinen entscheidenden Hinsweis auf
ein geeignetes Fenster, da diese sich lediglich zwischen 1 % und 11 % hin und
her bewegt. Beim Blick auf die Pairs Completeness zeigt sich, dass diese sich
zwischen 13 % und 16 % für alle Fenstergrößen bewegt, mit Außnahme von $w=2$.
Dort ist die Pairs Completeness für $lt \leq 0.3$ mit 95 % deutlich besser. Für
$w=2$ werden am wenigsten Paare gebildet, wie sich bei $lt=0.1$ bemerkbar macht,
da hier lediglich 8k Non-Matches generiert wurden. Bei $lt=0.2$ gibt es mit 655k
dann allerdings schon eine große Auswahl an Non-Matches und mit $lt=0.3$ wurden
bereits mehr Paare generiert als das Maximum. Aufgrund dieser Ergebnisse wird
die Fenstergröße für die weitere Evaluation mit 2 bestimmt.

| lt (w=2) | P    | N      | PC   | PQ   |
|----------+------+--------+------+------|
| 0.1      | 551k | 8k     | 0.95 | 0.11 |
| 0.2      | 551k | 655k   | 0.95 | 0.11 |
| 0.3      | 551k | 1,377k | 0.95 | 0.11 |
| 0.4      | 433k | 1,377k | 0.15 | 0.01 |
| 0.5      | 433k | 1,377k | 0.16 | 0.10 |

: Ergebnis mit Fenstergröße 2 {#tbl:w2}

| lt (w=5) | P    | N      | PC   | PQ   |
|----------+------+--------+------+------|
| 0.1      | 551k | 32k    | 0.15 | 0.01 |
| 0.2      | 551k | 1,377k | 0.15 | 0.01 |
| 0.3      | 551k | 1,377k | 0.15 | 0.01 |
| 0.4      | 551k | 1,377k | 0.16 | 0.11 |
| 0.5      | 551k | 1,377k | 0.16 | 0.11 |

: Ergebnis mit Fenstergröße 5 {#tbl:w5}

| lt (w=10) | P    | N      | PC   | PQ   |
|-----------+------+--------+------+------|
| 0.1       | 551k | 66k    | 0.15 | 0.01 |
| 0.2       | 551k | 1,377k | 0.15 | 0.01 |
| 0.3       | 551k | 1,377k | 0.15 | 0.01 |
| 0.4       | 551k | 1,377k | 0.16 | 0.11 |
| 0.5       | 551k | 1,377k | 0.13 | 0.13 |

: Ergebnis mit Fenstergröße 10 {#tbl:w10}

| lt (w=20) | P    | N      | PC   | PQ   |
|-----------+------+--------+------+------|
| 0.1       | 551k | 119k   | 0.15 | 0.01 |
| 0.2       | 551k | 1,377k | 0.15 | 0.01 |
| 0.3       | 551k | 1,377k | 0.15 | 0.01 |
| 0.4       | 551k | 1,377k | 0.16 | 0.11 |
| 0.5       | 551k | 1,377k | 0.13 | 0.12 |

: Ergebnis mit Fenstergröße 20 {#tbl:w20}

#### Untere und Obere Schwelle

![test](./images/ncvoter_matches_histo.svg){#fig:match_histo}

Die untere Schwelle $lt$ legt fest, bis zu welchem Ähnlichkeitswert Paare als
Non-Matches betrachtet werden und die obere Schwelle $ut$ legt fest, ab welchem
Ähnlichkeitswert Paare als Matches betrachtet werden, dabei gilt stets $lt \leq
ut$. In einem Experiment wurden $lt$ und $ut$ in 0.1 Schritten betrachtet und so
alle Konfigurationen bis 1.0, auf dem NCVoter-Datensatz ausprobiert. Für das
Fenster wurde der bereits bestimmte Wert von 2 gesetzt. Zur Auswertung wurden
Pairs Completeness, Pairs Quality und die Ground Truth analysiert. Anhand der
Pairs Completeness und Pairs Qualtity kann betrachtet werden, wie gut ein
Blocking Verfahren auf der generierte Ground Truth funktioniert. Durch die
gefilterte Ground Truth hingegen kann herausgefunden werden, wie viele Ground
Truth Paare für den Fusion-Lerner zur Verfügung stehen. Die [@tbl:recall;
@tbl:fp; @tbl:fn] betrachten nacheinander die Pairs Completeness, die Matches
und die Non-Matches. Die Pairs Quality ist uninteressant, da deren Werte relativ
konstant bei 0.1 liegen, mit einer Varianz von 0.03. In @tbl:recall ist gut zu
sehen, dass die Pairs Quality zwischen einer $ut$ von 0.1 und 0.4 immer eine
gute Pairs Qualtity von 95 % erzeugt. Der Blick auf das erlernte Blocking Schema
zeigt, dass dieses auch immer dasselbe ist. Dies trifft auch noch teilweise für
$ut=0.5$ zu, allderdings nur für $lt \leq 0.3$. Für alle $ut > 0.5$ varrieren
die Blocking Schema, wobei unabhängig von $lt$ keines über 17 % Pairs
Completeness kommt. Deshalb werden in den [@tbl:fp; @tbl:fn] lediglich
$ut$-Werte kleiner 0.6 betrachtet. Die maximalen Matches, die der Label
Generator erzeugen darf, liegen bei 10 % der Gesamtmenge und betragen 551k. Die
tatsächlichen Matches betragen 50k. Bei den Non-Matches ist das Limit 25 % und
damit 1,3 mio. Für die künstliche Anreicherung der gefilterten Non-Matches,
stehen jedoch alle erzeugten Non-Matches zur Verfügung, dementsprechend je höher
$lt$ desto mehr Non-Matches und umgekehrt für $ut$. In @tbl:fp ist zu sehen,
dass für $ut \leq 0.4$ die Ausgangsmenge der Matches auf das Maximum beschränkt
wurde. Da jeweils die Matches mit der höchsten Ähnlichkeit genutzt werden, sind
diese Mengen identisch, weshalb auch die gefilterte Mengen mit jeweils 300k
Datensätzen, aufgrund desselben Blocking Schema, identisch sind. Für $ut=0.5$
ist die Ausgangsmenge kleiner als das Maximum. Die gefilterte Menge beträgt in
diesem Fall 288k. In jedem Fall sind nach dem Filtern genügend Matches
vorhanden, um einen Klassifikator zu trainieren, auch wenn diese 6-Mal soviele
Matches beinhaltet, wie die tatsächlichen Matches. In @tbl:fn werden die Anzahl
der Non-Matches dargestellt. Für $lt=0.1$ sind insgesamt nur 8k Paare erzeugt
worden, weil das TF/IDF Blocking die meisten der sehr unähnlichen Paare
ausschließt. Nach dem Filtern durch das Blocking Schema sind keine Non-Matches
mehr vorhanden, da das Blocking Schema verhindert, dass diese offensichtlichen
Non-Matches zusammen gruppiert werden. Für $lt=0.2$ gibt es ein ähnliches Bild.
Zwar ist die Anzahl der Ausgangsmenge mit 655k deutlich höher, dennoch werden
lediglich 66 Paare zusammen gruppiert und damit nicht ausgefiltert.
Interessanter wird es erst ab $lt=0.3$. Hier wird erste Mal das Maximum der
Ausgangsmenge mit 1377k erreicht. Die gefilterten Non-Matches betragen 2430, was
im Vergleich zu den Matches immer noch sehr wenig ist, aber durchaus genügt um
einen Klassifikator zu trainieren. Mit $lt=0.4$ erhöht sich diese Anzahl
nochmals, um das 5-fache, allerdings weicht hier mit $ut=0.5$ die Pairs
Completeness das erste Mal von 95 % ab. Daraus folgt, dass sich im Bereich 0.3 -
0.5 eine essentielle Anzahl an Machtes befinden, die mit $lt=0.4$
fälschlicherweise als Non-Matches klassifiziert werden und mit $ut=0.5$ von den
Matches ausgeschlossen sind. Folglich wird ein Blocking Schema gelernt, das eine
schlechte Pairs Quality liefert. Anhand dieser Werte wird $lt$ mit 0.3
festgelegt, da ab hier genügend Paare zum trainieren  eines Klassifikator zur
Verfügung stehen und gleichzeitig die Wahrscheinlichkeit noch gering ist ein
Match fälschlicherweise als Non-Match aufzunehmen. Die $ut$ wird ebenfalls auf
0.3 festgelegt. Zwar scheinen die Werte für $ut=0.4$ auch noch stabil zu sein,
aber um möglichst robust zu sein, wird dieser Bereich als Puffer genutzt.

|  PC | 0.1  | 0.2  | 0.3  | 0.4  | 0.5  | 0.6  | 0.7  | 0.8  | 0.9  | 1.0  |
|----:+------+------+------+------+------+------+------+------+------+------|
| 0.1 | 0.95 | 0.95 | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.15 | 0.15 | 0.13 |
| 0.2 |      | 0.95 | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.15 | 0.15 | 0.13 |
| 0.3 |      |      | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.16 | 0.13 | 0.13 |
| 0.4 |      |      |      | 0.95 | 0.15 | 0.16 | 0.16 | 0.16 | 0.14 | 0.14 |
| 0.5 |      |      |      |      | 0.16 | 0.16 | 0.13 | 0.14 | 0.14 | 0.06 |
| 0.6 |      |      |      |      |      | 0.13 | 0.14 | 0.14 | 0.16 | 0.06 |
| 0.7 |      |      |      |      |      |      | 0.14 | 0.10 | 0.16 | 0.06 |
| 0.8 |      |      |      |      |      |      |      | 0.10 | 0.16 | 0.06 |
| 0.9 |      |      |      |      |      |      |      |      | 0.14 | 0.06 |
| 1.0 |      |      |      |      |      |      |      |      |      | 0.06 |

: Pairs Completeness {#tbl:recall}

| Matches | 0.1       | 0.2       | 0.3       | 0.4       | 0.5       |
|--------:+-----------+-----------+-----------+-----------+-----------|
|     0.1 | 551k/300k | 551k/300k | 551k/300k | 551k/300k | 443k/288k |
|     0.2 |           | 551k/300k | 551k/300k | 551k/300k | 443k/288k |
|     0.3 |           |           | 551k/300k | 551k/300k | 443k/288k |
|     0.4 |           |           |           | 551k/300k | 443k/287k |
|     0.5 |           |           |           |           | 443k/273k |

: Filtered Matches {#tbl:fp}

| Non-Matches | 0.1  | 0.2     | 0.3        | 0.4         | 0.5        |
|------------:+------+---------+------------+-------------+------------|
|         0.1 | 8k/0 | 8k/0    | 8k/0       | 8k/0        | 8k/0       |
|         0.2 |      | 655k/66 | 655k/66    | 655k/66     | 655k/66    |
|         0.3 |      |         | 1377k/2430 | 1377k/2430  | 1377k/2430 |
|         0.4 |      |         |            | 1377k/13916 | 1377k/11k  |
|         0.5 |      |         |            |             | 1377k/8k   |

: Filtered Non-Matches {#tbl:fn}

#### Maximum Ground Truth

* max positive/negatvie Paare, Labelgenerator


* Block Size Filter, Labelgenerator
* "Stop Token Filter" -> 100, Blocking Scheme
* Anzahl/Größe der Konjunktionen, Blocking Scheme
* Search, Crossval, Metric ,Fusion-Lerner

### Geeignete Prädikate

* Einfluss der Prädikate (commonToken, excactMatch, q-Qram, suffixe, prefixe),
  Blocking Scheme

* Stringähnlichkeiten (Levenshtein, Damerau, Jaro, Ratio), SimLearner
* MDySimII vs MDySimIII

* Schwelle des Klassifikators (`predict_proba`) verschieben. (ROC vs average
  precision)

=> Ziel: optimales System

\TODO{Zu Implementierung hinzufügen} Laut Kejriwal & Miranker
[@KM:Unsupervised:13] bieten Werte $>3$ keine wesentliche Verbesserung.

## Baseline vs GT partial vs GT full

Validierungsmenge

* Pair completeness/Reduction Ratio/Pairs Quality
* Presion/Recall/F-measure
* Memory usage
* Insert/Query Times

## Human Baseline

Train/Train

Train/Test

## Grund Truth vs No Ground Truth
