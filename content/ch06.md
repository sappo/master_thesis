# Zusammenfassung und Ausblick \label{chap:conclusion}

Dieses Kapitel gibt eine Zusammenfassung der Hauptbeiträge dieser Arbeit und
einen Ausblick in welche Richtungen die Beiträge weiterentwickelt werden können.

## Zusammenfassung

Die vorliegende Arbeit untersuchte das Problem der Selbstkonfiguration eines
Entity Resolution Workflows für Event Stream Processing Systeme. In Kapitel
\ref{chap:101} wurden dazu die drei Anforderungen **Niedrige Latenzen**,
**Datenmodifikation zur Laufzeit** und **Hohe Trefferrate** festgelegt, die
einen Kompromiss zwischen Qualität und Effizienz suchen. Hauptproblem bei der
Verwendung eines Entity Resolution Systems ist die Anpassung der freien
Parameter auf die Domäne der Daten. Für die drei entscheidenden
Parameterkonfigurationen, das Blocking Schema, die Ähnlichkeitsmetriken und den
Klassifikator, wurden in Kapitel \ref{chap:analysis} Verfahren analysiert und
entwickelt, die eine automatische Bestimmung der Parameter ermöglichen. Damit
diese Verfahren Erfolg haben, sind gelabelte Daten, anhand welcher ein
Duplikatspaar von einem nicht Duplikatspaar unterschieden werden kann, benötigt.
Für viele Datensätze sind die gelabelten Daten jedoch nicht verfügbar und das
manuelle Erzeugen ist sowohl zeit- als auch kostenintensiv. Aufgrunddessen wurde
ein Verfahren analysiert, dass automatisch gelabelte Daten erzeugt. Des Weiteren
wurde ein Blocking Verfahren weiterentwickelt, dass mit geeigneten Parametern,
alle drei Anforderungen erfüllt. In Kapitel \ref{chap:design} wurden die
analysierten und entwickelten Verfahren zu einem Gesamtsystem zusammengefügt,
dass sich dem Anwender gegenüber, transparent selbstkonfiguriert und anhand der
Konfiguration und der Bestandsdaten, Anfragen aus einem Eventstrom beantwortet.
Die Evaluation in Kapitel \ref{chap:evaluation} untersuchte zunächst die noch
offenen (nicht selbstkonfigurierbaren) freien Parameter und wählt robuste Werte
für diese aus. Die Qualität und Effizienz des Systems wurde danach gegenüber
einer manuell bestimmten Baseline überprüft. Dabei zeigt das Ergebnis vor allem
in der Effizienz eine deutliche Überlegenheit. Anschließend wurde das Ergebnis
mit einer Konfiguration aus automatisch erzeugten gelabelten Daten evaluiert,
deren Qualität gegenüber den manuell gelabelten Daten, z.T. fragwürdig ist. Das
Ergebnis ist umso erfreulicher, denn zum einen ist die Konfiguration in
Qualität und Effizienz besser als die Baseline und zum anderen der
Konfiguration der manuellen gelabelten Daten nur leicht unterlegen.

## Ausblick

Die in dieser Arbeit vorgestellten und entwickelten Verfahren und Algorithmen,
haben eine Reihe von Beiträgen und Verbesserungen der aktuellesten Entity
Resolution Methoden vorgenommen, bieten jedoch noch einige Möglichkeiten der
Weiterentwicklung und Probleme, die es zu lösen gilt, welche im Folgenden
betrachtet werden.

**Optimierung der Disjunktion des Blocking-Schema:** Beim Lernen des Blocking
Schema werden, nach der Bewertung eines Ausdrucks, dessen Datensatzpaare auf die
Ground Truth abgebildet. Damit bei der Disjunktion der Ausdrücke diese
vergleichbar sind und die Blöcke nicht erneut gebaut werden müssen, um eine
Disjunktion zu bewerten. Dabei werden vor allen Dingen die Non-Matches
unterrepräsentiert, da nur eine kleine Auswahl derer Teil der Ground Truth ist.
Über das Bauen der eigentlichen Blöcke kann daher deutlich genauer das F-measure
der Disjunktion ermittelt werden. Allerdings dauert dieser Vorgang deutlich
länger, da Blockschlüssel und Blöcke aus mehreren Attributen erzeugt werden.
Eine Idee dieses Problem zu lösen ist, aus den einzelnen F-measure Werten der
Ausdrücke, das maximale F-measure der Disjunktion zu errechnen und so nur
Disjunktionen zu Testen, die die Möglichkeit bieten einen besseres F-measure zu
erreichen. Eine optimistische Betrachtung nimmt beispielsweise an, dass sich die
Recall Werte der einzelnen Ausdrücke addieren und die Precision sich dabei
mittelt.

**Parallelisierung des Lernens:** In der Selbstkonfigurationsphase, dauert das
Lernen des Blocking Schema am längsten, da für jeden zu prüfenden Ausdruck der
Indexer alle Blöcke bauen muss. Dabei können die Ausdrücke unabhängig
voneinander bewertet werden, was eine reibungslose Parallelisierung ermöglicht.
Dieser Prozess benötigt jedoch z.T. sehr viel Arbeitsspeicher, weshalb
Multithreading bzw. Multiprocessing keine Optionen sind. Denkbar ist aber die
Ausführung auf einem Cluster von Rechnern. Die hierbei genutzen Daten sind
statisch, deshalb kann dazu ein Batchverfahren wie MapReduce eingesetzt werden.

**Parallelisieren der Anfragen**: Zur Zeit werden die Anfragen auf einem CPU
Kern von einem Thread bearbeitet. Durch die Parallelisierung kann hier die
Effizienz deutlich gesteigert werden, vor allem da die Ähnlichkeitsberechnung,
aufgrund der Teilvektoren, nicht vollständig vorausberechnet werden kann. Dabei
muss die Anforderung an die Datenmodifikation zur Laufzeit beachtet werden,
insbesondere, dass eine beantwortete und hinzugefügte Anfrage direkt für die
darauffolgende zur Verfügung stehen muss. Die Idee ist die Blöcke des
Blocking-Verfahrens auf mehrere Threads, Prozesse oder Rechner zu verteilt.
Dadurch können die Ähnlichkeiten verschiedener Attribute gleichzeitig berechnet
bzw. abgerufen werden. Damit allerdings kein Knoten zum Flaschenhals wird, wird
ein Algorithmus benötigt, der die Blöcke nach Bearbeitungszeit und
Anfragehäufigkeit verteilt und eventuell nach Last anpasst. Einen Ansatz hierfür
beschreibt Kolb in [@Kol:Effiziente:14]. Dabei ist sein Algorithmus für
MapReduce-Verfahren optimiert, dieser liefert jedoch einen guten Einstieg in die
Problematik.

**Verbessern des Bewertungsmaß des Similarity Lerners**: Der Similarity Lerner
bewertet Ähnlichkeitsmetriken anhand der Average Precision. Hat dieser genügend
Daten zum Lernen und und einen großen Pool von Ähnlichkeitsmetriken zur Auswahl,
werden gute Metriken gelernt, mit welcher ein Klassifikator qualitativ gute
Entscheidungen treffen kann. Allerdings liegen die unterschiedlichen Werte für
die Metriken oft im Bereich 10^-1^, was vor allem bei kleineren Datensätzen dazu
führen kann, dass die Auswahl der Ähnlichkeitsmetriken suboptimal ist. Hier gilt
es herauszufinden, wie die Bewertungsschwelle zwischen den Ähnlichkeitsmetriken
vergrößert werden kann, um eindeutigere Entscheidungen zu treffen.

**Lernen von Hyperparametern der Ähnlichkeitsmaße:** Der Similarity Lerner wählt
aktuell aus einer Menge von vorkonfigurierten Ähnlichkeitsmaßen aus. Dabei ist
aus @sec:similarity bekannt, dass Ähnlichkeitsmaße z.T. mehrere Parameter haben,
die je nach Datendomäne entscheidenden Einfluss auf die Qualität haben können.
Sollen durch den Similarity Lerner, ähnlich zum Fusion-Lerner, jeweils ein
Parametergrid für eine Ähnlichkeitsfunktion getestet werden, muss sichergestellt
sein, dass für jeden Parameterwert stets die Dreiecksungleichung erfüllt ist.
Damit ist ein ähnliches Szenario wie für die Klassifikatoren annehmbar, sodass
der Similarity Lerner, anhand eines Parametergrids, für jede
Ähnlichkeitsfunktion die Konfiguration vornimmt und in einer Grid Search testet.

**Kalibrierung der Wahrscheinlichkeitsschwellen:** Die aktuell gewählten
Wahrscheinlichkeitsschwellen der Klassifikatoren entsprechen den
Standardeinstellungen der Scikit-learn Bibliothek. In fast allen
Precision-Recall Kurven, die in der Evaluation gezeigt wurden, ist jedoch
deutlich zu sehen, dass eine Kalibrierung der Wahrscheinlichkeitsschwelle in
vielen Fällen die Precision, bei minimalem Recallverlust, dramatisch verbessert
werden kann. Die Kalibrierung wird dazu auf einer zweiten, von der zum
Trainieren genutzten Ground Truth, disjunkten Ground Truth durchgeführt.
Verschiedene Verfahren zur Kalibrierung werden beispielsweise in der
Scikit-learn Klasse `CalibratedClassifierCV` implementiert.

**Aktives Lernen der Ground Truth:** Das Verfahren von Kejriwal & Miranker aus
[@KM:Unsupervised:13] kann zwar ein schwache Ground Truth generieren, diese
trennt jedoch Matches von Non-Matches durch eine Ähnlichkeitsschwelle, was nicht
den realen Daten entspricht. Ein anderer Ansatz möglichst effizient eine Ground
Truth zu Erzeugen, sind aktive Lernmethoden (vgl. @sec:active_learn), welche
selbständig eine kleine Menge herausfordernder, manuell zu klassifizierende,
Datenpaare bestimmen und so sequentielle eine Ground Truth bestimmen. Eine
interessante Weiterentwicklung ist es, diese beiden Ansätze zu kombinieren und
die automatisch erzeugte schwache Ground Truth sequentielle, durch Auswahl
herausfordernder Paare, zu verbessern und die harte Ähnlichkeitsschwelle zu
lockern.

<!-- **Ressourcenbeschränkung**: Durch Python NCVoter hochrechnen, wie viele -->
<!-- Datensätze maximal möglich auf max VM mit 256 GB Ram. -->

<!-- **De-Duplikation:** -->
