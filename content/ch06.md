# Evaluierung der Qualtiät und Effektivität

TBW

## Berechnung der Metriken für dynamisches Enity Resolution

Während im statischen Entity Resolution, die Metriken (vgl. @sec:measurements)
am Ende des Verfahrens einmalig berechnet werden können, ist das im dynamischen
Falle nicht möglich, da es theoretisch kein Ende gibt. Das bedeutet, die
Metriken müssen inkrementell mit jeder Anfrage $q$ erhoben werden. Zur
Berechnung der Effektivitätsmaße Pairs Completeness, Pairs Quality und Reduction
Ratio werden die tatsächlichen Matches $n_M$, die tatsächlichen Non-Matches
$n_N$, die Matches in der Kandidatenmenge $s_M$ und Non-Matches in der
Kandidatenmenge $s_N$ benötigt. Die Kandidatenmenge wird mit $C$ bezeichnet und
die Menge der Datensätze des Indexer mit $IX$. Für jede Anfrage werden diese
Metriken folgendermaßen berechnet:

$$\begin{aligned}
s_M &= \forall c \in C: \sum |(q_{id}, c) \cap P| + |(c, q_{id}) \cap P| \\
s_N &= |C| - s_M \\
n_M &= \forall (p_1, p_2) \in P: \sum |(q_{id}, p_2) \cap P| + |(p_1, q_{id}) \cap P| \\
n_N &= (|IX \setminus q_{id}|) - n_M
\end{aligned}$$

$s_M$ ist die Anzahl der Matches zur Anfrage $q$ in der Kandidatenmenge $C$,
$s_N$ ist die Anzahl der Non-Matches zu $q$ in $C$, $n_M$ ist die Gesamtanzahl
der Matches zu $q$ in den Matches $P$ und $n_N$ ist die Gesamtanzahl an
Non-Matches zu $q$ in $IX$. Für $n_N$ muss der Anfragedatensatz von der
Gesamtmenge abgezogen werden, da dieser zu Begin jeder Anfrage vom Indexer in
den Datenbestand aufgenommen wird bzw. wenn er dort schon vorhanden ist keine
Rolle für die Enity Resolution spielt, da er herausgefiltert wird. Mit jeder
Anfrage werden diese vier Werte mit den vorherigen aufsummiert, sodass die
Effektivtätsmaße Bezug auf alle bisher gestellten Anfragen nehmen. Die
Qualitätsmaße Recall, Precision, F-measure und Average Precision werden über
die True Positives (TP), False Positives (FP) und False Negatives (FN)
bestimmt. Deren Berechnung ist identisch zu den Werten der Effektivitätsmaßen
mit der Abweichung, dass diese auf der Ergebnismenge $R$, welche die durch den
Klassifikator klassifizierten Matches aus $C$ enhählt.

$$\begin{aligned}
TP &= \forall r \in R: \sum |(q_{id}, r) \cap P| + |(r, q_{id}) \cap P| \\
FP &= |R| - TP \\
FN &= \forall (p_1, p_2) \in P: \sum |(q_{id}, p_2) \cap P| + |(p_1, q_{id}) \cap P| \\
\end{aligned}$$

Die True Negatives werden nicht berechnet, da diese in den Metriken nicht
benötigt werden. Auch hier werden die Werte für jede Anfrage summiert, sodass
die aus der Summe berechneten Metriken alle bisherigen Anfragen berücksichtigen.

## Experimenteller Aufbau

Für die Durchführung der Evaluierung wurden die Datensätze in vier disjunkte
Teildatensätze gesplittet. Diese Aufteilung ist in @fit:testsets dargestellt.
Die Hälfte der Datensätze befindet sich in der Base, die andere Hälfte ist zu
gleichen Teilen in Validierung, Training und Testing aufgeteilt. Datensätze in
den Mengen sind durch schwarze Punkte markiert. Matches sind durch Linien
verbunden. In der Build-Phase wird der initiale Index stets aus den Datensätzen
der Base gebaut. Der Anfragestrom, in der Query-Phase, wird durch Datensätze aus
Validierung, Training oder Testing zusammengestellt. Durch die Verteilung der
Matches ist sichergestellt, dass dadurch für jedes Match eine Query durchgeführt
wird, in welcher das jeweilige andere in der Base gefunden werden kann. In der
Fit-Phase werden die Duplikate zusammen benötigt, weshalb jeweils Validierung,
Training und Testing mit der Base zusammengefasst werden, wie durch die rote,
grüne bzw. blaue Umrandung dargestellt ist.

![Aufteilung der Datensätze in Validierungsmenge, Trainingsmenge und Testmenge.
Tupel in den Mengen sind durch Punkte markiert und Duplikate durch eine Line
zwischen zwei Tupeln. Die farbigen Linen zeigen, wie die jeweilige Untermenge
gebildet wird. ](./images/testsets.svg){#fig:testsets}

```{.texalgo #alg:split caption="SplitDataset(D, GT)"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Ground Truth Matches: $P$
  \end{itemize}
}
\Statex
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Base Dataset: $D_B$
  \item Validation Dataset: $D_V$, Validation Ground Truth: $P_V$
  \item Training Dataset: $D_{Tr}$, Training Ground Truth: $P_{Tr}$
  \item Testing Dataset: $D_{Te}$, Testing Ground Truth: $P_{Te}$
  \end{itemize}
}
\Statex
\State Initialize sets $D_B = (), D_V = (), D_{Tr} = (), D_{Te} = ()$
\State Initialize sets $P_V = (), P_{Tr} = (), P_{Te} = ()$
\State Initialize sets $P_B = ()$, set $P_Q = ()$
\State Initialize dictionary $B = \{\}, Q = \{\}$
\For{pairs $(p_1, p_2) \in P$}\label{alg:sp:1}
  \State Append $p_1$ to $P_B$
  \State Append $p_2$ to $P_Q$
  \State Append $p_2$ to $B[p_1]$
\EndFor\label{alg:sp:2}
\State $P_I = P_B \cup P_Q$\label{alg:sp:2.1}
\State $P_B = P_B \setminus P_I$
\State Remove $P_I$ from $B$\label{alg:sp:2.2}
\State Initialize $rr = 0$
\For{keys $p_1 \in B$}\label{alg:sp:3}
  \If{$|B[p_1]| > 1$}
    \For{$p_2 \in B[p_1]$}
      \State $Q[p_2] = rr \% 3$
      \State Increment $rr$
    \EndFor
  \EndIf
\EndFor\label{alg:sp:4}
\State Initialize $rr_m = 0, rr_{nm} = 0, rr_s = 0$
\State Initialize $ID_B = (), ID_V = (), ID_{Tr} = (), ID_{Te} = ()$
\For{record $r \in D$}\label{alg:sp:5}
  \If{$r.id \in P_B$}\label{alg:sp:6}
    \State Append $r$ to $D_B$ and append $r.id$ to $ID_B$
  \ElsIf{$r.id \in P_Q$}\label{alg:sp:7}
    \If{$r.id \in Q$}\label{alg:sp:8}
      \State $rr_s = Q[r.id]$
    \Else\label{alg:sp:9}
      \State $rr_s = rr_m$
      \State $rr_m = rr_m + 1 \% 3$
    \EndIf
    \If{$rr_s = 0$}\label{alg:sp:10}
      \State Append $r$ to $D_V$ and append $r.id$ to $ID_V$
    \ElsIf{$rr_s = 1$}
      \State Append $r$ to $D_{Tr}$ and append $r.id$ to $ID_{Tr}$
    \ElsIf{$rr_s = 2$}
      \State Append $r$ to $D_{Te}$ and append $r.id$ to $ID_{Te}$
    \EndIf\label{alg:sp:11}
  \Else\label{alg:sp:12}
    \If{$rr_{nm} > 2$}\label{alg:sp:13}
      \State Append $r$ to $D_B$ and append $r.id$ to $ID_B$
    \ElsIf{$rr_{nm} = 0$}\label{alg:sp:14}
      \State Append $r$ to $D_V$ and append $r.id$ to $ID_V$
    \ElsIf{$rr_{nm} = 1$}\label{alg:sp:15}
      \State Append $r$ to $D_{Tr}$ and append $r.id$ to $ID_{Tr}$
    \ElsIf{$rr_{nm} = 2$}\label{alg:sp:16}
      \State Append $r$ to $D_{Te}$ and append $r.id$ to $ID_{Te}$
    \EndIf\label{alg:sp:17}
    \State $rr_{nm} = rr_{nm} + 1 \% 6$
  \EndIf\label{alg:sp:18}
\EndFor\label{alg:sp:19}
\For{pairs $(p_1, p_2) \in P$}\label{alg:sp:20}
  \If{$p_1 \in ID_B and p_2 \in ID_V$}
    \State Append $(p_1, p_2)$ to $P_V$
  \ElsIf{$p_1 \in ID_B and p_2 \in ID_{Tr}$}
    \State Append $(p_1, p_2)$ to $P_{Tr}$
  \ElsIf{$p_1 \in ID_B and p_2 \in ID_{Te}$}
    \State Append $(p_1, p_2)$ to $P_{Te}$
  \EndIf
\EndFor\label{alg:sp:21}
\State Return $D_B, D_V, P_V, D_{Tr}, P_{Tr}, D_{Te}, P_{Te}$
```

Das Vorgehen zum Teilen eines Datensatzes ist in Algorithmus \ref{alg:split}
erläutert. Der Algorithmus bekommt dazu den Datensatz $D$ und die Matches der
Ground Truth $P$ übergeben. Das erwartete Ergebnis sind vier disjunkte Teilmenge
von $D$ für die Base $D_B$, Validierung $D_V$, Training $D_{Tr}$ und Testing
$D_{Te}$. Zudem die jeweiligen Ground Truth Matches für Validierung $P_V$,
Training $P_{Tr}$ und Testing $P_{Te}$. Im ersten Verarbeitungsschritt werden
die Paare $(p_1, p_2)$ der Ground Truth gelesen, dabei wird jeweils der erste
Datensatz des Tupels $p_1$ zur Menge der Matches $P_B$, die der Base zugeordnet
werden sollen, hinzugefügt. Der zweite Datensatz $p_2$ wird zur Menge der
Matches $P_Q$, auf welchen später die Query gestellt werden, hinzugefügt.
Zusätzlich werden in $B$ alle $p_1$ Datensätze mit $p_2$ verlinkt, sodass
Gruppen von Duplikaten gefunden werden (Zeilen \ref{alg:sp:1}-\ref{alg:sp:2}).
Danach wird die gemeinsamen Datensatzidentifier in $B$ und $Q$ ermittelt, was
bei Gruppen von Duplikaten vorkommen kann. Diese werden anschließend aus $P_B$
und $B$ entfernt, wodurch Matches zwischen den Anfragemengen (Validierung,
Training und Testing) entfernt werden (Zeilen
\ref{alg:sp:2.1}-\ref{alg:sp:2.2}). Diese sind für die Evaluierung nicht
relevant, da lediglich Matches zwischen der Base und der jeweiligen Anfragemenge
betrachtet werden. Im nächsten Schritt wird für Gruppen von Duplikaten in $B$
eine Verteilung bestimmt (Zeilen \ref{alg:sp:3}-\ref{alg:sp:4}). Für jedes $p_1$
in $B$, dass mehr als einen Matchpartner $p_2$ hat, wird $p_2$ zu $Q$
hinzugefügt und über einen Schlüssel (0 = Validierung, 1 = Training, 2 =
Testing) auf die Anfragemengen per Round-Robin verteilt. Dadurch ist
sichergestellt, dass Gruppen von Duplikaten fair aufgeteilt werden. Anschließend
erfolgt die Zuweisung der Datensätze, ebenfalls per Round-Robin und zwar erstens
für die Matches $rr_m$ und zweitens für die Non-Matches $rr_{nm}$ (Zeilen
\ref{alg:sp:5}-\ref{alg:sp:19}). Für jeden Datensatz $r$ wird geprüft, ob der
Datensatzidentifier von $r.id$ in $P_B$ ist, dann wird der Datensatz zu $D_B$
hinzugefügt. Ist $r.id$ in $P_Q$ und in $Q$, dann wird aus $Q$ die vorher
bestimmte Menge nach $rr_s$ ausgewählt. Ist $r.id$ in $P_Q$, aber nicht in $Q$
wird die Menge über $rr_m$ nach $rr_s$ ausgewählt und $rr_m$ danach
inkrementiert, damit dieser auf die nächste Menge zeigt, da es drei
Anfragemengen gibt wird $rr_m$ zusätzlich modulo 3 genommen. Anhand von $rr_s$
wird $r$ zu $D_V$, $D_{Tr}$ oder $D_{Te}$ hinzugefügt. Ist $r.id$ weder in $P_B$
noch in $P_Q$, dann wird über $rr_{nm}$ die Menge bestimmt, wobei 0-2 die
Anfragemengen sind und 3-5 die Base, da diese die Hälfte der Datensätze
beinhaltet Zeilen(\ref{alg:sp:12}-\ref{alg:sp:18}}). Dementsprechend wird
$rr_{nm}$ nach der Inkrementierung modulo 6 genommen. Zum Schluss werden die
Ground Truth Matches $P$ aufgeteilt. Ist der erste Datensatz $p_1$ eines Paares
$(p_1, p_2)$ in der Base und der zweite $p_2$, entweder in der
Validierungsmenge, Trainingsmenge oder Testingmenge, dann wird das Paar zu den
entsprechenden Matches $P_V$, $P_{Tr}$ oder $P_{Te}$ hinzugefügt (Zeilen
\ref{alg:sp:20}-\ref{alg:sp:21}). Durch diese Auswahl wird verhindert, dass
Matches zwischen $D_V$, $D_{Tr}$ oder $D_{Te}$ fälschlicherweise in $P_V$,
$P_{Tr}$ oder $P_{Te}$ aufgenommen werden.

## Auswahl der Komponenten

Die Komponenten des selbstkonfigurierenden Systems wurden in der Analyse in
Kapitel 3 und im Design in Kapitel 4 vorgestellt. Dabei ist der Label Generator
(@sec:ana_lbl), der Blocking Schema Lerner (@sec:ana_bs), und der Similarity
Lerner (@#sec:fit_comp) fix. Alternativen gibt es für jeweils für Parser,
Präprozessor, Fusion-Lerner, Klassifikator und Indexer.

Die Datensätze aus @sec:datasets liegen alle Samt im CSV-Format vor. Weshalb
auch die gesplitteten Datensätze ins CSV-Format geschrieben wurden. Deshalb ist
der Parser ein CSV-Parser. Da während der Thesis nicht genügend Zeit war, um die
Datentypen der Attribute zuverlässig zu erkennen, werden alle Attribute als
Strings behandelt, was unter anderem die Wahl geeigneter Prädikate für den
Blocking Schema Lerner vereinfacht.

Weiterhin sind alle Datensätze aus @sec:datasets in englischer Sprache, weshalb
der Präprozessor bekannt englische Stopwörter herausfiltert und anschließend
alle Attribute in kleinschreibweise konvertiert. Weitere Attributs- bzw.
Datensatzspezifische Anpassungen werden nicht durchgeführt.

Um die Hyperparameter der Klassifikatoren zu erlernen muss der
Fusion-Lerner insbesondere Wissen, wie deren API Schnittstelle ist, damit er die
Modelle trainieren und auswerten kann. Aufgrunddessen und weil die
Implementierung von verschiedenen Lernverfahren und Klassifikatoren nicht
Schwerpunkt der Thesis ist, wurde für die Umsetzung die Python Maschine Learning
Bibliothek Scikit-learn [@PVG.EA:Scikitlearn:11] eingesetzt. Diese bietet ein
breites Spektrum an Funktionen:

* Klassifikation, bestimmen zu welcher Klasse ein Objekt gehört.
* Regression, einen fortlaufenden Wert eines Objektes vorhersagen.
* Clustering, automatisches gruppieren von Objekten.
* Dimensionsreduktion, reduzieren der Anzahl zu betrachtender zufälliger
  Variablen.
* Modellauswahl, vergleichen, validieren und auswählen von Parametern und
  Modellen.
* Vorverarbeitung, Eingabetransformation und Normalisierung.
* Evaluation, berechnen der Effizienz und Qualität von Modellen.

Für den Fusion-Lerner sind dabei das Module zur Modellauswahl und Evaluation
interessant. Dieser ist zudem die einzige Komponente, die ihre Aufgabe
parallelisieren kann, da dies in Scikit-learn transparent implementiert ist. Zur
Auswahl stehen ein Grid Search und eine zufällige Suche mit begrenzter Tiefe.
Tests mit dem größten Datensatz (NCVoter) haben ergeben, dass die Zeit, die eine
Grid Search benötigt (2-4 Stunden), für die Evaluation vertretbar ist. Deshalb
wird die Klasse `GridSearchCV` verwendet. Für die Kreuzvalidierung wird der
Stratified K-Fold (implementiert in `StratifiedKFold`) verwendet, da dieser das
Verhältnis der Ground Truth beibehält.

Die als Klassifikator nutzbaren Komponenten müssen zur Grid Search kompatibel
sein. Das Scikit-learn Klassifikationsmodul beinhaltet dazu SVMs, DecisionTrees,
neuronale Netze und mehr. Diese Implementierungen können ohne Anpassungen mit
der Scikit-learn `GridSearchCV` verwendet werden. In den ähnlichen Arbeiten
wurden hauptsächlich DecisionTrees und Support Vector Machines verwendet.
Deshalb werden diese beiden für die Evaluation eingesetzt. Die entsprechenden
Scikit-learn Klassen sind `DecisionTreeClassifier`, `SVC` für SVM mit RBF-Kernel
und `LinearSVC` für SVM mit Linearkernel.

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_index_bt.pdf}
        \caption{MDySimII vs MDySimIII - Bauzeit}
        \label{fig:IIvsIIIbt}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_memusg.pdf}
        \caption{MDySimII vs MDySimIII - Speicherverbrauch}
        \label{fig:IIvsIIImem}
    \end{minipage}
\end{figure}
```

Der Indexer kann durch den MDySimII oder den MDySimIII aus @sec:anaindxer
besetzt werden. Beide Indexer wurden auf dem ferbl-9k-1k-1 Datensatz mit Ground
Truth gegeneinander getestet. In @fig:IIvsIIIbt sind die Bauzeiten werden die
Bauzeiten der beiden Indexer verglichen. Aufgrund der komplexeren Struktur
schneidet der MDySimIII hier leicht schlechter ab. @fig:IIvsIIImem zeigt die
Speicherbedarf beider Indexer. Überraschend ist, dass MDySimII fast das doppelte
an Speicher benötigt als MDySimIII. Der Grund dafür kann in der Precision-Recall
Kurve in @fig:IIvsIIIprc abgelesen werden. Zwar erreicht der MDySimII einen
Recall von über 80 %, doch die Precision ist nahezu 0 %. Im Gegensatz dazu
errreicht der MDySimIII lediglich knapp 60 % Recall, dafür ist die Precision
nahe 100 %. Die schlechte Precision des MDySimII macht sich direkt in den
Anfragezeiten in @fig:IIvsIIIqry bemerkbar. Die durchschnittliche Anfragezeit
für den MDySimII liegt bei 10^-2^ und ist damit um 10^-2^ dramatisch schlechter
als die des MDySimIII mit 10^-4^. Der Datensatz ist mit 10.000 Einträgen relativ
klein, dennoch macht sich bereits hier ein deutlicher Unterschied, sowohl in der
Qualität als auch in der Effektiviät, bemerkbar. Aufgrund der schlechten
Precision ist davon auszugehen, dass der MDySimII für größe Datensätze Anfragen
nur sehr langsam beantworten kann und das die Anfragezeiten größer werden je
größer die Datensätze werden. Deswegen wird in der weiteren Evaluation der
MDySimIII als Indexer genutzt.

```tex
\begin{figure}
    \centering
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_prc.pdf}
        \caption{MDySimII vs MDySimIII - Precision-Recall Kurve}
        \label{fig:IIvsIIIprc}
    \end{minipage} \\
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_tc_query.pdf}
        \caption{MDySimII vs MDySimIII - Anfragezeiten}
        \label{fig:IIvsIIIqry}
    \end{minipage}
\end{figure}
```

## Auswahl der Freien Parameter {#sec:free_params}

Auf der Validierungsmenge wurden robuste Parameter für die freien Parameter zur
die Evaluierung gewählt. Robust bedeutet, dass diese nicht optimal für jeden
Datensatz sind, sondern gute Ergebnisse für alle Datensätze liefern und
gleichzeitig verhindern, dass die Entity Resolution katastrophal versagt.

### Labelgenerator

Die freien Parameter des Labelgenerators sind die Fenstergröße, die untere
und obere Schwelle, sowie die maximalen Matches und Non-Matches. Dabei werden
die Schwellen nur benötigt falls keine Ground Truth existiert und diese vom
Label Generator selbstständig erzeugt wird.

#### Fenstergröße

Zur Bestimmung einer geeigneten Fenstergröße wurde der Label Generator ohne
Matches betrachtet, da diese Variante deutlich empfindlicher auf die
unterschiedlichen Parameter reagiert und dadurch der Effekt unterschiedlicher
Fenstergrößen $w$ einfacher ausgewertet werden kann. Dazu wurde die untere
Schwelle $lt$ und die obere Schwelle $ut$ in 0.1 Schritten bis 0.5 erhöht und
mit den Fenstergrößen 2, 5, 10 und 20 verglichen. Die maximalen Matches wurden
mit 10 % der Gesamtmenge und die maximalen Non-Matches mit 25 % der Gesamtmenge
bestimmt. Bei dem genutzten NCVoter Datensatz sind die maximalen Matches bei 551k
und die maximalen Non-Matches bei 1.337k. In den [@tbl:w2; @tbl:w5; @tbl:w10;
@tbl:w20;] sind die Ergebnisse für die unterschiedlichen Fenstergrößen
dargestellt. Die obere Schwelle $ut$ hatte dabei keine entscheidende Auswirkung,
sodass lediglich die untere Schwelle $lt$ betrachtet wird. Für jede Schwelle
wurden Matches (P), Non-Matches (N), Pairs Completeness (PC) und Pairs Qualtity
(PQ) analysiert. Die Pairs Quality liefert keinen entscheidenden Hinweis auf
ein geeignetes Fenster, da diese sich lediglich zwischen 1 % und 11 % hin und
her bewegt. Beim Blick auf die Pairs Completeness zeigt sich, dass diese sich
zwischen 13 % und 16 % für alle Fenstergrößen bewegt, mit Außnahme von $w=2$.
Dort ist die Pairs Completeness für $lt \leq 0.3$ mit 95 % deutlich besser. Für
$w=2$ werden am wenigsten Paare gebildet, wie sich bei $lt=0.1$ bemerkbar macht,
da hier lediglich 8k Non-Matches generiert wurden. Bei $lt=0.2$ gibt es mit 655k
dann allerdings schon eine große Auswahl an Non-Matches und mit $lt=0.3$ wurden
bereits mehr Paare generiert als das Maximum. Aufgrund dieser Ergebnisse wird
die Fenstergröße für die weitere Evaluation mit 2 bestimmt.

```tex
\begin{figure}
```

| lt (w=2) | P    | N      | PC   | PQ   |
|----------+------+--------+------+------|
| 0.1      | 551k | 8k     | 0.95 | 0.11 |
| 0.2      | 551k | 655k   | 0.95 | 0.11 |
| 0.3      | 551k | 1,377k | 0.95 | 0.11 |
| 0.4      | 433k | 1,377k | 0.15 | 0.01 |
| 0.5      | 433k | 1,377k | 0.16 | 0.10 |

: Ergebnis mit Fenstergröße 2 {#tbl:w2}

| lt (w=5) | P    | N      | PC   | PQ   |
|----------+------+--------+------+------|
| 0.1      | 551k | 32k    | 0.15 | 0.01 |
| 0.2      | 551k | 1,377k | 0.15 | 0.01 |
| 0.3      | 551k | 1,377k | 0.15 | 0.01 |
| 0.4      | 551k | 1,377k | 0.16 | 0.11 |
| 0.5      | 551k | 1,377k | 0.16 | 0.11 |

: Ergebnis mit Fenstergröße 5 {#tbl:w5}

| lt (w=10) | P    | N      | PC   | PQ   |
|-----------+------+--------+------+------|
| 0.1       | 551k | 66k    | 0.15 | 0.01 |
| 0.2       | 551k | 1,377k | 0.15 | 0.01 |
| 0.3       | 551k | 1,377k | 0.15 | 0.01 |
| 0.4       | 551k | 1,377k | 0.16 | 0.11 |
| 0.5       | 551k | 1,377k | 0.13 | 0.13 |

: Ergebnis mit Fenstergröße 10 {#tbl:w10}

| lt (w=20) | P    | N      | PC   | PQ   |
|-----------+------+--------+------+------|
| 0.1       | 551k | 119k   | 0.15 | 0.01 |
| 0.2       | 551k | 1,377k | 0.15 | 0.01 |
| 0.3       | 551k | 1,377k | 0.15 | 0.01 |
| 0.4       | 551k | 1,377k | 0.16 | 0.11 |
| 0.5       | 551k | 1,377k | 0.13 | 0.12 |

: Ergebnis mit Fenstergröße 20 {#tbl:w20}

```tex
\end{figure}
```

#### Untere und Obere Schwelle

![Histogramm der TF/IDF Ähnlichkeiten der Matches aus dem NCVoter
Validierungsdatensatz mit 50566 Einträgen. Die Datensätze wurden in
5 % Schritten nach Ähnlichkeit zusammengefasst.
](./images/ncvoter_matches_histo.svg){#fig:match_histo}

Die untere Schwelle $lt$ legt fest, bis zu welchem Ähnlichkeitswert Paare als
Non-Matches betrachtet werden und die obere Schwelle $ut$ legt fest, ab welchem
Ähnlichkeitswert Paare als Matches betrachtet werden, dabei gilt stets $lt \leq
ut$. In einem Experiment wurden $lt$ und $ut$ in 0.1 Schritten betrachtet und so
alle Konfigurationen bis 1.0, auf dem NCVoter-Datensatz ausprobiert. Für das
Fenster wurde der bereits bestimmte Wert von 2 gesetzt. Zur Auswertung wurden
Pairs Completeness, Pairs Quality und die Ground Truth analysiert. Anhand der
Pairs Completeness und Pairs Qualtity kann betrachtet werden, wie gut ein
Blocking Verfahren auf der generierte Ground Truth funktioniert. Durch die
gefilterte Ground Truth hingegen kann herausgefunden werden, wie viele Ground
Truth Paare für den Fusion-Lerner zur Verfügung stehen. Die [@tbl:recall;
@tbl:fp; @tbl:fn] betrachten nacheinander die Pairs Completeness, die Matches
und die Non-Matches. Die Pairs Quality ist uninteressant, da deren Werte relativ
konstant bei 0.1 liegen, mit einer Varianz von 0.03. In @tbl:recall ist gut zu
sehen, dass die Pairs Quality zwischen einer $ut$ von 0.1 und 0.4 immer eine
gute Pairs Qualtity von 95 % erzeugt. Der Blick auf das erlernte Blocking Schema
zeigt, dass dieses auch immer dasselbe ist. Dies trifft auch noch teilweise für
$ut=0.5$ zu, allerdings nur für $lt \leq 0.3$. Für alle $ut > 0.5$ variieren
die Blocking Schema, wobei unabhängig von $lt$ keines über 17 % Pairs
Completeness kommt. In @fig:match_histo sind die Ähnlichkeiten der 50k
tatsächlichen Matches als Histogramm dargestellt. Der Großteil der Matches hat
eine Ähnlichkeit zwischen 0.2 und 0.5, dadurch fällt für $ut > 0.5$ der Recall
dramatisch ab, weil der Großteil der Matches nicht erfasst wurde und zudem
teilweise Teil der Non-Matches ist. Für $ut = 0.5$ sind die Recallwerte für $lt
\leq 0.3$ noch gut. Wird $lt$ weiter erhöht fällt der Recall, da sich nun zu
viele tatsächliche Matches in den Non-Matches befinden.

|  PC | 0.1  | 0.2  | 0.3  | 0.4  | 0.5  | 0.6  | 0.7  | 0.8  | 0.9  | 1.0  |
|----:+------+------+------+------+------+------+------+------+------+------|
| 0.1 | 0.95 | 0.95 | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.15 | 0.15 | 0.13 |
| 0.2 |      | 0.95 | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.15 | 0.15 | 0.13 |
| 0.3 |      |      | 0.95 | 0.95 | 0.95 | 0.15 | 0.15 | 0.16 | 0.13 | 0.13 |
| 0.4 |      |      |      | 0.95 | 0.15 | 0.16 | 0.16 | 0.16 | 0.14 | 0.14 |
| 0.5 |      |      |      |      | 0.16 | 0.16 | 0.13 | 0.14 | 0.14 | 0.06 |
| 0.6 |      |      |      |      |      | 0.13 | 0.14 | 0.14 | 0.16 | 0.06 |
| 0.7 |      |      |      |      |      |      | 0.14 | 0.10 | 0.16 | 0.06 |
| 0.8 |      |      |      |      |      |      |      | 0.10 | 0.16 | 0.06 |
| 0.9 |      |      |      |      |      |      |      |      | 0.14 | 0.06 |
| 1.0 |      |      |      |      |      |      |      |      |      | 0.06 |

: Pairs Completeness {#tbl:recall}

Deshalb werden in den [@tbl:fp; @tbl:fn] lediglich $ut$-Werte kleiner 0.6
betrachtet. Die maximalen Matches, die der Label Generator erzeugen darf, liegen
bei 10 % der Gesamtmenge und betragen 551k. Bei den Non-Matches ist das Limit 25
% und damit 1,3 mio. Für die künstliche Anreicherung der gefilterten
Non-Matches, stehen jedoch alle erzeugten Non-Matches zur Verfügung,
dementsprechend je höher $lt$ desto mehr Non-Matches und umgekehrt für $ut$. In
@tbl:fp ist zu sehen, dass für $ut \leq 0.4$ die Ausgangsmenge der Matches auf
das Maximum beschränkt wurde. Da jeweils die Matches mit der höchsten
Ähnlichkeit genutzt werden, sind diese Mengen identisch, weshalb auch die
gefilterte Mengen mit jeweils 300k Datensätzen, aufgrund desselben Blocking
Schema, identisch sind. Für $ut=0.5$ ist die Ausgangsmenge kleiner als das
Maximum. Die gefilterte Menge beträgt in diesem Fall 288k. In jedem Fall sind
nach dem Filtern genügend Matches vorhanden, um einen Klassifikator zu
trainieren, auch wenn diese 6-Mal soviele Matches beinhaltet, wie die
tatsächlichen Matches.

| Matches | 0.1       | 0.2       | 0.3       | 0.4       | 0.5       |
|--------:+-----------+-----------+-----------+-----------+-----------|
|     0.1 | 551k/300k | 551k/300k | 551k/300k | 551k/300k | 443k/288k |
|     0.2 |           | 551k/300k | 551k/300k | 551k/300k | 443k/288k |
|     0.3 |           |           | 551k/300k | 551k/300k | 443k/288k |
|     0.4 |           |           |           | 551k/300k | 443k/287k |
|     0.5 |           |           |           |           | 443k/273k |

: Filtered Matches {#tbl:fp}

In @tbl:fn werden die Anzahl der Non-Matches dargestellt. Für $lt=0.1$ sind
insgesamt nur 8k Paare erzeugt worden, weil das TF/IDF Blocking die meisten der
sehr unähnlichen Paare ausschließt. Nach dem Filtern durch das Blocking Schema
sind keine Non-Matches mehr vorhanden, da das Blocking Schema verhindert, dass
diese offensichtlichen Non-Matches zusammen gruppiert werden. Für $lt=0.2$ gibt
es ein ähnliches Bild. Zwar ist die Anzahl der Ausgangsmenge mit 655k deutlich
höher, dennoch werden lediglich 66 Paare gefunden, die zusammen gruppiert und
damit nicht ausgefiltert wurden. Interessanter wird es erst ab $lt=0.3$. Hier
wird erste Mal das Maximum der Ausgangsmenge mit 1377k erreicht. Die gefilterten
Non-Matches betragen 2430, was im Vergleich zu den Matches immer noch sehr wenig
ist, aber durchaus genügt um einen Klassifikator zu trainieren. Mit $lt=0.4$
erhöht sich diese Anzahl nochmals, um das 5-fache. Ein Blick auf
@fig:match_histo zeigt, dass sich dadurch der Großteil der Matches in den
Non-Matches befindet.

| Non-Matches | 0.1  | 0.2     | 0.3        | 0.4         | 0.5        |
|------------:+------+---------+------------+-------------+------------|
|         0.1 | 8k/0 | 8k/0    | 8k/0       | 8k/0        | 8k/0       |
|         0.2 |      | 655k/66 | 655k/66    | 655k/66     | 655k/66    |
|         0.3 |      |         | 1377k/2430 | 1377k/2430  | 1377k/2430 |
|         0.4 |      |         |            | 1377k/13916 | 1377k/11k  |
|         0.5 |      |         |            |             | 1377k/8k   |

: Filtered Non-Matches {#tbl:fn}

Anhand dieser Werte wird $lt$ mit 0.3 festgelegt, da ab hier genügend Paare zum
Trainieren eines Klassifikators zur Verfügung stehen und gleichzeitig die Menge
der Matches, die fälschlicherweise in die Non-Matches aufgenommen werden noch
relativ gering ist. Die $ut$ wird ebenfalls auf 0.3 festgelegt, da so am meisten
Matches auch in den Matches landen.

#### Maximale Paare der Ground Truth

Sowohl mit, also auch ohne Ground Truth Match werden dem Label Generator
mitgeteilt, wie viele Matches $max_p$ bzw. Non-Matches $max_n$ in der Ground
Truth enthalten sein dürfen. Im Fall ohne Ground Truth Matches werden jeweils
die $max_p$ Matches bzw. $max_n$ Non-Matches ausgewählt, die die höchste
Ähnlichkeit haben. Im Fall mit Ground Truth Matches werden sowohl Matches als
auch Non-Matches nach ihrer Ähnlichkeitsverteilung ausgewählt. Mithilfe dieser
Ground Truth sucht der Blocks DNF Generator nach dem besten Blocking Schema.
Anschließend wird die Ground Truth, anhand des Blocking Schema, gefiltert,
sodass diese nur Paare enthält, die einen gemeinsamen Blockschlüssel haben. In
diesem Schritt werden sehr viele Non-Matches herausgefiltert, da das der primäre
Zweck des Blocking Schema ist. Damit für den Fusion-Lerner genügend Non-Matches
zur Verfügung stehen, werden die Non-Matches mit allen generierten Non-Matches
des Label Generators angereichert. Hierbei spielt $max_n$ keine Rolle. Die
Auswirkungen dieser Parameter wurden auf dem NCVoter Datensatz mit Ground Truth
und ohne Ground Truth mit Fenstergröße $w=2$ und Schwellen $lt=ut=0.3$
evaluiert. Ausgangssituation ist für $max_p$ 10 % und $max_n$ 25 % der
Gesamtmenge, welche bereits zur Ermittlung der Fenstergröße und der Schwellen
genutzt wurden. Getestet wurden zunächst größere Werte mit (0.5, 2.5), (1, 5),
(5, 25), (10, 50). Dabei wuchsen die Matches leicht auf 650k und die Non-Matches
bis stark auf 19 Mio. Auf das Blocking Schema hat die vergrößerte Ground Truth
in keinem Fall einen Einfluss. Die Betrachtung der Matches im größten Fall (10,
50) ergibt, dass diese sich lediglich um ca. 100k verändert hat. Die neu
hinzugekommen Matches wurden durch die Filterung auf ein paar Hundert reduziert.
Das bedeutet, dass die erweiterten Matches die Ausdrücke des Blocking Schema,
im Bezug auf Pairs Completeness, abwerten und zwar je mehr desto größer die
Anzahl der Matches. Allerdings haben sich die Non-Matches mit 19 mio. dramatisch
erhöhtet und durch die Filterung bleiben lediglich die bekannten 2k übrig,
sodass die erhöhte Menge sich positiv auf das Reduction Ratio und die Pairs
Qualtity auswirkt. Damit wird der Negativeffekt auf die Pairs Completeness
aufgehoben und das Blocking Schema bleibt dasselbe. Neben größeren Werten wurden
auch kleinere getestet (0.0001, 0.00025), (0.001, 0.0025), (0.01, 0.025).
Evaluation der Werte läuft aktuell noch ...

* Block Size Filter, Labelgenerator

### DNF Blocks Lerner

#### Maximale Konjunktion/Disjunktion

* Anzahl/Größe der Konjunktionen, Blocking Scheme

#### Stopptoken-Filter

Tests its09 - its14

* "Stop Token Filter" -> 100, Blocking Scheme

#### Schlechter Block-Filter

* ab 10 %

#### Blockschlüsselgenerator

### Fusion-Lerner

* Search, Crossval, Metric ,Fusion-Lerner

### Geeignete Prädikate

* Einfluss der Prädikate (commonToken, excactMatch, q-Qram, suffixe, prefixe),
  Blocking Scheme

* Stringähnlichkeiten (Levenshtein, Damerau, Jaro, Ratio), SimLearner
* MDySimII vs MDySimIII

* Schwelle des Klassifikators (`predict_proba`) verschieben. (ROC vs average
  precision)

=> Ziel: optimales System

\TODO{Zu Implementierung hinzufügen} Laut Kejriwal & Miranker
[@KM:Unsupervised:13] bieten Werte $>3$ keine wesentliche Verbesserung.

## Baseline vs GT partial vs GT full

Validierungsmenge

* Pair completeness/Reduction Ratio/Pairs Quality
* Presion/Recall/F-measure
* Memory usage
* Insert/Query Times

## Human Baseline

* Train/Train
* Train/Test

## Grund Truth vs No Ground Truth
