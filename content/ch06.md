# Zusammenfassung und Ausblick \label{chap:conclusion}

Dieses Kapitel gibt eine Zusammenfassung der Hauptbeiträge dieser Arbeit und
einen Ausblick in welche Richtungen diese weiterentwickelt werden können.

## Zusammenfassung

Die vorliegende Arbeit untersucht das Problem der Selbstkonfiguration eines
Entity Resolution Workflows für Event Stream Processing Systeme. In Kapitel
\ref{chap:101} wurden dazu die drei Anforderungen **Niedrige Latenzen**,
**Datenmodifikation zur Laufzeit** und **Hohe Trefferrate** festgelegt. Diese
Anforderungen können jedoch nur durch einen Kompromiss zwischen Qualität und
Effizienz erfüllt werden. Hauptproblem bei der Verwendung eines Entity
Resolution Systems ist die Anpassung der freien Parameter auf die Domäne der
Daten. Die drei entscheidenden Parameterkonfigurationen dabei sind das Blocking
Schema, die Ähnlichkeitsmetriken und die Klassifikation, für welche in Kapitel
\ref{chap:analysis} Verfahren analysiert und entwickelt wurden, die eine
automatische Bestimmung der Parameter ermöglichen. Die Grundlage dieser
Verfahren bilden gelabelte Daten, die vorklassifizierte Duplikatspaare und nicht
Duplikatspaare beinhalten. Für viele Datensätze sind jedoch keine gelabelten
Daten verfügbar und das manuelle Erzeugen ist sowohl zeit- als auch
kostenintensiv. Aufgrunddessen wurde ein Verfahren entwickelt, dass automatisch
gelabelte Daten synthetisiert, damit es möglich ist, dass das System sich
unüberwacht auf einer neuen Domäne selbst adaptiert. Des Weiteren wurde ein
Blocking Verfahren weiterentwickelt, dass mit geeigneten Parametern, alle drei
Anforderungen erfüllt. In Kapitel \ref{chap:design} wurden die analysierten und
entwickelten Verfahren zu einem Gesamtsystem zusammengefügt, dass sich dem
Anwender gegenüber transparent selbstkonfiguriert und anhand der Konfiguration
und der Bestandsdaten, Anfragen aus einem Eventstrom beantwortet. Die Evaluation
in Kapitel \ref{chap:evaluation} untersuchte zunächst die noch offenen (nicht
selbstkonfigurierbaren) freien Parameter und wählte robuste Werte für diese aus.
Die Qualität und Effizienz des Systems wurde danach gegenüber einer manuell
bestimmten Baseline überprüft. Dabei zeigte sich, dass es mit einigen
praktischen Hinweisen in angemessener Zeit möglich ist, eine Konfiguration zu
bestimmen, die qualitativ gute Ergebnisse ermittelt. Bezüglich der Anforderungen
an die Effizienz versagt die Baseline jedoch komplett. Diese ist maßgeblich vom
Blocking Schema abhängig, allerdings werden für dieses nur Qualitätsparameter
konfiguriert, die implizit Auswirkungen auf die Effizienz haben. Daher ist es
ohne bzw. mit nur geringer Kenntnis über die Funktionsweise des
Blocking-Verfahren, nur schwer möglich abzuschätzen, wie sich eine Konfiguration
auf die Effizienz auswirkt. Die Selbstkonfiguration kann dies deutlich besser
beurteilen und erreicht deshalb ca. 20-Mal bessere Effizienzwerte, zudem können
gegenüber der Baseline theoretisch 15 % mehr Duplikate gefunden werden. Während
das selbstkonfigurierte Blocking Schema in der Evaluation überzeugte, erwies
sich die Konfiguration des Klassifikationsmodells als wenig stabil, sodass
unterschiedliche Konfigurationen auf denselben Ausgangsdaten, z. T. bis zu 40 %
in der Precision variieren. Abschließend wurde die Konfiguration aus den
automatisch synthetisierten gelabelten Daten evaluiert. Gegenüber den
tatsächlichen gelabelten Daten, sind diese qualitativ fragwürdig, dennoch wird
ein Blocking Schema gelernt, dass in Qualität und Effizienz fast ebenbürtig ist.
Zum Trainieren des Klassifikationsmodells sind die gelabelten Daten jedoch
unbrauchbar, weshalb keine sinnvolle Klassifikation der Kandidatenmenge
durchführt werden kann.

## Ausblick

Die in dieser Arbeit vorgestellten und entwickelten Verfahren und Algorithmen
haben eine Reihe von Beiträgen und Verbesserungen der aktuellsten Entity
Resolution Methoden vorgenommen. Angepasst wurde das Blocking-Schema
Lernverfahren, für die Anforderungen an ESP-Systeme und das DySimII
Blocking-Verfahren, für das neue Blocking-Schema. Des Weiteren wurde im
DySimII-Verfahren eine essentielle Schwäche korrigiert. Entwickelt wurde ein
Lernverfahren zur Auswahl von Ähnlichkeitsfunktionen und ein Verfahren zur
Bestimmung der Parameter eines Klassifikators. Zudem wurde an der
Synthetisierung von Ground Truth Paaren gearbeitet. Alle Beiträge bieten jedoch
noch einige Möglichkeiten der Weiterentwicklung und Probleme, die es zu lösen
gilt, welche im Folgenden betrachtet werden.

**Selbstkonfiguration bzgl. einer Mindestqualität oder -effizienz:** Die
aktuelle Selbstkonfiguration versucht stets einen Kompromiss zwischen Qualität
und Effizienz einzugehen, ohne dabei einer Mindestanforderung an Qualität oder
Effizienz genüge zu tun. In der Evaluation sind aus Effizienzgründen,
beispielsweise ein Großteil der Blockingprädikate ausgeschlossen worden, womit
das System einen Durchsatz von mindestens 500 Anfragen pro Sekunde erreicht hat.
Für ein System, dass maximal 10 Anfragen pro Sekunde beantworten muss, wäre ein
Teil dieser Prädikate durchaus noch infrage gekommen, wodurch die Qualität des
Ergebnisses hätte verbessert werden können. Anhand solcher Vorgaben kann
versucht werden, weitere freie Parameter, automatisch konfigurierbar zu machen.

**Optimierung der Disjunktion des Blocking-Schema:** Beim Lernen des Blocking
Schema, werden nach der Bewertung eines Ausdrucks, dessen Datensatzpaare auf
die Ground Truth abgebildet. Damit bei der Disjunktion der Ausdrücke diese
vergleichbar sind und die Blöcke nicht erneut gebaut werden müssen, um eine
Disjunktion zu bewerten. Dabei werden vor allen Dingen die Non-Matches
unterrepräsentiert, da nur eine kleine Auswahl derer Teil der Ground Truth ist.
Über das Bauen der eigentlichen Blöcke kann daher deutlich genauer das F-Measure
der Disjunktion ermittelt werden. Allerdings dauert dieser Vorgang auch deutlich
länger, weil Blockschlüssel und Blöcke aus mehreren Attributen erzeugt werden.
Eine Idee dieses Problem zu lösen ist ein Branch-and-Bound-Verfahren zu nutzen,
dass anhand einer Heuristik, beispielsweise dem maximal erreichbaren F-Measure,
nur sinnvolle Disjunktionen testet. Eine optimistische Heuristik ist
beispielsweise, dass sich die Recallwerte der einzelnen Ausdrücke addieren und
die Precision sich dabei mittelt.

**Parallelisierung des Lernens:** In der Selbstkonfigurationsphase dauert das
Lernen des Blocking Schema am längsten, da für jeden zu prüfenden Ausdruck der
Indexer alle Blöcke bauen muss. Dabei können die Ausdrücke unabhängig
voneinander bewertet werden, was eine reibungslose Parallelisierung ermöglicht.
Dieser Prozess benötigt jedoch z.T. sehr viel Arbeitsspeicher, weshalb
Multithreading bzw. Multiprocessing keine Optionen sind. Denkbar ist aber die
Ausführung auf einem Cluster von Rechnern. Die hierbei genutzen Daten sind
statisch, deshalb kann dazu ein Batchverfahren wie MapReduce eingesetzt werden.

**Parallelisieren der Anfragen**: Zur Zeit werden die Anfragen auf einem CPU
Kern von einem Thread bearbeitet. Durch die Parallelisierung kann hier die
Effizienz deutlich gesteigert werden, vor allem da die Ähnlichkeitsberechnung,
aufgrund der Teilvektoren, nicht vollständig vorausberechnet wird. Eine Idee
ist, die Blöcke, die durch das Blocking-Verfahrens gebildet werden, auf mehrere
Threads, Prozesse oder Rechner zu verteilen. Dadurch können die Ähnlichkeiten
verschiedener Attribute gleichzeitig berechnet bzw. abgerufen werden. Damit
allerdings kein Knoten zum Flaschenhals wird, benötigt es einen Algorithmus, der
die Blöcke nach Bearbeitungszeit und Anfragehäufigkeit verteilt und eventuell
nach Last anpasst. Einen Ansatz hierfür beschreibt Kolb in [@Kol:Effiziente:14].
Dabei ist der Algorithmus für MapReduce-Verfahren optimiert, liefert jedoch
einen guten Einstieg in die Problematik. Ein interessanter Effekt des Verteilens
der Blöcke auf mehrere Rechner ist, dass damit auch Datensätze verarbeitet
werden können, die deutlich mehr Arbeitsspeicher benötigen als technisch auf
einem einzelnen Rechner möglich sind.

**Verbessern des Bewertungsmaß des Similarity Lerners**: Der Similarity Lerner
bewertet Ähnlichkeitsmetriken anhand der Average Precision. Hat dieser genügend
Daten zum Lernen und einen großen Pool von Ähnlichkeitsmetriken zur Auswahl,
werden gute Metriken gelernt, mit welcher ein Klassifikator qualitativ gute
Entscheidungen treffen kann. Allerdings sind die Average Precision Werte der
Metriken pro Attribut oft im Bereich 10^-1^, was vor allem bei kleineren
Datensätzen dazu führt, dass die Auswahl der Ähnlichkeitsmetriken suboptimal
ist. Hier gilt es herauszufinden, wie die Qualtität der Ähnlichkeitsmetriken
besser differenziert werden kann, um robustere Entscheidungen zu treffen.

**Lernen von Hyperparametern der Ähnlichkeitsmaße:** Der Similarity Lerner wählt
aktuell aus einer Menge von vorkonfigurierten Ähnlichkeitsmaßen aus. Dabei ist
aus @sec:similarity bekannt, dass Ähnlichkeitsmaße z.T. mehrere Parameter haben,
die je nach Datendomäne entscheidenden Einfluss auf die Qualität haben können.
Ähnlich zum der Fusion-Lerner, könnte der Similarity Lerner ebenfalls ein
Parametergrid für eine Ähnlichkeitsfunktion abtestet. Dazu muss allerdings
sichergestellt sein, dass für jeden Parameterwert stets die Dreiecksungleichung
erfüllt ist.

**Verbessern des Bewertungsmaß des Fusion-Lerners:** Aktuell wählt der
Fusion-Lerner die besten Parameter für einen Klassifikationsmodell aus, indem
dieses über das F-Measure bewertet wird. Die Evaluation hat jedoch gezeigt, dass
für viele der unterschiedlichen Parametereinstellungen sehr ähnliche Werte
berechnet werden, die sich je nach gesubsampelter Ground Truth zu Gunsten der
einen oder anderen Parameter ändern. Dementsprechend variiert die Qualität der
Klassifikation teilweise stark. Auch hier gilt es noch herauszufinden, wie die
Parameter besser bewertet werden können, sodass die Auswahl auch auf
unterschiedlich gesubsampelten Ground Truths robustere Ergebnisse erzielt.

**Kalibrierung der Wahrscheinlichkeitsschwellen:** Die aktuell gewählten
Wahrscheinlichkeitsschwellen der Klassifikatoren entsprechen den
Standardeinstellungen der Scikit-learn Bibliothek. Die Evaluation hat gezeigt,
dass durch eine Kalibrierung in fast allen Fällen die Precision bei minimalem
Recallverlust dramatisch verbessert werden kann.

**Aktives Lernen der Ground Truth:** Das Verfahren von Kejriwal & Miranker aus
[@KM:Unsupervised:13] kann zwar ein schwache Ground Truth generieren, diese
trennt jedoch Matches von Non-Matches durch eine harte Ähnlichkeitsschwelle, was
nicht den realen Daten entspricht. Ein anderer Ansatz möglichst effizient eine
Ground Truth zu Erzeugen, sind aktive Lernmethoden (vgl. @sec:active_learn),
welche selbständig eine kleine Menge herausfordernder, manuell zu
klassifizierende, Datenpaare bestimmen und so sequentielle eine Ground Truth
synthetisieren. Eine interessante Weiterentwicklung ist es, diese beiden Ansätze
zu kombinieren und die automatisch erzeugte schwache Ground Truth sequentielle,
durch Auswahl herausfordernder Paare, zu verbessern und die harte
Ähnlichkeitsschwelle zu lockern.

<!-- **Ressourcenbeschränkung**: Durch Python NCVoter hochrechnen, wie viele -->
<!-- Datensätze maximal möglich auf max VM mit 256 GB Ram. -->
<!-- **De-Duplikation:** -->
