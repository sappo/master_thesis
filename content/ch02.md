# Grundlagen

## Entity Resolution

Die Methoden zur Duplikatserkennung stammen ursprünglich aus dem
Gesundheitsbereich und wurden erstmal 1969 von Felegi & Sunter [@FS:Theory:69]
formal formuliert. Je nach Fachgebiet gibt es unterschiedliche Fachbegriffe.
Statistiker und Epidemiologen sprechen von *record* oder *data linkage*, während
Informatiker das Problem unter *entity resolution*, *data* oder *field
matching*, *duplicate detection*, *object identification* oder *merge/purge*
kennen. Identifiziert werden sollen dabei beliebige Entitäten. Die Schwierigkeit
dabei ist allerdings, dass Entitäten nicht durch ein einzigartiges Attribut
identifiziert werden können. Zudem sind die Datensätze oft fehlerhaft,
beispielsweise durch Rechtschreibfehler oder unterschiedliche Konventionen. Die
Methoden zur Entity Resolution (ER) arbeiten meist auf Datensatzpaaren und
liefern als Ergebnis eine Menge von Übereinstimmungen. Damit eine
Übereinstimmung zwischen zwei oder mehr Entitäten festgestellt werden kann
müssen diese Verglichen werden und ein Ähnlichkeitswert (engl. similarity score)
bestimmt werden. Dieser Ähnlichkeitswert gibt die Intensität der Übereinstimmung
an.

Zur Bestimmung der Ähnlichkeit eines Datensatzpaares unterscheiden Elmagarmid et
al. [@EIV:Duplicate:07] zwischen Attributsvergleichs- (engl. field matching) und
Datensatzvergleichsmethoden (engl. record matching). Methoden zum
Attributsvergleich sind zeichenbasierend (edit distance, affine gap distance,
Jaro distance metric oder Q-gram distance), tokenbasierend (atomic strings,
Q-grams mit tf.idf), phonetisch (soundex) oder nummerisch. Die
Datensatzvergleichsmethoden sind probabilistisch (Naive Bayes), überwachtes bzw.
semi-überwachtes Lernen (SVMLight [@Joa:Svmlight:99], Markov Chain Monte Carlo
[@SD:Entity:06]), aktives Lernen (ALIAS [@SB:Interactive:02]), distanzebasierend
(siehe Attributsvergleich - Datensatz als konkatenierter String) oder
regelbasierend (AJAX [@GFS.EA:Declarative:01]). Damit verschieden
Vergleichmethoden miteinander kombiniert werden können, wird der
Ähnlichkeitswert für gewöhnlich auf einen Wert zwischen 0 und 1 normiert. Wobei
1 totale Übereinstimmung und 0 keine Übereinstimmung bedeutet.

Da es keine Methode zur Entity Resolution gibt, welche allen anderen überlegen
ist, wurde Ende der 90er Jahre begonnen Frameworks zu entwickeln, welche
verschiedene Methoden miteinander kombinieren. Einen Vergleich dieser Frameworks
wurde durch Köpcke & Rahm [@KR:Frameworks:10] durchgeführt. Ein Framework
besteht aus verschiedenen Matchern. Ein Matcher ist dabei ein Algorithmus,
welcher die Ähnlichkeit zweier Datensätze ermittelt. Ähnlich wie Elmagarmid et
al. unterscheiden Köpcke & Rahm zwischen attributs- und kontextbasierenden
Matchern. Als Kontext bezeichnen Sie die semantische Beziehung bzw. Hierarchie
zwischen den Attributen. Um die Matcher miteinander zu kombinieren nutzen die
Frameworks min. eine Matching Strategie. Durch die Match-Strategie sollen
verglichene Datensätze in Paare von Matches und Non-Matches klassifiziert
werden. Ein Match entspricht dabei einer Übereinstimmung in der Entity zweier
Datensätze, wohingegen ein Non-Match das Gegenteil ist. Eine Strategie ist, die
Ähnlichkeitswerte verschiedener Matcher nummerisch zu kombinieren,
beispielsweise durch eine gewichtete Summe oder einen gewichteten Durchschnitt.
Ein anderer Ansatz ist regelbasierend. Eine einfache Regel besteht aus einer
logischen Verbindung und einer Match-Kondition, beispielsweise einem
Schwellenwert. Die dritte und komplexeste Strategie ist workflow-basierend.
Hierbei kann beispielsweise eine Sequenz von Matchern die Ergebnisse iterativ
einschränken. Grundsätzlich können Workflows beliebig komplex werden. Einen
passenden Workflow zu finden kann selbst Domainexperten vor eine große
Herausforderung stellen. Daher gibt es trainingbasierende Ansätze passende
Parameter für Matcher oder Kombinationsfunktionen (z.B. Gewicht für Matcher) zu
bestimmen. Solche Ansätze sind etwa, Naive Bayes, Logistic Regression, Support
Vector Maschine oder Decision Trees.

In der klassischen Variante arbeitet Entity Resolution auf statischen Daten.
D.h. das während des ER-Prozesses keine neuen Daten hinzukommen. Hierbei werden
die zwei Disziplinen Deduplizierung und Entity-Linking unterschieden. Die
Deduplizierung wird auf einer Datenquelle durchgeführt und hat den Zweck alle
Duplikate in dieser Datenquelle zu finden. Anschließend werden die gefundenen
Duplikate automatisch oder manuell zusammengeführt. Entity Linking hingegen wird
auf mindestens zwei verschiedenen Datenquellen durchgeführt. Das Ziel ist es,
nicht Duplikate zusammenzuführen, sondern Entitäten unter den Datenquellen zu
verlinken. Damit die Links eindeutig sind, wird vorausgesetzt, dass die
einzelnen Datenquellen dedupliziert sind.

Die Ausführung der Vergleichsmethoden ist enorm teuer, da diese das Kreuzprodukt
zweier Mengen bilden müssen. Dies führt zu einer quadratischen Komplexität,
welche dafür sorgt, dass bei großen Datenmengen die Ausführungszeit im
statischen Fall bzw. die Latenzen im dynamischen Fall unakzeptabel groß werden.
Um die Ausführungszeit zu reduzieren wird versucht den Suchraum auf die
wahrscheinlichsten Duplikatsvorkommen zu begrenzen. Diese Vorgehen werden als
Blocking oder Indexing bezeichnet.

![Vereinfachter Entity Resolution Workflow. Nach [@Kol:Effiziente:14].](pictures/entity_resolution_workflow.png){#fig:er_workflow}

@fig:er_workflow zeigt einen vereinfachten typischen Entity Resolution Workflow
für statisches ER. Zunächst werden die Datensätze vorverarbeitet, um typische
Fehler zu entfernen. Dazu gehört das Korrigieren von Rechtschreibfehler,
ignorieren von Groß- bzw. Kleinschreibung und ersetzten von bekannten
Abkürzungen. Durch die Vorverarbeitung kann die Qualität des Matchings
verbessert, indem verhindert wird, dass offensichtliche Abweichungen den
Ähnlichkeitswert beeinflussen. Der nächste Schritt, das Blocking, teilt die
Gesamtmenge in Submengen zur Reduzierung der Komplexität. Auf das Blocking folgt
das Matching hierbei werden Ähnlichkeitswerte bestimmt und Matches sowie
Non-Matches klassifiziert. Abschließend findet noch die Berechnung der
transitiven Hülle statt, um beispielsweise aus Paaren von Matches Gruppen von zu
bilden, welche derselben Entität entsprechen.

Ein Großteil der Forschung in Entity Resolution konzentriert sich auf die
Qualität der Vergleichsergebnisse. Die von Köpcke & Rahm verglichenen Frameworks
konzentrieren sich alle Samt darauf zwei statische Mengen zu miteinander
vergleichen. Bei großen Datenmengen kann dies durchaus mehrere Stunden dauern.
Daher gibt es in den letzten Jahre einige Ansätze und Frameworks, welche
MapReduce Algorithmen zum Skalieren nutzen [@KR:Parallel:13, @MAS:Graph:14].
Einen weiteren Ansatz die Laufzeit für nahe Echtzeit Anwendungen zu optimieren
präsentieren Whang et al. [@WMG:PayAsYouGo:13]. Anstatt eine
Übereinstimmungsmenge nach Abschluss eines Algorithmus zu liefern, zeigen Sie
Möglichkeiten partielle Ergebnisse während der Laufzeit des Algorithmus zu
erhalten. Dabei werden die Suchalgorithmen so modifiziert, dass zunächst die
Wahrscheinlichsten Kandidaten miteinander verglichen werden. Dabei wird in
relativ kurzer Zeit ein Großteil der Entitäten gefunden.

Neben den statischen Verfahren gibt es zunehmend Bedarf an dynamischen
Verfahren. Das Finden gleicher Enitäten erfolgt hier auf Anfrage, weshalb die
gesamte Datenmenge vorab nicht bekannt ist. Ramadan et al. [@RCLG:Dynamic:15]
formulieren die Problemstellung für dynamische ER-Verfahren folgendermaßen:

$$M_{q_j} = \{r_i|r_i.id = q_j.id, r_i \in R\}, M_{q_j} \subseteq R, q_j \in Q$$ {#eq:dyer}

@eq:dyer beschreibt, dass für jeden Anfragedatensatz $q_j$ eines Anfragestroms
$Q$ alle Datensätze $M_{q_j}$ in $R$ gefunden werden sollen, welche die selbe
Enität wie $q_j$ beschreiben. Beispielsweise mussen Kreditauskunfteien auf
Anfrage prüfen, ob ein Kunde kreditwürdig ist. Dazu müssen die passendend
Entitäten möglichst schnell gefunden werden, um eine Entscheidung treffen zu
können. Zudem ist es notwending eine Historie der unveränderten Anfragen aller
Entität vorzuhalten, da die Beweise über frühere Anfragen liefern. Die
Herausforderung für dynamische ER-Verfahren ist weiter nach Ramadan et al.
Indexing-Verfahren zu entwicklen, welche es erlauben den Index dynamische zu
erweitern und eine kleine Zahl qualitativer Ergebnisse in nahe Echtzeit
(Subsekundenbereich) zu liefern. Ein dynamisches ER System ist ähnlich einer
Suchmaschine, doch anstatt einer gewerteten Liste möglicher Treffer, soll es
alle gleichen Entitäten finden, welche zur Anfrage passen. Das bedeutet
insbesondere, dass die Anfrage die gleiche Datenstruktur haben muss, wie die zu
durchsuchende Datenquelle. Erste Ergebnisse Entity Resolution in nahe Echtzeit
zu erreichen, präsentieren Christen & Gayler in [@CG:Scalable:08], unter
Verwendung von Inverted Indexing Techniken, welche normalerweise bei der
Websuche Anwendung finden.

## Statisches Blocking

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|*|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}*|A|*(|A|-1)$ Vergleiche durchgeführt werden. In beiden Fällen ist
die Anzahl der Vergleiche quadratisch zur Eingabemenge [@Kol:Effiziente:14]. In
der Studie [@KTR:Evaluation:10] zeigen Köpcke et al., dass das kartesische
Produkt für größe Datenmengen nicht skaliert. Aus diesem Grund reduzieren
moderne Entity Resolution Frameworks den Suchraum auf die wahrscheinlichsten
Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden zur Reduzierung des
quadratischen Suchraum werden übergreifend als Blockingmethoden bezeichnet.
Neben Blocking werden auch Windowing- und Indexing Verfahren eingesetzt. Während
Blockingverfahren die Anzahl der notwendigen Vergleiche drastisch reduzieren,
indem Non-Matches ausgeschlossen werden, besteht dennoch die Gefahr, dass
fälschlicherweise tatsächliche Matches ausgefiltert werden. Daher ist es
notwendig die Güte des Blockingverfahrens zu bestimmen. Dazu werden zwei
Kennziffern erhoben. Zum einen die *Reduction Ratio*, welche die Reduzierung der
Vergleiche im Gegensatz zum Kartesischen Produkt ausdrückt, sowie die *Pairs
Completeness*, welche den Anteil der tatsächlich ausgewählten Duplikate, die
sich nach dem Blocking in der Kandidatenmenge befinden, beschreibt.

Prinzipelle erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" voneinander einfinden. Zur
Durchführung der Gruppierung oder Sortierung müssen sog. Block- bzw.
Sortierschlüssel für jeden Datensatz erzeugt werden. Diese Schlüssel werden von
den Attributwerten oder einem Teil der Attributwerte abgeleitet und stellen eine
Signatur des Datensatzes dar. Eine beliebte Variante für Schlüssel sind etwa
phoenitische Enkodierung.

### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee ist eine Menge von Datensätzen in disjunkte
Partitionen (genannt Blöcke) zu teilen. Anschließend werden nur die Datensätze
des jeweiligen Blocks miteinander verglichen. Dazu wird jedem Datensatz ein
Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens hängt daher
maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und Größe der
Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
@fig:sb_blocking zeigt die Ausführung des Blockingverfahrens beispielhaft an
einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein Blockschlüssel
(hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses Schlüssels gruppiert.
Die Größe der einzelnen Blöcke bestimmt die Reduktion Ratio. Diese hängt
allerdings immer von der Datenquelle ab und kann daher nicht pauschalisiert
werden. Bei der Generierung der Blockschlüssel können fehlerhafte Werte
einzelner Attribute dazu führen, dass Duplikate in unterschiedlichen Blöcken
landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden Datensatz
mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert werden.
Dieser Ansatz nennt sich Multi-pass Blocking.

![Beispielhafte Standard Blocking Ausführung. Nach [@Kol:Effiziente:14].](pictures/standard_blocking.pdf){#fig:sb_blocking}

### Q-gram Indexing

Das Q-gram Indexing basiert auf der Idee Datensätze unterschiedlicher aber
ähnlicher Blockschlüssel miteinander zu vergleichen. Ein Blockschlüssel wird
dazu in eine Liste $G$ von q-Grammen überführt. Ein q-Gram ist ein Substring der
Länge q des ursprünglichen Blockschlüssels. Alle Permutationen der q-Gram Liste
mit einer Mindestlänge $l=max(1,\floor{\#G*t})$ werden konkateniert und dienen
als Schlüssel der Blöcke, wobei $t$ ein Schwellwert zwischen 0 und 1 ist. Dabei
werden Datensätze mehreren Blöcken zugewiesen. Ist $t=1$ entspricht Q-gram
Indexing dem Standard Blocking. Dieses Verfahren kann als Alternative zum
Multi-pass Verfahren beim Standard Blocking genutzt werden. Der große Nachteil
ist der hohe Aufwand bei der Berechnung aller möglichen Sublisten. Ein
Blockschlüssel mit $n$ Zeichen muss in $k=n-q+1$ q-Gramme zerlegt werden.
Insgesamt müssen dadurch $\sum_{i=max\{1,[k*t]\}}^{k} {k \choose i}$ Sublisten
berechnet werden [@Chr:Survey:12].

### Suffix Array Indexing

Das Suffix Array Indexing [@AO:Fast:05a] leitet, ähnlich wie Q-gram Indexing,
mehrere Schlüssel aus einem Blockschlüssel ab. Grundidee ist es alle Suffixe mit
einer Mindestlänge von $l$ zu bestimmen. Ein Datensatz mit Blockschlüssellänge
$n$ wird in $n-l+1$ Blöcke eingeordnet. Ist $n<l$ wird der Ausgangsschlüssel als
einziger Schlüssel verwendet. Durch die größere Menge an Kandidatenpaaren ist
i.Allg. die *Pair Completeness* höher (vgl. Multi-pass). Zudem ist der Aufwand
der Berechnung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer. Im
Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren jedoch
deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze
unnötigerweise mehrfach miteinander verglichen werden hoch. Deshalb werden aus
Blöcken, welche einen bestimmten Schwellwert überschreiten alle Datensätze
entfernt, die min. einen weiteren längeren Blockschüssel haben.

### Sorted Neighborhood

Das Sorted Neighborhood Verfahren, ist ein Sortierverfahren, welches 1995 von
Hernández & Stolfo [@HS:Merge:95] zur Erkennung von Dupliakten in
Datenbanktabellen vorgestellt wurde. Es besteht aus drei Phasen. Zunächst
bekommt jeder Datensatz einen Sortierschlüssel zugewiesen. Dabei muss der
Sortierschlüssel nicht einzigartig sein. Um die Berechnung des Schlüssels gering
zu halten, soll dieser durch Verkettung von Attributen bzw. Teilen der Attribute
bestimmt werden. Attribute die vorne im Schlüssel stehen haben dadurch eine
höhere Priorität. In der zweiten Phase werden die Datensätze anhand des
Schlüssels sortiert. In der dritten Phase wird ein Fenster (engl. Window) über
die sortierten Datensätze geschoben und alle Datensätze innerhalb des Windows
werden miteinander verglichen. Dieses Verfahren eignet sich besonders gut zur
Erkennung von Duplikaten innerhalb einer Datenquelle. Sollen Duplikate in
mehreren Datenquellen gefunden werden, müssen die Einträge beim Sortieren
gemischt werden. Dadurch besteht allerdings die Gefahr, das vorrangig Datensätze
einer Datenquelle miteinander verglichen werden. Vorteil zum Standard Blocking
ist, dass die Anzahl der Vergleiche lediglich von der Größe der Datenquelle und
der gewählten Fenstergröße abhängen. Ein großer Nachteil ist, dass Datensätze
die sich in der ersten Stelle des Schlüssels unterscheiden, weit voneinander
entfernt sind und dadurch nicht als Matches identifiziert werden. Um dennoch
eine hohe Pair Completeness zu erreichen, werden mehrere Schlüssel pro Datensatz
generiert und ein Fenster mit kleiner Größe über die verschieden sortierten
Listen geschoben. Dieses Verfahren entspricht im Grunde dem Multi-pass Verfahren
beim Standard Blocking.

Ein großes Problem bei der klassischen und der Multi-pass Variante des Sorted
Neighborhood Verfahrens ist, dass die zu wählende Fenstergröße $w$ größer als
die Anzahl der Datensätze mit dem am häufigsten vorkommenden Sortierschlüssel
sein muss, um eine gute Pair Completeness zu erreichen. Sei $n$ die Menge an
Datensätzen mit dem am häufigsten vorkommenden Schlüssel $k$ und $m$ die Menge
der Datensätze des darauffolgenden Schlüssels $k+1$, dann ist $w=n+m$. Nur
dadurch kann sichergestellt werden, dass alle Datensätze aus $n$ mit den "nahen"
Datensätzen aus $m$ verglichen werden. Da Sortierschlüssel für gewöhnlich nicht
gleich verteilt sind, gibt es meist wenige Größe und viele kleine Mengen an
Datensätzen mit dem gleichen Sortierschlüssel. Dadurch werden Datensätze mit
seltenen Sortierschlüsseln unnötig oft mit "weit" entfernten Datensätzen
verglichen. Zudem dominiert der am häufigsten vorkommenden Schlüssel, genauso
wie beim Standard Blocking, die Ausführungszeit des Algorithmus.

In [@DN:comparison:09] schlagen Draisbach & Naumann eine optimierte Variante des
Sorted Neighborhood Verfahrens vor. Dabei zeigen Sie, dass Standard Blockung und
Sorted Neighborhood extreme von Overlapping bei Partitionen sind. Ihre Idee ist
es diese Überlappung zu optimieren. Dabei soll die Überlappung groß genug sein,
um tatsächliche Matches zu finden, aber gering genug, um die Menge der
Vergleiche zu reduzieren. Zunächst wird wie beim klassischen Verfahren zu
sortiert. Danach werden angrenzende Datensätze in disjunkte Partitionen zerlegt
und schließlich wird ein Überlappungsfaktor (genannt Overlap) $u$ gewählt.
Innerhalb jedes Blockes wird analog zum Standard Blocking jeder Datensatz mit
jedem anderen verglichen. Innerhalb des Overlap-Window $w=u+1$, wird jeweils das
erste Element mit allen anderen Verglichen. Ist $w=0$ entspricht das Verfahren
Standard Blocking und hat jede Partition nur ein Element entspricht es der
Sorted Neighborhood Methode. Um zu vermeiden, dass eine Partition dominiert,
können größere Partitionen in Subpartitionen geteilt werden.

Weitere Varianten verändern die Fenstergröße anhand identifizierte Duplikate
[@DNSW:Adaptive:12].

### Canopy Clustering

Die Idee von Canopy Clustering ist es, Datensätze anhand einer einfachen
Vergleichsmetrik in überlappende Cluster (=Canopies) zu partionieren. Zur
Generierung wird eine Kandidatenliste gebildet, welche initial als allen
Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen Clusters
gewählte und alle Datensätze innerhalb des Mindestabstandes $d_1$ zugewiesen.
Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren
Mindestabstandes $d_2 < d_1$ aus der Kandidatenliste entfernt. Dieser
Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die *Pair
Completeness* hängt hierbei stark der gewählten Abstandsfunktion ab.
Anschließend wird werden alle Datensätze eines Cluster miteinander verglichen.

## Dynamisches Blocking

Für die Dupliaktserkennung in einer Datenquelle $A$, sind bei einer Anfrage $|A|
- 1$ Vergleiche notwendig. Da dies zu ungewollt hohen Latenzen führen würde,
  werden auch im dynamischen Fall Blocking Verfahren eingesetzt.

### DySimII

DySimII [@RCL.EA:Dynamic:13] ist die dynamische Variante des Similarity Aware
Index von Christen & Gayler [@CG:Scalable:08], welcher es zusätzlich erlaubt den
Index während der Laufzeit zu erweitern. Dabei ist die Grundidee die benötigten
Ähnlichkeiten vorauszuberechnen, um während der Laufzeit diese nur nachschlagen
zu müssen.

Der Index besteht aus drei Teilen. Dem **Record Index (RI)**, welcher alle
Attribute speichert und diese ihren Datensätzen zuordnet, dem **Block Index
(BI)**, welcher Attribute anhand einer Enkodierungsfunktion gruppiert und
zuletzt dem **Similarity Index (SI)**, welcher dieselben Schlüssel wie der
Record Index verwendet und die Ähnlichkeiten der Attribute im gleichen Block
hält. @fig:dysim_example zeigt ein Beispiel eines DySimII Index. Im Record Index
wurden die Datensatzidentifier von Tony (r1, r3) und Cathrine (r2, r6) als
Attributsübereinstimmung gruppiert. Anschließend wurden im Block Index über die
Double-Metaphone Enkodierung ähnliche Schreibweisen von Tony und Cathrine
zusammengeführt, was dem Standard Blocking entspricht. Im Similarity Index
wurden die Ähnlichkeiten von (Tony, Tonia und Tonya) bzw. (Cathrine, Kathryn),
welche sich in einem gemeinsamem Block befinden, untereinander mit ihren
berechneten Ähnlichkeiten verknüpft.

![DySimII Beispiel. Aus [@RCL.EA:Dynamic:13].](pictures/dysim_example.png){#fig:dysim_example}

Das Verfahren unterscheitet zwei Phasen. die Bauphase (Build-Phase), in welcher
der Index aus einem initiallen Datenbestand erzeugt wird und die Anfragephase
(Query-Phase), welche Anfragen aus einem Datenstrom beantwortet.

**Build Phase**. Das Einfügen von Datensätzen läuft nach folgendem Schema ab.
Zunächst werden alle Attribute mit Verweis auf den Datensatzidentifier im Record
Index gespeichert. Falls ein Attribut dort schon existiert wird lediglich der
Identifier angefügt. Anschließend wird für jedes Attribut eine Enkodierung
bestimmt. Anhand dieser Enkodierung werden die Attribute in jeweils einen Block
im Block Index eingefügt. Beinhaltet der Block mehr als ein Attribut wird zu
allen Attributen die Ähnlichkeit bestimmt und die Ähnlichkeit des eingefügten
Attributes mit der verglichenen Attribut in den Similarity Index eingefügt.
Gleichzeitig wird die bestimmte Ähnlichkeit des eingefügten Attributes auch zu
dem verglichenen Attribute im Similarity Index ergänzt.

**Query Phase**. Bei einer Anfrage wird zunächst der neue Datensatz dem Index
hinzugefügt. Anschließend werden aus dem Record Index alle Identifier
ausgelesen, welche ein gleiches Attribute besitzen und werden in einen
Akkumulator mit dem Ähnlichkeitswert 1 aufgenommen. Bei mehreren gleichen
Identifiern werden die Ähnlichkeitswerte addiert. Anschließend werden die
Attribute des Anfragedatensatzes im Similarity Index nachgeschlagen und alle
Attribute des gleichen Blockes mit ihrer Ähnlichkeit ausgelesen. Zu diesen
Attributen werden aus dem Record Index die Identifier abgefragt und mit ihrer
Ähnlichkeit aus dem Similarity Index im Akkumulator aufsummiert.

Im Gegensatz zum Standard Blocking können Anfragen deutlich schneller
beantwortet werden, da im Optimalfall keine Ähnlichkeitsberechnung stattfinden
muss und lediglich Werte nachgeschlagen werden. Auf der negativen Seite steht
hingegen der deutlich erhöhte Speicherbedarf, welcher durch das Halten der
Ähnlichkeitswerte zurückzuführen ist.

### Similarity-Aware Index with Local Sensitive Hashing (LSH)

Dieses Verfahren ist eine Erweiterung des DySimII durch LSH, welches von Li et
al. [@LLRo:Two:13] vorgestellt wurde. Die hier genutzte Variante des Local
Sensitive Hashing nutzt das Minhash Verfahren. Minhashing ist eine effiziente
Abschätzung der Überlappung zweier Mengen bekannt als Jaccard-Ähnlichkeit.
Mittels des Minhash-Algorithmus ist es möglich für jeden Datensatz $n$
Signaturen der Länge $k$ zu generieren. Dazu werden $n$ verschieden zufällig
gewählte Hashfunktionen genutzt. Um die Wahrscheinlichkeit zu erhöhen, dass nur
gleiche Paar die selbe Signatur haben wird eine Technik namens Banding genutzt.
Dazu werden $l$ Signaturen zu einem Band zusammengefügt und damit verundet.
Mehrere Bänder sind logisch gesehen eine Veroderung. Auch diese Verfahren teilt
sich in Bau- und Anfragephase.

**Build Phase**. Beim Erzeugen des Index werden zunächst die Minhash Signaturen
erzeugt und zu Bändern verundet. Anschließend werden die Datensatzidenfier wird
mit den erzeugten Bändern verknüpft. Dazu wird ein Index erstellt, welcher als
Schlüssel zunächst die verschieden Bänder hat. Innerhalb der Bänder gibt es
weitere Subindicies, welche als Schlüssel die Minhash Signaturen haben. Den
jeweiligen Signaturen innerhalb der Bänder wird der Datensatzidentifier
zugewiesen. Dadurch sind gleiche Signaturen durch die Bänder getrennt, was die
Wahrscheinlichkeit erhöht, dass unähnliche Datensatze eine gemeinsame Signatur
im Index haben. Der LSH Index ersetzt dadurch den Record Index. Die Schritte zum
Einfügen in den Block Index bzw. den Similarity Index sind analog zum DySimII.

**Query Phase**. Für die Beantwortung einer Anfrage werden zunächst für den
neuen Datensatz die Minhash Signaturen und Bänder erzeugt und mit
Datensatzidentifier in den LSH Index eingefügt. Dannach werden die
Datensatzidenfier mit gleichen Signaturen in den gleichen Bändern als
Kandidatenmenge ausgelesen. Nun müssen die Attribute der Kandidaten aus einer
Datenquelle geladen werden. Mit diesen Attributen können aus dem Similarity
Index die Ähnlichkeitswerte jedes Kandidaten bestimmt werden. Die Kandidaten,
welche aus dem LSH Index erhalten wurden haben allerdings nicht zwingend
Attribute in denselben Blöcken im Block Index wie der Anfragedatensatzes.
Deshalb können zu einigen Attributen keine vorberechneten Ähnlichkeiten aus dem
Similarity Index bezogen werden. Da das berechnen zur Laufzeit zu lange dauert,
werden diese mit dem Ähnlichkeitswert 0 miteinberechnet. Dies mindert zwar die
Genauigkeit etwas sorgt dennoch für gute Latenzen.

Im Gegensatz zum DySimII ist die Berechnung des Index aufwendiger, da für jeden
Datensatz die Minhash Signaturen und Bänder berechnet werden müssen. Allerdings
ist die Kandidatenmenge potentielle deutlich geringer als beim DySimII, wodurch
die Anfragen schneller beantwortet werden.

### DySNI

Das DySNI Verfahren von Ramadan et al. [@RCLG:Dynamic:15] ist ein dynamische
Umsetzung des Sorted Neighborhood Verfahren aus dem statischen Blocking. Anstatt
eines Arrays verwenden Sie eine Baumstruktur, um Datasätze möglichst schnell zu
selektieren. Der gewählte beim ist ein BraidedTree (BRT), welcher ein
balanzierter binärer AVL-Baum ist. Innerhalb des Baumes hat jeder Knoten jeweils
einen Verweis auf seinen Vorgänger und seinen Nachfolger. Die Sortierung erfolgt
alphabetisch nach einem gewählten Sortierschlüssel. Ein Knoten besteht dabei aus
einem Sortierschlüsselwert (Sorting Key Value. kurz: SKV) und einer Liste an von
Datensätzes mit diesem SKV. Ein Anfrageknoten, ist derjenige in welchen ein
Anfragedatensatz eingefügt wurde, und wird mit $N_q$ bezeichnet.

**Build Phase**. Beim Einfügen eines neuen Datensatzes wird zunächst dessen SKV
erzeugt. Wenn der SKV noch nicht im BRT-Baum vorhanden ist, wird ein neuer
Knoten erzeugt und der Datensatzidentifier angehängt. Ist der Knoten bereits
vorhanden wird lediglich der Datensatzidentifier zum existierenden Knoten
hinzugefügt. Zusätzlich wird der Datensatz in einen Inverted Index $D$
eingefügt, um ihn zum Attributsvergleich mit anderen Datensätzen schnell
selektieren zu können.

**Query Phase**. Zunächst wird der Anfragedatensatz, nach dem Vorgehen aus der
Build Phase, eingefügt. Dannach ist bekannt welchem Knoten der Anfragedatensatz
zugeordnet wurde und das Fenster mit den benachbarten Knoten kann erzeugt
werden. Alle Datensätze, welche in Knoten innerhalb des Fenster gespeichert
sind, werden als Kandidatenmenge $C$ selektiert. Aus $D$ werden dann für jeden
Kandidaten seine Attribute geholt und anschließend in einem Paarvergleiche mit
dem Anfragedatensatz die Ähnlichkeit ermittelt. Für Erzeugung des Fenster werden
vier Methoden vorgestellt, welche sich an Varianten des statischen Sorted
Neighborhood Verfahrens orientieren.

* **Fixed Window Size** ist das einfachste Verfahren, bei welchem das Fenster um
  einen festen Wert $w$ in Vorgänger- und Nachfolgerrichtung aufgespannt wird.
* **Candidates-Based Adaptive Window** erweitert das Fenster abwechselnd in
  Vorgänger- und Nachfolgerrichtung, solange bis eine Mindestanzahl an
  Kandidaten gefunden wurde.
* **Similarity-Based Adaptive Window** nutzt die Ähnlichkeit zwischen SKVs.
  Dabei wird ein Fenster in eine Richtung solange erweitert bis die Ähnlichkeit
  zwischen dem SKV von $N_q$ und dem nächsten Vorgänger bzw. Nachfolger eine
  mindestähnlichkeit $\Delta$ unterschreitet.
* **Duplicate-Based Adaptive Window** erweitert das Fenster aus Basis gefundener
  Matches in beide Richtungen unabhängig. Dabei wird das Fenster um jeweils
  einen Knoten erweitert und zwischen dem Anfragedatensatz und den Datensätzen
  des neuen Knoten der Ähnlichkeitswert ermittelt, sowie klassifiziert, ob es
  sich um ein Match oder Non-Match handelt. Sinkt die Anzahl an gefunden
  Matches unter eine Schranke $\delta$ wird das Fenster in diese Richtung nicht
  weiter vergrößert.

Damit Ähnlichkeiten zwischen den Datensätzen nicht jedes Mal neuberechnet werden
müssen. Schlagen die Authoren vor, je nach gewählter Fensterberechunung, die
Ähnlichkeit der SKVs zu berechnen und in den beteiligten Knoten abzuspeichern.
Dadurch wird allerdings die Auswahl an SKVs auf Konkatenationen von Attributen
beschränkt. Attribute die nicht im SKV genutzt wurden müssen bei diesem
Verfahren trotzdem jedes Mal neuberechnet werden.

Des Weiteren ist auch dieses Verfahren sensitiv auf Fehler am Anfang des SKV. Um
dies zu korrigieren wird, ähnlich zum Multi-pass Verfahren des Sorted
Neighborhood Verfahren, vorgeschlagen mehrere BRT-Bäume mit unterschiedlichen
SKVs zu erstellen.

In ihrer Auswertung zeigen die Authoren, dass das Duplikatsbasierende Verfahren
im BRT-Baum nicht funktioniert, weil ein Großteil der Duplikate im
Ausgangsknoten landet und damit eine Erweiterung des Fenster nicht zustande
kommt. Die besten Recall Ergenisse wurden mit Ähnlichkeitsbasierenden Verfahren
erreicht, da der Baum nach den SKVs sortiert wurde und dieses Fenster damit am
Besten passt. Das beiden anderen Verfahren ebenfalls gute Ergenisse und haben
den Vorteil, dass sich das Fenster und damit die Anzahl der Kandidaten gut
kontrollieren lässt und damit auch die Latenzen.

## Blocking Schema

Die Qualität aller Verfahren, ob statisch oder dynamische, hängt maßgeblich von
der Auswahl des richtigen Blocking Schema ab. Wie genau diese Schema auszuwählen
ist, wird von vielen Verfahren offen gelassen. Die meisten Verfahren schränken
jedoch ein, dass zu einem Datensatz nur ein Blockschlüssel oder Sortierschlüssel
erzeugt werden darf. Bei der Verwendung von Multi-pass Ansätzen werden
demensprechend verschiedene Schema gefordert. Ein funktionierendes Schema zu
finden ist oft, auch von Domainexperten, nur durch auspropieren herauszufinden.
Oft genutzt werden phonetische Enkodierung und Konkatenationen von Attributen.
Weitere Beispiele sind Q-Grame oder Suffixe aus den vorgestellten statischen
Verfahren.

### DNF-Blocking Schema {#sec:blk_scheme}

Um ein gutes Blocking Schema automatisch zu lernen schlagen Kejriwal & Miranker
[@KM:Unsupervised:13] ein Verfahren vor, welches ein Blocking Schema in
Disjunktiver Normalform erzeugt. Dieses Schema wird folgendermaßen definiert.

Das kleinste Element eines Blocking Schema ist eine *Indizierungsfunktion*
$h_i(x_t)$. Diese akzeptiert einen Attributwert eines Datensatzes und erzeugt
eine Menge $Y$, welche 0 oder mehr Blockschlüssel (engl. *blocking key value*,
kurz BKV) beinhaltet. Ein BKV identifiziert einen Block, welchem ein Datensatz
zugeordnet wird. Ein Beispiel einer Indizierungsfunktion ist *Tokens*. Tokens
zerlegt einen Eingabestring in eine Menge von Token mittels eines Trennzeichens
(z.B. Komma oder Leerzeichen).

Daraus folgt das allgemeine Blockingpredikat (engl. *general blocking
predicate*). Das allgemeine Blockingpredikat $p_i(x_{t_1}, x_{t_2})$ nimmt zwei
Attribute unterschiedlicher Datensatze $t_1$ und $t_2$ und nutzt die i^te^
Indizierungsfunktion, um zwei Mengen $Y_1$ und $Y_2$ zu erzeugen. Das Predikat
ist wahr, wenn die beiden Mengen eine gemeinsame Schnittmenge haben. Angenommen
die i^te^ Indizierungsfunktion ist *Tokens*, dann haben wir das Predikat
*EnthältGemeinsamenToken*, welche Wahr ist wenn zwei Attribute mindestens einen
gemeinsamem Token haben.

Das spezifische Blockingpredikat (engl. *specific blocking predicate*) ist ein
Paar $(p_i, f)$, dass ein allgemeines Blockingpredikat $p_i$ mit einem Attribute
$f$ verbindet. Dazu nimmt das spezifische Blockingpredikat zwei Datensätze $t_1$
und $t_2$ und wendet $p_i$ auf die entsprechenden Attribute $f_1$ und $f_2$ der
Datensätze an. Ein solche Paar ist beispielsweise (*EnthältGemeinsamenToken*,
*name*). Dieses spezifische Predikat ist wahr, wenn der Name zweier Datensätze
einen gemeinsamem Token enthält.

Das DNF Blocking Schema $f_P$ ist eine Funktion, welche in der Disjunktiven
Normalform ohne Negation durch eine Menge von $P$ Termen erzeugt wird. Jeder
Term in $f_P$ besteht aus mindestens einem spezifischen Blockingpredikat.
Mehrere spezifische Blockingpredikat in einem Term werden durch eine Konjunktion
verbunden. Das DNF Blocking Schema ist demensprechend Wahr, wenn einer seiner
Terme Wahr ist. Wahr für zwei Datensätze bedeutet, dass diese von einem
Blockingverfahren selektiert und verglichen werden würden. Dabei entspricht die
Konjunktion eines Terms dem Konkatenieren von Strings. Die Disjunktion der Terme
kann bei vielen Verfahren als Multi-pass Ansatz implementiert werden. Des
Weiteren ist zu beachten, dass das Blocking Schema potentiell mehrere Schlüssel
pro Datensatz erzeugt. Ein Beispiel eines solchen Schema ist
(*EnthältGemeinsamenToken*, Name) $\wedge$ (*ExakteÜbereinstimmung*, Stadt).

### Weak Labels

Das Lernen eines Blocking Schema erfordert eine Menge von klassifizierten Daten.
Diese sind allerdings oft nicht vorhanden und das Erstellen ist ein sehr
aufwendiger, langwieriger und teurer Prozess, welcher von Domainexperten
durchgeführt werden muss.

Deshalb haben Kejriwal & Mirankern [@KM:Unsupervised:13] ein Verfahren
entwickelt, um eine Menge schwach klassifizierter Daten zu generieren. Dabei
werden sowohl positive, als auch negative Datensatzpaare klassifiziert. Über
zwei Schranken kann der Benutzer festlegen wie ähnlich (obere Schranke $ut$)
bzw. wie verschieden (untere Schranke $lt$) die Paare sein sollen. Anschließend
wird ein Blocking der Daten per Standard Blocking und Sorted Neighborhood
durchgeführt. Zunächst werden dazu alle Attribute jedes Datensätzen in Token
zerlegt. Anhand der Token wird das Standard Blocking durchgeführt, wobei jeder
Datensatz in mehreren Blöcken vertreten sein kann. Um innerhalb der Blöcke den
Paarvergleichsaufwand zu reduzieren, wird zusätzlich ein Fenster der Größe $c$
über den Block geschoben, was der Sorted Neighborhood entspricht. Nachdem das
Fenster über jeden Block geschoben wurde steht die Menge möglicher
Kandidatenpaare fest. Diese Paare werden nun mit der TF/IDF-Ähnlichkeit ($sim$)
[@AO:Fast:05a] verglichen. Diese ermöglicht, nachdem die TF/IDF Statistik über
die kompletten Daten erfasst wurde, eine Komplexität von $O(1)$, da lediglich
die Werte des Paares nachgeschlagen werden müssen. Ist $sim \geq ut$ wird das
Paar positiv klassifiziert. Ebenso, ist $sim < lt$ wird ein Paar negativ
klassifiziert. Damit die Menge der klassifizierten Daten nicht beliebig groß
wird, kann der Anwender festlegen, wie viele positive $max_p$ bzw. negative
Paare $max_n$ maximal erzeugt werden sollen. Von allen positiven Paaren werden
abschließend die besten $d$ ausgewählt, aber maximal $max_p$. Analog werden
ebenfalls die besten $nd$ negative Paare gewählt. Bei den negativen Paaren soll
dadurch verhindern, dass lediglich Paare mit $sim \approx 0.0$ ausgewählt
werden, da diese für gewöhnlich zu niedrigen Klassifikationsraten führen. Die
Gesamtkomplexität des Algorithmus ist $O(n + nm + nm)$, welcher sich in die
Erzeugung der TF/IDF Statistik ($O(n)$) , die Erzeugung der Blöcke über $m$
Attribute ($O(nm)$) und die Erzeugung der Kandidatenpaare $O(nm)$ gliedert.

### Lernen des DNF-Blocking Schema

Der erste Schritt zum Lernen eines DNF-Blocking Schema ist, eine Liste an
spezifischen Blockingpredikaten zu benennen. Beispielsweise
*EnthältGemeinsamenToken* und *ExakteÜbereinstimmung* für Strings und
*Erste3Ziffern* für nummerische Attribute. Anschließend wird für jedes
spezifische Blockingpredikat, auf Basis der schwach klassifizierten Daten, ein
positiver $P_f$ und ein negativer $N_f$ boolscher Featurevektor erzeugt. Dabei
ist ein Wert innerhalb des Vektors Wahr, wenn das entsprechende Predikat für das
gewählte Datensatzpaar Wahr ist.

Anschließend werden die Terme des Blocking Schema erzeugt. Da potentiell
beliebig viele Terme erzeugt werden können, muss der Anwender die Tiefe, d.h.
Anzahl der Predikate pro Term, festlegen. Die Featurevektoren der Terme können
durch Konjunktion der Featurevektoren der einzelnen Predikate ohne große
Aufwände berechnet werden. Danach wird die Qualität der einzelnen Terme
bewertet. Dazu nutzen Kejriwal & Miranker die Fisher-Score. Die Idee der
Fisher-Score nach [@GLH:Generalized:12] ist, ein Untermenge von Features (hier:
Terme) zu finden, sodass die Datenpunkte der Klassen (hier: positive und
negative Labels) möglichst weit voneinander entfernt und gleichzeitig die
Datenpunkte innerhalb der Klasse möglichst nahe zusammen sind. Die Formel zur
Berechnung der Fisher-Score des i^ten^ Terms sieht folgendermaßen aus:
$\rho_i = \frac{|P_{f,i}|(\mu_{p,i} - \mu_i)^2 + |N_{f,i}|(u_{\mu,i} -
\mu_i)^2}{|P_{f,i}|\sigma^2_{p,i} + |N_{f,i}|\sigma^2_{n,i}}$. Dabei ist
$\mu_{p,i}$ bzw. $\mu_{n,i}$ die Anzahl der wahrer Werte in $P_{f,i}$ und
$N_{f,i}$. Weiterhin ist $\mu_i$ die Anzahl wahrer Werte in $P_{f,i} \cup
N_{f,i}$ und $\sigma$ ist die positive bzw. negative Varianz.

Anhand der bewerteten Terme wird das DNF Blocking Schema gebildet. Dazu werden
die Terme nach ihrer Fisher-Score sortiert. Anschließend werden die Terme
absteigend zur DNF Blocking hinzugefügt, wenn ein Term mindestens ein weiteres
Paar im positiven Featurevektor erfasst. Dies geschieht solange bis ein Minimum
an positiven Paaren noch nicht erfasst worden sind oder keine Terme mehr
verfügbar sind.

## Ähnlichkeitsmaße

Über Ähnlichkeitsmaße (engl. similarity measures) wird die Ähnlichkeit zweier
Datensätze bestimmt. Genauer wird die Ähnlichkeit der einzelnen Attribute
bestimmt, aus welcher sich die Gesamtähnlichkeit der Datensätze bestimmen lässt.
Die meisten Fehler, welche zu unterschiedlichen Datensätzen führen sind
typographische Variationen von Strings. Weshalb sich entsprechend viele Ansätze
für den Vergleich von Stringattributen finden. Attributsähnlichkeiten kann man
grob in fünf Kategorien ordnen:

* zeichenbasierend
* tokenbasierend
* phonetisch
* nummerisch
* kernelbasierend

#### Zeichenbasierende Ähnlichkeit

Wie sich die Ähnlichkeit von Strings bestimmen lässt, wird seit den 60er Jahren
intensiv erforscht. Die Stärke von zeichenbasierten Ähnlichkeiten, ist das
Erkennen von typografischen Fehlern.

Der älteste und wohl auch bekannteste Algorithmus ist die Levenshtein Distanz
[@Lev:Binary:66]. Die Levenshtein Distanz ist Begründer der
**Editierdistanzen**, welche die minimalen Schritte berechnet, die benötigt
werden um einen String $\sigma_1$ in einen anderen $\sigma_2$ umzuwandeln. Diese
Schritte beinhalten das Einfügen, das Löschen, das Ersetzen und mit der
Modifiaktion von Damerau [@Dam:technique:64] auch das Vertauschen von Zeichen.
Da potenziell alle Zeichen beider Strings miteinander vergleichen werden ist die
Komplexität $O(|\sigma_1|*|\sigma_2|)$. Needleman und Wunsch [@NW:general:70]
erweitern die originale Editierdistanz dahingehen, dass bestimmte Operationen
anders gewichtet werden. Dazu können die Kosten für die einzelnen Schritte,
welche in der einfachsten Form 1 sind, auf einen beliebigen Gleitkommawert
angepasst werden. In dieser Variante enspricht das Lösen der Editierdistanz dem
Traveling Salesmen Problem und ist daher NP-schwer.

Eine weitere Modifikation der Editierdistanz ist es etwa Lücken zu erkennen
[@WSB:biological:76], beispielsweise wenn ein Wort abgekürzt wurde.
Dementsprechend können Kosten für das Öffnen und das Erweitern einer Lücke
festgelegt werden. Eine andere ist Fehler am Anfang oder am Ende des String mit
geringeren Kosten zu versehen [@SW:Identification:81].

Die Jaro-Distanz berechnet die Anzahl der gemeinsamen Zeichen $m$, wobei eine
Verschiebung von $\frac{1}{2}*min(|\sigma_1|, |\sigma_2|)$ zugelassen wird. Von
den gemeinsamen Zeichen werden anschließend die Transpositionen $t$ berechnet,
d.h. wie viele gemeinsame Zeichen nicht in der gleiche Reihenfolge sind. Daraus
berechnet sich die Jaro-Distanz $d_j$ folgendermaßen: $$d_j =
\left\{\begin{array}{l l} 0 & \text{wenn }m = 0\\
\frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} + \frac{m-t}{m}\right) &
\text{anderenfalls} \end{array} \right.$$

#### Tokenbasierende Ähnlichkeit

Die tokenbasierende Ähnlichkeit bietet, im Gegensatz zu den zeichenbasierenden,
den Vorteil, dass Vertauschungen von Wörtern erkannt werden. Beispielsweise bei
Vorname und Nachname.

Monge und Elkan [@MEo:Field:96] schlagen einen Algorithmus auf Basis atomarer
Strings vor. Für zwei Strings $\sigma_1, \sigma_2$ werden alle Token aus
$\sigma_1 = (\sigma_{1_{t_1}}, \dots, \sigma_{1_{t_i}})$ mit allen Token aus
$\sigma_2 = (\sigma_{2_{t_1}}, \dots, \sigma_{2_{t_j}})$ vergleichen. Zum
Vergleich muss eine beliebige zeichenbasierende Ähnlichkeit $sim$ gewählt
werden. Anschließend wird für jeden Token in $\sigma_1$ die Ähnlichkeit mit
$s_{t_k} = max(sim(\sigma_{1_{t_k}}, \sigma_{2_{t_1}}), \dots,
sim(\sigma_{1_{t_k}}, \sigma_{2_{t_j}}))$ berechnet. Die Gesamtähnlichkeit ist
der Durchschnitt der Tokenähnlichkeiten $m = avg(s_{t_1}, \dots, s_{t_i})$. Das
Problem mit diesem Algorithmus ist seine Komplexität, welche quadratisch zur
Tokenmenge und damit zu $sim$.

Eine weitere Möglichkeit Token zu bilden sind Q-Gramme. Diese haben den Vorteil,
das neben Vertauschungen von Wörtern auch typographische Fehler erkannt werden
können. Über Q-Gramme wird ein String $\sigma$ in $k$ überlappende Token der
Länge $n$ zerlegt. Dazu wird ein Fenster der Länge $n$ von Position 1 bis
$|\sigma|-(n-1)$ geschoben. Um aus Q-Grammen eine Ähnlichkeit zu bestimmen gibt
es viele Möglichkeiten. Denkbar ist die Anwendung des Algorithmus von Monge und
Elkan auf Q-Gramme. Wesentlich einfacher und schneller zu berechnen ist jedoch
der *Jarcard-Koeffizienten*. Dieser gibt die Ähnlichkeit zweier Mengen $A, B$
mit $J(A,B) = {{|A \cap B|}\over{|A \cup B|}}$ an. Ein ähnliches Maß bietet der
*Simpson-Koeffizienten*, welcher die Überlappung zweier Mengen $S(A,B) = \frac{|
A \cap B | }{\min(|A|,|B|)}$ bestimmt.

Ein weiteres Vorgehen auf Basis atomarer Strings ist WHIRL von Cohen
[@Coh:WHIRL:00]. Es kombiniert die Kosinus-Ähnlichkeit mit dem TF/IDF
Gewichtungsschema. Für alle atomaren Strings $w$ wird ein Gewicht berechnet
$$\nu_\sigma(w) = log(tf_w + 1) \cdot log(idf_w)$$, wobei $tf_w$ die Anzahl der
Vorkomnisse von $w$ in $\sigma$ ist und $idf_w$ die Anzahl der Datensätze, einer
Datenbank $D$, in welchen $w$ vorkommt. Die Kosinus-Ähnlichkeit zweier String
$\sigma_1$, $\sigma_2$ ist dementsprechend definiert als $$sim(\sigma_1,
\sigma_2) = \frac{\sum_{i=1}^{|D|}{\nu_{\sigma_1}(j) \cdot
\nu_{\sigma_2}(j)}}{\|\nu_{\sigma_1}\| \|\nu_{\sigma_2}\|}$$ Mit WHIRL ist es
möglich vertauschungsicher die Ähnlichkeit von Strings zu bestimmen. Ein großer
Vorteil dieses Algorithmus ist, dass nach erheben der TF/IDF Statistik, die
Ähnlichkeit in $O(1)$ berechnet werden kann, da lediglich Werte nachgeschlagen
werden müssen. Dem entgegen steht, wie bei Monge und Elkan, dass typographische
Fehler nicht erkannt werden. Deshalb erweitern Gravano et. al [@GIKS:Text:03]
WHIRL und nutzen statt atomare Strings Q-Gramme. Dadurch lassen sich bei
gleicher Komplexität auch Rechtschreibfehler erkennen, da ein Großteil der
Q-Gramme gleich ist. Allerdings geht zu zugunsten von Speicherkosten, da die
TF/IDF Statistik dementsprechend größer wird.

#### Phonetische Ähnlichkeit

Die phonetische Ähnlichkeit ist sowohl zeichen- als auch tokenbasiert. Es
werden jedoch nicht die Zeichen oder Token miteinander verglichen, sonderen die
Sprechlaute von Wörtern. So ist es möglich gleich gesprochene Wörter mit
unterschiedlicher Schreibweise zu finden. Dies ist vor allen Dingen bei Namen
sehr nützlich, da es hier eine besonders hohe Dichte an gleich klingenden
Worten mit kleinen Variationen in der Schreibweise gibt. Phonetische
Enkodierungsschema funktionieren allerdings meist nur für eine Sprache oder
einen Akzent.

#### Nummerische Ähnlichkeit

Während es für Strings eine Vielzahl ein Vergleichmöglichkeiten gibt ist die
Anzahl bei den nummerischen Überschaubar. Die offensichtlichste Methode ist eine
Nummer als String zu behandeln und entsprechende Algorithmen zu verwenden.
Andere eher primitive Vorgehensweisen sind etwa, die ersten $n$ oder letzten $m$
Ziffern miteinander zu vergleichen.

#### Kernel Ähnlichkeit

Anhand zweier gegebener Strings, gibt es keine offensichtliche Antwort auf die
Frage: Wie ähnlich sind $\sigma_1$ und $\sigma_2$. Im Gegensatz dazu kann dies
für Vektoren in $\mathbb{R}^d$ eindeutig, beispielsweise über die
Kosinus-Ähnlichkeit $\|\sigma_1\|-\|\sigma_2\|$ berechnet werden
[@SRR:Large:07]. Kernelfunktionen sind die Hauptidee von Support Vector Machines
(SVM), welche von Boser et al. [@BGV:Training:92] eingeführt wurden. In ihrer
einfachsten Form lernen SVMs eine separierende Hyperplane zwischen einer Menge
von Punkten, welche den Abstand zwischen der Hyperplane und dem nähesten Punkt
maximiert. Eine Kernelfunktion berechnet das innere Produkt zwischen Objekten im
Hyperraum (Feature Space). Dazu bildet der Kernel die Objekte (Vektoren)
implizit auf den Feature Space ab [@LSS.EA:Text:02]. Diese Kernel weisen eine
Reihe statistischer interessanter Eigenschaften auf, beispielsweise, dass ihre
Performanz unabhängig von der Dimensionalität ist, auf welcher die Seperation
stattfindet. Dadurch ist es möglich in hohen Dimensionalitäten zu arbeiten ohne
Überanzupassen [@LSS.EA:Text:02].

Der einfachste und meist genutzte String Kernel ist der Bag-of-Words und n-gram
Kernel. Dabei werden die Anzahl der vorkommenden Worte in $\sigma$ gezählt und
in einem dünnbesetzten Vektor, über die Menge aller bekannten Worte aller
bekannten Strings, erzeugt. Wie bereits bekannt sind atomare String anfällig für
typographische Fehler. Aus diesem Grund gibt es auch Variationen des Kernels,
welche Q-Gramme nutzen, um dieses Problem zu umgehen.

Bilenko & Mooney [@BM:Adaptive:03] stellen ein Verfahren auf Basis der TF/IDF
Ähnlichkeit von Cohen vor und nutzen eine SVM, um eine Stringähnlichkeit zu
erzeugen. Dazu nutzen Sie die gewichteten Terme nicht, um einen Ähnlichkeitswert
zu berechnen, sondern um einen Vektor zu erzeugen. Dabei werden die Komponenten
der bekannten Summe $\frac{{x_i \cdot y_i}}{\|x\|\|y\|}$, welche zum i^ten^
Element des Vokabulars gehören, einem $d$-dimensionalen Vektor zugeordnet
$\mathbf{p}(x, y) = \langle \frac{{x_i \cdot y_i)}}{\|x\|\|y\|} \rangle$. Durch
die Vektoren ist es möglich einen beliebigen SVM-Kernel zu verwenden, um die
Daten im Feature Space abzubilden. Die SVM wird anhand von klassifizierten
Matches und Non-Matches trainiert und bestimmt so eine Hyperplane, welche
Matches von Non-Matches trennt. Die Ähnlichkeit zwischen $x$ und $y$ wird
anschließend anhand der Entfernung zur Hyperplane berechnet. Ein Paar das zu den
Matches gehört und weit von der Hyperplane entfernt ist, bekommt eine hohe
Ähnlichkeit. Daraus folgt, dass ein Paar welches zu den Non-Matches gehört und
weit von der Hyperplane entfernt ist, eine sehr niedrige Ähnlichkeit erhält.

Ebenfalls von Lodhi et al. kommt eine Kernelfunktion, um Strings im Feature
Space miteinander zu vergleichen, ohne diese vorher in Vektoren zu zerlegen. Der
sogennante String Subsequence Kernel (SSK) vergleicht Strings, indem er
Stringvektoren erzeugt, welche einen bestimmten Substring beinhalten oder nicht.
Dabei wird jedes Vorkommen eines Substrings anhand der Übereinstimmung
gewichtet. Die Übereinstimmung erlaubt beispielseweise auch Lücken, sodass der
Substring 'c-a-r' in den beiden Wörtern '**car**d' und ' **c**ust**ar**d' mit
unterschiedlicher Gewichtung vorkommt.

## Klassifizierer

Die Aufgabe von Klassifizieren oder Matching-Strategien (vgl. Köpcke & Rahm
[@KR:Frameworks:10]) ist es, Datensatzpaare in zwei Mengen Matches und
Non-Matches kategorisieren. Im Gegensatz zu den Attributesähnlichkeitsmaßen
bewerten diese einen kompletten Datensatz, welcher im Normalfall aus mehreren
Attributen besteht. Klassifizierer können nach Elmagarmid et al.
[@EIV:Duplicate:07] in zwei Kategorien einordnen werden.

* Vorgehen die *Trainingsdaten* benötigen, um zu Lernen welche Datensätze
  übereinstimmen. Hierzu gehören überwachte, semi-überwachte, aktive und
  unüberwachte Lernstrategien.
* Vorgehen die *Domänenwissen* oder *generische Distanzmaße* nutzen, um
  Übereinstimmungen zu finden.

### Distanzbasierende Verfahren

Nachdem die Ähnlichkeit zwischen den Attributen der Kandidatenpaaren berechnet
wurden, gibt es zu jedem Attributspaar $a_1, \dots, a_n$ einen Vektor $(a_{1_s},
\dots, a_{n_s})$ mit der ermittelten Attributsähnlichkeit $s$.

#### Schwellenwertbasierend

Die naivste Art und Weise zu klassifizieren sind nach Christen [@Chr:Data:12,
Kap. 6] Schwellenwerte. Dazu werden die Ähnlichkeitsvektoren zu einer
Gesamtähnlichkeit $g$ aufsummiert. Anschließend werden je nach Ausprägung bis zu
zwei Schwellen festgelegt. In der Variante mit einer Schwelle $t$, werden die
Kandidatenpaare in zwei Klassen mit $g \geq> t$ als Matches und $g < t$ als
Non-Matches klassifiziert. Werden zwei Schranken $t1$ und $t2$ genutzt, wird in
drei Klassen gegliedert, Matches mit $g \geq t1$,  Non-Matches mit $g \leq t2$
und zusätzlich gibt es noch den Bereich $t2 < g < t1$, welcher ein potientielles
Match bedeutet und manuelle klassifiziert werden muss. Dabei sollen $t1$ und
$t2$ so gewählt werden, dass die Rate an Missklassifikationen minimiert wird.
Der Nachteil dieser Methodik ist, dass beim Mitteln des Vektors alle Attribute
mit gleichem Gewicht zum endgüligen Wert beitragen. Dadurch wird die Wichtigkeit
der unterschiedlicher Attribute und Wertstellung innerhalb des Datensatzes
verworfen. Um dem entgegenzuwirken können für jedes Attribut Gewichte vergeben
werden, mit welchen die Wertstellung der Attribute angegeben werden kann.
Dennoch gehen beim Aufsummieren von Ähnlichkeiten detailerte Informationen über
die einzelenen Ähnlichkeiten verloren.

#### Regelbasierend

Christen [@Chr:Data:12, Kap. 6] beschreibt die regelbasierenden Klassifikation
als Anwendung der Prädikatenlogik erster Stufe (PL1). Dabei wird ein
Klassifikationspredikat in konjunktiver Normalfrom mit disjunktiven Termen
geschrieben $$P = (term_1,1 \lor term_1,2 \lor \dots) \land \dots \land
(term_n,1 \lor term_n,2 \lor \dots).$$ Der Vorteil gegenüber den
schwellenbasierenden Verfahren ist, dass Terme auf Attribute angewendet werden
und dadurch die Informationen der einzelnen Attributsähnlichkeiten nicht
verloren gehen. Mit der regelbasierten Klassifikation kann in beliebig viele
Klassen kategorisiert werden. Typischerweise werden entweder zwei Match,
Non-Match oder zusätzlich potientielles Match klassifiziert. Bei Match und
Non-Match in nur ein Predikat $P_m$ notwendig, da alle wahren Paare als Matches
und alle falschen Paare als Non-Matches klassifiziert werden. Für potentielle
Matches wird ein weiteres Predikat $P_pm$ benötigt, dementsprechend sind
Attribute Non-Matches, wenn sowohl $P_m$ als auch $P_pm$ falsch ist. Für die
Bestimung der Prädikate gibt es zwei Möglichkeiten. Die erste ist einen
Domainexperten das Prädikat festlegen zu lassen. Dies ist allerdings ein sehr
zeitintensiver Prozess, welcher bei aller Expertise in den meisten Fällen durch
ausprobieren gelöst werden muss. Die Alternative ist ein Prädikat zu Lernen, was
ähnlich zum Lernen eines Blocking Schema (vgl. @sec:blk_scheme) funktioniert.

### Überwachtes bzw. semi-überwachtes Lernen

Die Verfahren für überwachtes und semi-überwachtes Lernen benötigen eine Menge
von klassifizierten Daten in der Form von Matches und Non-Matches. Anhand dieser
Trainingsdaten kann ein Klassifikationsmodel erstellt werden. Soll das Model
Datensätze nur in die zwei Klassen Matches und Non-Matchses ordnen, wird ein
binärer Klassifizierer gesucht.

#### Decision Trees

Decision Trees sind als Klassifizierer sehr beliebt, da ihre Funktionsweise
anschaulich ist. Zudem kann ein Modell übersichtlich visualisiert werden, sodass
es intiutiv,  auch von Laien, interpretiert werden kann. Ähnlich zu dem
regelbasierten Verfahren prüft auch der Decision Tree den Ähnlichkeitswert eines
bestimmten Attributes, welches einem Wert im Vektor entspricht. Dementsprechend
kann ein Model eines Decision Tree direkt in einem Prädikat formuliert werden.

(Beispiel?)

#### Support Vector Machines

Bilenko & Mooney [@BM:Adaptive:03] stellen ein Lernverfahren vor, welches eine
SVM-Klassifizierer nutzt. In @fig:bilenkosvm sind zwei Datensätze gezeigt. Jedes
Attributspaar ist dabei ein Teil eines Vektors, wobei die beiden Werte durch
eine beliebige Attributsähnlichkeit repräsentiert werden. Die Vektoren werden
dann von einem SVM-Model in Matches und Non-Matches klassifiziert. Trainiert
wird das SVM-Model anhand von Match und Non-Match Vektoren.

![Datensatzklassifikation. Aus [@BM:Adaptive:03].](pictures/bilenko_class_svm.png){#fig:bilenkosvm}

Christen [@Chr:Automatic:08] erweitert dieses Verfahren, indem mehrere SVM
Modelle trainiert werden. Die initiale SVM wird mit der Mengen der
offensichtlichen Matches und Non-Matches der Gesamttrainingsmenge trainiert. Bei
offensichtlichen Matches sind die Werte der Vektoren sehr nahe an 1 und bei
offensichtlichen Non-Matches sehr nahe bei 0. Alle nicht offensichtlichen
Vektoren der Trainingsmenge werden anschließend mit der initialen SVM
klassifiziert. Je nach Klassifizierungsergebnis werden diejenigen, welche
am weitesten von der seperierenden Hyperplane entfernt sind, zur Menge der
offensichtlichen Matches bzw. Non-Matches hinzugefügt. Die zweite SVM wird dann
mit den erweiterten Trainingsmengen trainiert. Diese Schritte werden solange
wiederholt, bis ein Stopkriterium erfüllt ist.

### Aktives Lernen

Ein großer Nachteil der überwachten Lernverfahren ist, dass die Trainingsmenge
viele Beispiele benötigt und das diese repräsentativ für die zu Gesamtmenge von
Entitäten sein muss. Als Alternative dazu gibt es die aktiven Lernverfahren,
welche initial nur eine sehr kleine Trainingsmenge (*seed*) benötigen. Auf Basis
des initialen Models werden in Interaktion mit einem erfahrenen Benutzer
Datensatzpaare selektiert, die helfen das Klassifikationsmodel zu verbesern.
Eine initiale Model kann relativ einfach über offensichtliche Matches und
Non-Machtes erzeugt werden. Anschließend ist aktives Lernen ein iterativer
Prozess. Zunächst wird die Trainingsmenge mit dem Model klassifiziert
Anschließend ist aktives Lernen ein iterativer Prozess. Zunächst wird die
Trainingsmenge mit dem Model klassifiziert. Aus der Menge klassifizierte Daten
werden die Interessantesten ausgewählt, die manuelle von einem Benuzter
klassifiziert werden. Anschließend werden diese zu den initialen Daten
hinzugefügt und es wird ein neues Model trainiert. Diese Schleife wird solange
wiederholt bis ein Stopkriterum (Anzahl von Iterationen oder minimale
Genauigkeit) erreicht wurde.

Arasu et al. [@AGK:active:10] kombinieren eine aktives Lernvorgehen mit einem
Blockingmechanismus, welcher entweder mit einem Decision Tree oder einer SVM
funktioniert. Dabei gibt der Benutzer als Stopkriterium die Minstestpräzision
an. Der Lernprozess versucht dann eine Model zu finden, welches einen hohen
Recall liefert und gleichzeitig die Anzahl der manuell zu klassifizierenden
Paare gering hält.

## Messen von Qualität- und Komplexität

Folgender Abschnitt bezieht sich auf Analysen und Erklärungen zu Qualität und
Komplexität von Entity Resolution Systemen aus Christen [@Chr:Data:12].

Aus den bis hier vorgestellten Vefahren zu Entity Resolution stellt sich die
Frage: Wie kann die Qualität und Komplexität dieser Verfahren gemessen werden,
sodass zum einen das Verfahren selbst bewertet werden kann und gleichzeitig eine
Vergleichbarkeit zu anderen Verfahren hergestellt wird. Damit das Ergebnis der
ER-Verfahren überprüft werden kann, ist es unerlässlich über die Ground Truth
Daten (auch Gold Standard Daten) zu verfügen. Dabei sollten diese, wie auch bei
den maschinellen Lernverfahren, möglichst die Charackteristik der zu
überprüfenden Daten wiederspiegeln. Daraus entsteht die nächste Frage: Woher
kommen die Ground Truth Daten:

* Wird versucht einen entwickelten Algorithmus/Verfahren zu bewerten, dann
  empfiehlt es sich einen der frei verfügbaren Datensätze zu nehmen, zu welchen
  bereits Ground Truth Daten existieren. Welche beispielsweise von
  Wissenschaftlern oder Domainexperten manuell klassifiziert wurden. Das Problem
  ist, dass viele dieser Datensätze nur wenige Einträge (meist $< 10.000$) haben
  und daher kaum Bezug zu Realdaten haben.
* Soll ein Verfahren auf Daten angewendet werden, zu welchen keine Ground Truth
  existiert, müssen diese manuell erzeugt werden. Dabei werden Datensatzpaare
  zufällig erzeugt und müssen anschließend von einem Prüfern in Matches und
  Non-Matches klassifiziert. Ein großer Nachteil dieser Methode ist, selbst wenn
  ein Blockingverfahren angewendet wurde, dass die Zahl der zu klassifizierenden
  Paare riesig ist. Hinzu kommt, dass die Anzahl der Matches nur einen Bruchteil
  der Paare betrifft, weshalb die Ground Truth ein deutliches Ungleichgewicht
  aufweisen wird. Ein weiteres Problem ist, dass in diesem Prozess Fehler
  gemacht werden. Dabei entstehen die Fehler nicht bei den offensichtlichen
  Match und Non-Matches, sondern meist in Paaren, die auch für den Menschen nur
  schwer zu bewerten sind. Des Weiteren kann es zu unterschiedlichen
  Klassifizierungen je nach Prüfer kommen und auch der selbe Prüfer kann je nach
  Gemütslage und Konzentrationslevel unterschiedliche Aussagen über das selbe
  Paar treffen. Vogel et al. [@VHD.EA:Reach:14] haben deshalb einen sogenannten
  *Annealing Standard* entwickelt, welcher das Erstellen einer Ground Truth über
  einen iterativen Prozess vereinfachen sollen. Dabei wird zunächst mit einem
  Klassifizierer eine Baseline erzeugt, die den Annealing Standard darstellt.
  Anschließend werden mit einem weiteren Klassifizierer, welcher der vorherige
  mit anderen Parametern sind kann, Paare erzeugt und mit der Baseline
  verglichen. Das Quorum der beiden bildet den neuen Annealing Standard. Die
  übrigen Paare werden zu manuellen Inspektion Prüfern vorgelegt und die dadurch
  erzeugten Matches und Non-Matches werden mit dem Annealing Standard
  verschmolzen. Diese Iteration wird solange wiederholt, bis das Delta der
  Klassifizierer einen bestimmten Maximalwert an Paaren unterschreitet.
* Werden schnell große Datensätze mit entsprechender Ground Truth benötigt,
  bieten sich synthetische generierte Datensätze an. Damit diese repräsentativ
  sind, sollten Sie gleichen Attribute haben, wie die echten Datensätze. Dazu
  wird eine Datenbank möglicher Attributswerte benötigt, welche der Generator
  verwenden soll. Zusätzlich gibt es Parameter, um die größe des Datensatzes und
  Anzahl der Duplikate, die Häufigkeitsverteilung der einzelnen Attribute und
  die Modifikationen der Duplikaten gegenüber dem Original, in typographische,
  OCR oder poenetische Fehler, zu bestimmen. Ein solcher Generator beinhaltet
  beispielsweise Ferbl Framework von Christen [@Chr:Febrl:08].
* Anstatt synthetische Datensätze zu generieren und anschließend Fehler
  einzufügen, ist stattdessen auch möglich ein einen bestehenden Datensatz
  Fehler einzubauen und diese als entsprechende Ground Truth zu verwenden.
  Dadurch werden allerdings die tatsächlichen Matches unterschlagen, was zu
  Konflikten bei der Entity Resolution führen kann.

Ist zu einem Datensatz die Ground-Truth verfügbar, so können die klassifizierten
Datensätze einer der Kategorien in @fig:cls_pred_matrix zugeordnet werden.

* True Positives (TP), sind alle Paare die als Matches klassifiziert wurden und
  auch nach Ground Truth tatsächlich Matches sind.
* False Positives (FP), sind alle Paare die als Matches klassifiziert wurden
  aber keine sind. D.h. der Klassifizier hat bei diesen Paaren einen Fehler
  gemacht.
* False Negatives (FN), sind alle Paare die als Non-Matches klassifiziert wurden
  aber tatsächlich Matches sind. Auch hier hat der Klassifizierer einen Fehler
  gemacht.
* True Negatives (TN), sind alle Paare die als Non-Matches klassifiziert wurden
  und auch tatsächlich zwei verschiedene Entitäten identifizieren.

![Matrix mit den vier Klassifikationszuständen](pictures/matrix_classification.pdf){#fig:cls_pred_matrix}

#### Qualitätsmaße

Das Ergebnis eines idealen Klassifizierer ist, dass soviele Matches wie möglich
True Positives sind und die Anzahl der False Positives, sowie False Negatives
klein ist. Auf Basis der vier Klassifikationsklassen können Qualitätsmaße
bestimmt werden. Die folgende Liste zeigt die beliebtesten Methoden und erklärt
ihre Stärken und Schwächen.

* *Accuracy*.
  $$acc = \frac{TP+TN}{TP+FP+TN+FN}$$ {#eq:accuracy}
  Die Genauigkeit ist ein weit verbreitetes Qualitätsmaß für Binär- und
  Multi-Klassen Probleme im Maschine-Learning Bereich. Die Accuracy ist nützlich
  in Situationen, in welchen die Klassen möglichst gleichverteilt sind. Für
  Entity Resolution ist dieses Maß daher nur bedingt geeignet, da zwischen
  Matches und Non-Matches fast immer ein Ungleichgewicht zugunsten von
  Non-Matches besteht. Daher sind die meisten klassifizierten Ergebnisse True
  Negatives, welche die Gleichung dominieren. Dadurch wird fälschlicherweise,
  trotz weniger True Positives und vieler False Positives, sowie False
  Negatives, eine hohe Accuracy gemessen. Daher sind für Entity Resolution auch
  andere Maße wie *Specificity* oder *False positive rate* zu vermeiden.

* *Precision*.
  $$prec = \frac{TP}{TP+FP}$$ {#eq:precision}
  Precision wird oft als Qualitätsmaß vor Suchergebnisse genommen, da es den
  Anteil der True Positives in den Matches berechnet. Für Entity Resolution
  misst die Precision wie viele Matches (TP + FP) ein Klassifizierer korrekt
  bestimmt hat.

* *Recall*.
  $$rec = \frac{TP}{TP+FN}$$ {#eq:recall}
  Recall misst den Anteil der tatsächlichen Matches (TP + FN), welche korrekt
  (TP) als Matches klassifiziert wurden. Zwischen Recall und Precision gibt es
  einen Kompromiss. Beispielsweise kann der Recall verbessert werden, indem die
  Precision abgesenkt bzw. die Precision verbessert indem der Recall gesenkt
  wird.

* *F-measure*. Auch bekannt als *f-score* oder *f_1-score*.
  $$fmeas = 2 \cdot \left(\frac{prec \cdot rec}{prec + rec}\right)$$ {#eq:fscore}
  Das F-measure berechnet das harmonische Mittel zwischen Precision und Recall.
  Eine guter F-measure Wert ist daher ein Kompromiss zwischen den beiden.

Die oben genannten Qualitätsmaße berechnen alle einen exakten Wert für die
Qualität eines Klassifizierers. Aus den bekannten Verfahren für Blocking,
Vergleich und Klassifizierung geht hervor, dass diese eine Reihe von Parametern
haben, um das Ergebnis zu kalibrieren. Deshalb ist es sinnvoll eine Reihe von
Werten zu erzeugen, um diese miteinander zu vergleichen. Ein solcher Vergleich
funktioniert am einfachsten per Visualisierung. Die folgenden drei
Visualisierungen werden dazu oft verwendet:

* *Precision-recall Graph*. Diese Visualisierung zeigt den Kompromiss zwischen
  Precision und Recall. Für jede Parametereinstellung eines Klassifizierers wird
  ein Punkt im Graph erzeugt. Dabei ist die X-Achse stets der Recall und die
  Y-Achse die Precision. Durch den Kompromiss started die Kurve meist in der
  oberen linken Ecke mit hoher Precision und niedrigen Recall und endet in der
  linken unteren Ecke mit entgegengesetzen Werten. Dabei ist das Ziel die Kurve
  möglichst Nahe an die linke obere Ecke zu bekommen, in welcher Precision und
  Recall maximal sind.
* *F-measure Graph*. Anstatt zwei Qualitätmaße gegeneinander zu zeichen, kann
  man diese auch zusammen im Bezug auf einen bestimmten Parameter darstellen.
  Der F-measure Graph, beispielsweise plottet Precision, Recall und F-measure
  gegen einen Parameter, wie etwa die akkumulierte Gesamtwahrscheinlichkeit bei
  den schwellenbasierten Klassifizierern genutzt. Darüber kann dann abgelesen
  werden, bei welchem Wert (z.B. Schwelle) die beste Precision, der beste Recall
  und das beste F-measure erreicht wird.
* *ROC Kurve*. Wie der Precision-recall Graph vergleicht die
  Receiver-Operating-Characteristic (ROC) Kurve zwei Qualitätmaße. In diesem
  Fall die auf der X-Achse die False-Positve-Rate und auf der Y-Achse der
  Recall. Obwohl die ROC Kurve robust gegen ungleichgewichtete Klassen ist, so
  ist diese mit vorsicht für Entity Resolution zu genießen, da die False
  Positive Rate die True Negatives miteinbezieht, hat die Kurve das Problem
  etwas zu optimistische zu sein. Verschiedene ROC Kurven verschiedener
  Klassifizierer mit unterschiedlichen Parametern zu verlgeichen, kann dennoch
  nützlich sein, um deren Qualität zu bewerten.

#### Komplexitätsmaße

Neben der Qualität bestimmt auch die Effektivität wie gut Entity Resolution
Systeme funktionieren. Die offensichtlichste Art Effektivität zu messen ist, die
Laufzeit des Verfahrens zu messen und miteinander zu vergleichen. Allerdings ist
dieser Ansatz abhängig von der genutzten Hardware und bietet keinen
plattformübergreifenden Vergleich. Für die folgenden Maße müssen zunächst einige
Mengen definiert werden. Zunächst gliedern wird in die Menge der verlichenen
Datensatzpaar $n_M$ und die Menge der nicht vergleichen Datensatzpaar $n_N$.
Dementsprechend ist $n_M + n_N = m \cdot n$ für Entity-Linking und $n_M
+ n_N = m(m-1)/2$ für Deduplizierung. Die Mengen Matches und Non-Matches werden
  mit $s_M$ bzw. $s_N$ bezeichnet, wobei $s_M + s_N \leq n_M + n_N$.

* *Reduction Ratio*. Dieses Maß gibt an wie viele Datensatzpaare von einem
  Blockingverfahren generiert worden sind und setzt diese ins Verhältnis mit der
  Anzahl von Datensatzpaaren, welche ohne Blocking generiert worden wären. Das
  Reduction Ratio ist definert als $$rr = 1 - \left(\frac{s_M + s_N}{n_M +
  n_N}\right).$$ {#eq:reductionratio}
* *Pairs completeness*. Dieses Maß berechnet den Anteil der möglichen Matches.
  Es wird berechnet mit $$pc = \frac{s_M}{n_M}$$ {eq#:pairscompletness}.
  Pairs completeness ist mit dem Recall aus @eq:recall verwandt. Je geringer die
  Pairs completeness ist, desto geringer ist auch die Matchingqualität, da
  dieses Maß eine Obergrenze für einen möglichen Recall bestimmt. Denn
  tatsächliche Matches, die von einem Blocking Mechnismus nicht selektiert
  werden, können auch nicht klassifiziert werden. Zwischen Reduction Ratio und
  Pairs Completness gibt es einen offensichtlichen Kompromiss, je mehr
  Datensatzpaare erzeugt werden, desto mehr tatsächliche Matches können gefunden
  werden.
* *Pairs quality*. Dieses Maß berücksichtigt die Qualität eines
  Blockingverfahren, indem es selektierten tatsächlichen Matches in relation mit
  mit allen selektierten Paaren stellt. Es wird berechnet mit $$pq =
  \frac{s_M}{s_M + s_N}.$$ {#eq:pairsquality}
  Die Pairs quality ist verwandt mit der Precision aus @eq:precision. Eine hohe
  Pairs qualtiy bedeutet, dass ein Blockingverfahren hauptsächlich Paare
  erzeugt, welche tatsächlich Matches sind. Auch hier gibt es ähnlich zu
  Precision und Recall einen Kompromis zwischen Pairs completness und Pairs
  quality.

## Datensätze

### CORA

Der CORA Datensatz beinhaltet 1879 bibliographische Einträge über
wissenschaftliche Veröffentlichungen aus dem Maschine Learning Bereich. Die
Einträge bestehen aus Authoren, Titel, Publikationsjahr und Konferenz bzw.
Journal. Insgesamt beinhaltet dieser Datensatz 64.577 Duplikate. Dieser
Datensatz ist besonders schwierig zu Deduplizieren, da teilweise nur Initialen
der Authoren vorhanden sind bzw. Attribute zusammengefügt oder getauscht wurden.

### Abt-Buy & Amazon-GoogleProducts

Diese beiden Datensätze beinhalten Produkte aus dem Onlinehandel verschiedener
Plattformen mit Name, Beschreibung, Hersteller und Preis. Der Abt-Buy Datensatz
beinhaltet 2171 Einträge mit 1096 Duplikaten. Im Amazon-GoogleProducts Datensatz
sind es 4587 Einträge mit 1299 Duplikaten.

### DBLP-ACM & DBLP-Scholar

Diese beiden Datensätze beinhalten bibliograpische Einträge mit Titel,
Author(en), Konferenz, und Jahr. Der DBLP-ACM Datensatz beinhaltet 4908 Einträge
und 2223 Duplikate. Im DBLP-Scholar Datensind sind 66877 Einträge mit 5346
Duplikaten. Dabei ist zu beachten, dass der DBLP-ACM Datensatz einfach zu
klassifizieren ist, da ein Großteil der Daten durch eine Instanz gepflegt wird.

### Restaurant

Der Restaurant Datensatz ist ein kleiner mit lediglich 864 Einträgen, welche aus
Restaurantname, Adresse, Telefonnummer und der Küchenart bestehen. Es gibt
insgesamt 112 Restaurantduplikate, welche doppelt vorkommen.

### NCVR

Der NC Voter Registration (NCVR) Datensatz beinhaltet ca. 6 mio Datensätze aus
dem Wählerverzeichnis des Bundestates North Carolina in den USA. Eine genaue
Analyse des Datensatzes wurde von Christen [@Chr:Preparation:13] durchgeführt.
Der Datensatz beinhaltet ca. 145.000 Duplikate zwischen zwei Einträgen, sowie
3.500 zwischen drei und mehr Einträgen. Die Zuordnung der Duplikate wurde dabei
über die Wählerregistriernummer getätigt. Weitere Attribute sind Namenpräfix,
Vorname, Zweiter, Vorname, Nachname, Namensuffix, Alter, Geschlecht,
Rassenziffer, Ethnizitätsziffer, Strasse + Hausnummer, Stadt, Bundesland,
Postleitzahl, Telefonnummer, Geburtsort und Registrierdatum.

### Febrl

Die Febrl-Datensätze wurden syntetisch durch den Febrlgenerator erzeugt. Die
Attributsdaten dafür liefert ein australisches Telefonbuch. Die generierten
Einträge haben folgende Attribute: Kultur, Geschlecht, Alter, Geburtsdatum,
Titel, Vorname, Nachname, Bundesland, Vorort, Postleitzahl, Hausnummer, Straße
und Telefonnummer.

Zum Entwickeln:

* Febrl-4k-1k: 5.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-9k-1k: 10.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-90k-10k: 100.000 Einträge mit 10.000 Duplikaten zwischen zwei Datensätzen

Zum Evaluieren:

* 5.000.000 Einträge 100.000 Duplikate zwischen zwei und mehr und
  Attributsverteilung (Uniform, Poisson, Zipfian).
