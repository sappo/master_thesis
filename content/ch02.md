# Grundlagen

## Blocking

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|*|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}*|A|*(|A|-1)$ durchgeführt Vergleiche werden. In beiden Fällen ist
die Anzahl der Vergleiche quadratisch zur Eingabemenge  [@Kol:Effiziente:14]. In
der Studie [@KTR:Evaluation:10] zeigen Köpcke et al., dass das kartesische
Produkt für größe Datenmengen nicht skaliert. Aus diesem Grund reduzieren
moderne Entity Resolution Frameworks den Suchraum auf die wahrscheinlichten
Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden zur Reduzierung des
quatratischen Suchraum werden übergreifened als Blockingmethoden bezeichnet.
Neben Blocking werden auch Windowing- und Indexing Verfahren eingesetzt. Während
Blockingverfahren die Anzahl der notwendigen Vergleiche drastisch reduzieren,
indem Non-Matches ausgeschlossen werden, besteht dennoch die Gefahr, dass
fälschlicherweise tatsächliche Matches rausgefiltert werden. Daher ist es
notwendig die Güte des Blockingverfahrens zu bestimmen. Dazu werden zwei
Kennziffern erhoben. Zum einen die *Reduction Ratio*, welche die Reduzierung der
Vergleiche im Gegensatz zum Kartesischen Produkt ausdrückt, sowie die *Pairs
Completeness*, welche den Anteil der tatäschlich ausgewählten Duplikate, die
sich nach dem Blocking in der Kandidatenmenge befinden, beschreibt.

Prinzipelle erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" von einander befinden. Zur
Durchführung der Gruppierung oder Sortierung müssen sog. Block- bzw.
Sortierschlüssel für jeden Datensatz erzeugt werden. Diese Schlüssel werden von
den Attributwerten oder einem Teil der Attributwerte abgeleitet und stellen eine
Signatur des Datensatzes dar.

### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee ist eine Menge von Datensätzen in disjunkte
Partitionen (gennant Blöcke) zu teilen. Anschließend werden nur die Datensätze
des jeweiligen Blocks miteinander verglichen. Dazu wird jedem Datensatz ein
Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens hängt daher
maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und Größe der
Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
Abbildung @fig:sb_blocking zeigt die Ausführung des Blockingverfahrens
beispielhaft an einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein
Blockschlüssel (hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses
Schlüssels gruppiert. Die größe der einzelnen Blöcke bestimmt die Reduction
Ratio. Diese hängt allerdings immer von der Datenquelle ab und kann daher nicht
pauschalisiert werden. Bei der Generierung der Blockschlüssel können fehlerhafte
Werte einzelner Attribute dazu führen, dass Dupliakte in unterschiedlichen
Blöcken landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden
Datensatz mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert
werden. Dieser Ansatz nennt sich Multi-pass Blocking.

![Beispielhafte Standard Blocking Ausführung. Nach [@Kol:Effiziente:14].](pictures/standard_blocking.pdf){#fig:sb_blocking}

### Q-gram Indexing

Das Q-gram Indexing basiert auf der Idee Datensätze unterschiedlicher aber
ähnlicher Blockschlüssel miteinander zu vergleichen. Ein Blockschlüssel wird
dazu in eine Liste $G$ von q-Grammen überführt. Ein q-Gram ist ein Substring der
Länge q des ursprünglichen Blockschlüssels. Alle Permuationen der q-Gram Liste
mit einer Mindestlänge $l=max(1,\floor{\#G*t})$ werden konkateniert und dienen
als Schlüssel der Blöcke, wobei $t$ ein Schwellwert zwischen 0 und 1 ist. Dabei
werden Datensätze mehreren Blöcken zugewiesen. Ist $t=1$ entspricht Q-gram
Indexing dem Standard Blocking. Dieses Verfahren kann als Alternative zum
Multi-pass Verfahren beim Standard Blocking genutzt werden. Der große Nachteil
ist der hohe Aufwand bei der Berechnung aller möglichen Sublisten. Ein
Blockschlüssel mit $n$ Zeichen muss in $k=n-q+1$ q-Gramme zerlegt werden.
Insgesamt müssen dadurch $\sum_{i=max\{1,[k*t]\}}^{k} {k \choose i}$ Sublisten
berechnet werden [@Chr:Survey:12].

### Suffix Array Indexing

Das Suffix Array Indexing [@AO:Fast:05a] leitet, ähnlich wie Q-gram Indexing,
mehrere Schlüssel aus einem Blockschlüssel ab. Grundidee ist es alle Suffixe mit
einer Mindestlänge von $l$ zu bestimmen. Ein Datensatz mit Blockschlüssellänge
$n$ wird in $n-l+1$ Blöcke eingeordnet. Ist $n<l$ wird der Ausgangsschlüssel als
einziger Schlüssel verwendet. Durch die größere Menge an Kandidatenpaaren ist
i.Allg. die *Pair Completeness* höher (vgl. Multi-pass). Zudem ist der Aufwand
der Berechung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer. Im
Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren jedoch
deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze
unnötigerweise mehrfach miteinander verglichen werden hoch. Deshalb werden aus
Blöcken, welche einen bestimmten Schwellwert überschreiten alle Datensätze
entfernt, die min. einen weiteren längeren Blockschüssel haben.

### Sorted Neighborhood

Das Sorted Neighborhood Verfahren, ist ein Sortierverfahren, welches 1995 von
Hernández & Stolfo zur Erkennung von Dupliakten in Datenbanktabellen vorgestellt
wurde [@HS:Merge:95]. Es besteht aus drei Phasen. Zunächst bekommt jeder
Datensatz einen Sortierschlüssel zugewiesen. Dabei muss der Sortierschlüssel
nicht einzigartig sein. Um die Berechnung des Schlüssel gering zu halten, soll
dieser durch Verkettung von Attributen bzw. Teilen der Attribute bestimmt
werden. Attribute die vorne im Schlüssel stehen haben dadurch eine höhere
Priorität. In der zweiten Phase werden die Datensatze anhand des Schlüssels
sortiert. In der dritten Phase wird ein Fenster (engl. Window) über die
sortierten Datensätze geschoben und alle Datensätze innerhalb des Windows werden
miteinander verglichen. Dieses Verfahren eignet sich besonders gut zur Erkennung
von Duplikaten innerhalb einer Datenquelle. Sollen Duplikate in mehreren
Datenquellen gefunden werden, müssen die Einträge beim Sortieren gemischt
werden. Dadurch besteht allerdings die Gefahr, das vorrangig Datensätze einer
Datenquelle miteinander verglichen werden. Vorteil zum Standard Blocking ist,
dass die Anzahl der Vergleiche lediglich von der Größe der Datenquelle und der
gewählten Fenstergröße abhängen. Ein großter Nachteil ist, dass Datensätze die
sich in der ersten Stelle des Schlüssels unterscheiden, weit von einander
entfernt sind und dadurch nicht als Matches identifziert werden. Um dennoch eine
hohe Pair Completeness zu erreichen, werden mehrere Schlüssel pro Datensatz
generiert und ein Fenster mit kleiner größe über die verschieden sortierten
Listen geschoben. Dieses Verfahren entspricht im Grunde dem Multi-pass Verfahren
beim Standard Blocking.

Ein großes Problem bei der klassischen und der Multi-pass Variante des Sorted
Neighborhood Verfahrens ist, dass die zu wählende Fenstergröße $w$ größer als
die Anzahl der Datensätze mit dem häufig vorkommensten Sortierschlüssel sein
muss, um eine gute Pair Completeness zu erreichen. Sei $n$ die Menge an
Datensätzen mit dem am häufig vorkommensten Schlüssel $k$ und $m$ die Menge der
Datensätze des darauffolgenden Schlüssels $k+1$, dann ist $w=n+m$. Nur dadurch
kann sichergestellt werden, dass alle Datensätze aus $n$ mit den "nahen"
Datensätzen aus $m$ verglichen werden. Da Sortierschlüssel für gewöhnlich nicht
gleichverteilt sind, gibt es meist wenige größe und viele kleine Mengen an
Datensätzen mit dem gleichen Sortierschlüssel. Dadurch werden Datensätze mit
seltenen Sortierschlüsseln unnötig oft mit "weit" entfernten Datensätzen
verglichen. Zudem dominiert der am häufig vorkommenste Schlüssel, genauso wie
beim Standard Blockong, die Ausführungszeit des Algorithmus.

In [@DN:comparison:09] schlagen Draisbach & Naumann eine optimierte Variante des
Sorted Neighborhood Verfahrens vor. Dabei zeigen Sie, dass Standard Blockung und
Sorted Neighborhood extreme von Overlapping bei Partitionen sind. Ihre Idee ist
es diese Überlappung zu optimieren. Dabei soll die Überlappung groß genug sein,
um tatsächliche Matches zu finden, aber gering genug, um die Menge der
Vergleiche zu reduzieren. Zunächst wird wie beim klassischen Verfahren zu
sortiert. Danach werden angrenzende Datensätze in disjunkte Partitionen zerlegt
und schließlich wird ein Überlappungsfaktor (gennant Overlap) $u$ gewählt.
Innerhalb jedes Blockes wird analog zum Standard Blocking jeder Datensatz mit
jedem anderen verglichen. Innerhalb des Overlap-Window $w=u+1$, wird jeweils das
erste Element mit allen anderen Verglichen. Ist $w=0$ entspricht das Verfahren
Standard Blocking und hat jede Partition nur ein Element entspricht es der
Sorted Neighborhood Methode. Um zu vermeiden, dass eine Partition dominiert,
können größere Partitionen in Subpartitionen geteilt werden.

Weitere Varianten veränderen die Fenstergröße anhand identifizierte Duplikate
[@DNSW:Adaptive:12].

### Canopy Clustering

Die Idee von Canopy Clustering ist es Datensätze anhander einer einfachen
Vergleichsmetrik in überlappende Cluster (=Canopies) zu partionieren. Zur
Generierung wird eine Kandidatenliste gebildet, welche inital als allen
Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen Clusters
gewählte und alle Datensätze innerhalb des Mindestabstandes $d_1$ zugewiesen.
Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren
Mindestabstandes $d_2 < d_1$ aus der Kandidatenliste entfernt. Dieser
Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die *Pair
Completeness* hängt hierbei stark der gewählten Abstandsfunktion ab.
Anschließend wird werden alle Datensätze eines Cluster miteinander verglichen.

## Blocking-Paralellisierung
