# Hinführung

## Entity Resolution

Die Methoden zur Duplikatserkennung stammen ursprünglich aus dem
Gesundheitsbereich und wurden erstmal 1969 von Felegi & Sunter [@FS:Theory:69]
formal formuliert. Je nach Fachgebiet gibt es unterschiedliche Fachbegriffe.
Statistiker und Epidemiologen sprechen von *record* oder *data linkage*, während
Informatiker das Problem unter *entity resolution*, *data* oder *field
matching*, *duplicate detection*, *object identification* oder *merge/purge*
kennen. Identifiziert werden sollen dabei beliebige Entitäten, welche oft in
Form von Datensätzen einer Datenbank vorliegen. Die Schwierigkeit dabei ist
allerdings, dass Entitäten nicht durch ein einzigartiges Attribut identifiziert
werden können, beispielsweise Produkte, bibliografische Einträge oder
selbsterfasste Onlineauskünfte. Zudem sind die Datensätze oft fehlerhaft,
beispielsweise durch Rechtschreibfehler, welche durch Tippfehler, Hörfehler oder
OCR-Fehler enstehen. Eine andere Fehlerquelle sind unterschiedliche
Konventionen, beispielsweise bei Endungen von Strassennamen `strasse`, `straße`
oder `str`. Auch denkbar sind Fehler aufgrund von Betrug. Die Methoden zur
Entity Resolution (ER) vergleichen meist eine oder mehrere Datenbanken, indem
Datensatzpaare gebildet werden. Als Ergebnis wird eine Menge von
übereinstimmenden Datensatzpaaren, d.h. zwei Datensätze, welche die selbe
Entität beschreiben, geliefert. Damit eine Übereinstimmung zwischen zwei oder
mehr Entitäten festgestellt werden kann, müssen diese verglichen werden und ein
Ähnlichkeitswert (engl. similarity score) bestimmt werden. Dieser
Ähnlichkeitswert gibt die Intensität der Übereinstimmung an. Das ER Problem wird
Formal von Köpcke & Rahm in [@KR:Frameworks:10] folgendermaßen beschrieben.
Gegeben sind zwei Mengen von Entitäten $A \in S_A$ und $B \in S_B$ zweier
Datenquellen $S_A$ und $S_B$, welche semantisch dem selben Entitätstypen
entsprechen. Das ER Problem ist es, alle Paare in $A \times B$ zu
identifizieren, welche derselben Entität entsprechen. Als Spezialfall ist dabei
die Suche in einer Datenquelle $A = B, S_A = S_B$ zu betrachten. Eine
Übereinstimmung $u = (e_i, e_j, s)$ verknüpft zwei Entitäten $e_i \in S_A$ und
$e_j \in S_B$ mit einem Ähnlichkeitswert $s \in [0, 1]$.

In der klassischen Variante arbeitet Entity Resolution auf statischen Daten,
d.h., dass während des ER-Prozesses keine neuen Daten hinzukommen. Hierbei
werden die zwei Disziplinen Deduplizierung und Entity-Linking unterschieden. Die
Deduplizierung wird auf einer Datenquelle durchgeführt und hat den Zweck alle
Duplikate in dieser Datenquelle zu finden. Anschließend werden die gefundenen
Duplikate automatisch oder manuell zusammengeführt. Entity Linking hingegen wird
auf mindestens zwei verschiedenen Datenquellen durchgeführt. Das Ziel ist es,
nicht Duplikate zusammenzuführen, sondern Entitäten zwischen den Datenquellen zu
verlinken. Damit die Links eindeutig sind, wird in der Regel vorausgesetzt, dass
die einzelnen Datenquellen dedupliziert sind.

Die Ausführung der Vergleichsmethoden ist enorm teuer, da diese jedes Attribut
zweier Datensätze miteinander vergleichen. Bei einem Vollvergleich aller
Datensätze, führt dies zu einer quadratischen Komplexität pro Attribut, was
dafür sorgt, dass bei großen Datenmengen die Ausführungszeit unakzeptabel lang
wird. Um die Ausführungszeit zu reduzieren wird versucht den Suchraum auf die
wahrscheinlichsten Duplikatsvorkommen zu begrenzen. Diese Vorgehen werden als
Blocking oder Indexing bezeichnet.

![Vereinfachter Entity Resolution Workflow aus [@Kol:Effiziente:14]. Die
Datenquelle $S$ wird vorverarbeitet und in kleinere Submengen gegliedert.
Innerhalb dieser werden Datensatzpaare miteinander verglichen und paarweise
bestimmt, ob diese der selben Entität entsprechen. Abschließend werden aus
Paaren Gruppen von Duplikaten ermittelt und als Ergenisse $M$ geliefert.
](images/entity_resolution_workflow.png){#fig:er_workflow}

@fig:er_workflow zeigt einen vereinfachten typischen Entity Resolution Workflow.
Zunächst werden die Datensätze einer Datenquelle $S$ vorverarbeitet, um typische
Fehler zu entfernen. Dazu gehört das Korrigieren von Rechtschreibfehler,
ignorieren von Groß- bzw. Kleinschreibung, beispielsweise durch Konvertierung in
Kleinschreibung, und das Ersetzten von bekannten Abkürzungen. Durch die
Vorverarbeitung kann die Qualität des Matchings verbessert, indem verhindert
wird, dass offensichtliche Abweichungen den Ähnlichkeitswert beeinflussen. Der
nächste Schritt, das Blocking, teilt die Gesamtmenge in Submengen $b_1, b_2,
\cdots, b_n$ zur Reduzierung der Gesamtkomplexität, da nur die jeweiligen Blöcke
voll verglichen werden. In @sec:blocking werden detailiert verschiedene
Blockingverfahren erläutert. Auf das Blocking folgt das Matching, hierbei werden
innerhalb der Submengen von Datensatzpaaren Ähnlichkeitswerte bestimmt. Die
Möglichkeiten der Ähnlichkeitsbestimmung werden in @sec:similarity beschrieben.
Anhand der Ähnlichkeitswerte wird anschließend für jedes Datensatzpaar
entschieden, ob es sich um ein Match (beide Datensatze beschreiben dieselbe
Entität) oder ein Non-Match (die Datensätze beschreiben unterschiedliche
Entitäten) handelt. Diese Klassifikation wird genauer in @sec:classifier
erklärt. Abschließend findet noch die Berechnung der transitiven Hülle statt, um
beispielsweise aus Paaren von Matches Gruppen zu bilden, welche derselben
Entität entsprechen $M = {(a,b), (b,c)} \implies M = {(a,b,c)}$.

Laut Köpcke & Rahm [@KR:Frameworks:10] gibt es keine Methode zur Entity
Resolution, welche allen anderen überlegen ist. Vielmehr ist der Erfolg
unterschiedlicher Methoden domänabhangig. Deshalb wurde Anfange der 00er Jahre
begonnen Frameworks zu entwickeln, welche verschiedene Methoden miteinander
kombinieren. Einen Vergleich dieser Frameworks wurde durch Köpcke & Rahm
[@KR:Frameworks:10] durchgeführt. Ein Framework besteht hierbei aus
verschiedenen Matchern und Matching Strategien. Ein Matcher ist dabei ein
Algorithmus, welcher die Ähnlichkeit zweier Datensätze ermittelt. Köpcke & Rahm
unterscheiden zwischen attributs- und kontextbasierenden Matchern. Als Kontext
bezeichnen Sie die semantische Beziehung bzw. Hierarchie zwischen den
Attributen, beispielsweise in Graphstrukturen, welche es erlauben
Ähnlichkeitswerte über Kanten zu propagieren. Um die Matcher miteinander zu
kombinieren nutzen die Frameworks mindestens eine Matching Strategie. Durch die
Match-Strategie werden verglichene Datensatzepaare in die Mengen *Matches* und
*Non-Matches* klassifiziert.

Ein Großteil der Forschung in Entity Resolution konzentriert sich auf die
Qualität der Vergleichsergebnisse. Die von Köpcke & Rahm verglichenen Frameworks
konzentrieren sich hierbei alle darauf zwei statische Mengen zu miteinander
vergleichen. Bei großen Datenmengen kann dies durchaus mehrere Stunden dauern.
Daher gibt es in den letzten Jahre einige Ansätze und Frameworks, welche
MapReduce Algorithmen zum Skalieren nutzen [@KR:Parallel:13, @MAS:Graph:14].
Einen Ansatz die Laufzeit für Anwendungen mit Laufzeitanforderungen zu
optimieren präsentieren Whang et al. [@WMG:PayAsYouGo:13]. Anstatt eine
Übereinstimmungsmenge nach Abschluss eines Algorithmus zu liefern, zeigen Sie
Möglichkeiten partielle Ergebnisse während der Laufzeit des Algorithmus zu
erhalten. Dabei modifizieren Sie die Blockingalgorithmen so, dass zunächst die
wahrscheinlichsten Kandidaten miteinander verglichen werden. Dabei wird in
relativ kurzer Zeit ein Großteil der Duplikate gefunden.

Neben den statischen Verfahren gibt es zunehmend Bedarf an dynamischen
Verfahren. Dynamisch bedeutet hier, dass während der Laufzeit neue Datensätze
hinzugefügt werden können. Das Finden gleicher Entitäten erfolgt dabei auf
Anfrage, weshalb die gesamte Datenmenge vorab nicht bekannt ist. Beispielsweise
müssen Kreditauskunfteien auf Anfrage prüfen, ob ein Kunde kreditwürdig ist.
Dazu müssen die passenden Entitäten möglichst schnell gefunden werden, um eine
Entscheidung treffen zu können. Zudem ist es notwendig eine Historie der
unveränderten Anfragen aller Entität vorzuhalten, da diese Beweise über frühere
Anfragen liefern. Ramadan et al. [@RCLG:Dynamic:15] formulieren die
Problemstellung für dynamische ER-Verfahren folgendermaßen. Für jeden
Anfragedatensatz $q_j$ eines Anfragestroms $Q$ sollen alle Datensätze $M_{q_j}$
in einem Datensatz $R$ gefunden werden, welche dieselbe Entität wie $q_j$
beschreiben. $$M_{q_j} = \{r_i|r_i.eid = q_j.eid, r_i \in R\}, M_{q_j} \subseteq
R, q_j \in Q,$$ wobei $eid$ ein eindeutiger Identitiver einer Entität ist,
welcher so nicht existiert. Die Herausforderung für dynamische ER-Verfahren ist
weiter nach Ramadan et al. Indexing-Verfahren zu entwickeln, welche es erlauben
den Index dynamische zu erweitern und eine kleine Zahl qualitativer Ergebnisse
in nahe Echtzeit (Subsekundenbereich) zu liefern. Ein dynamisches ER System ist
ähnlich einer Suchmaschine, doch anstatt einer gewerteten Liste möglicher
Treffer, soll es alle gleichen Entitäten finden, welche zur Anfrage passen. Das
bedeutet insbesondere, dass die Anfrage die gleiche Datenstruktur haben muss,
wie die zu durchsuchende Datenquelle. Zudem kann eine Anfrage während der
Abfrage als neuer Datensatz aufgenommen werden. Erste Ergebnisse Entity
Resolution in nahe Echtzeit zu erreichen, präsentieren Christen & Gayler in
[@CG:Scalable:08], unter Verwendung von Inverted Indexing Techniken, welche
normalerweise bei der Websuche Anwendung finden. Die dynamischen Verfahren
werden in @sec:dyblocking behandelt.

## Anforderungen an eine ER-System für ESP {#sec:anf}

Das Event Stream Processing besteht aus drei Teilen[^5]:

* Event – Ein Ereignis irgendeines Vorkommnis, zu einer eindeutig definierten
  Zeit, welches durch eine Menge von Attributen aufgezeichnet wird.
* Stream – Ein Datenstrom ist ein konstanter Fluss oder stetiger Ansturm von
  Daten, die von einer beliebigen Anzahl verbundener Geräte zu einem Dienst
  fließen.
* Processing – Ist die Handlung, welche die Daten analysiert.

[^5]: https://www.sas.com/en_us/insights/articles/big-data/3-things-about-event-stream-processing.html

Für ein Entity Resolution Framework, das Teil eines Event Stream Processing
Systems ist, werden Ereignisse als Anfragen bezeichnet. Dabei besteht eine
Anfrage aus einem Datensatz, der strukturell identisch zu den Bestandsdaten ist.
Das Ergebnis einer Anfrage ist eine Menge von Datensätzen, die derselben Entität
entsprechen. Unabhängig von den Datenstrukturen verschiedener Domänen, muss ein
Entity Resolution Framework für ESP-Systeme dieselben drei Probleme lösen:

* **Niedrige Latenzen**. Der Zeitraum vom Stellen einer Anfrage bis zu deren
  Beantwortung sollte im Subsekundenbereich liegen. Die Gründe dafür sind
  vielfältig, beispielsweise bietet eine Produktsuche eine bessere
  Benutzererfahrung, je schneller die Daten verfügbar sind, weil Benutzer dazu
  geneigt sind einen Dienst nicht mehr zu nutzen, wenn die Benutzererfahrung
  durch zu lange Wartezeiten getrübt wird. Eine Kreditauskunft ist meist Teil
  eines größeren Prozesses, sodass deren Laufzeit die Benutzererfahrung von
  anderen Diensten beeinflussen kann, weshalb eine solche Auskunft seinen Kunden
  Garantien für die Laufzeit der Anfragen gibt, um sich gegenüber der Konkurrenz
  hervorzuheben. Hauptverantwortlich für niedrige Latenzen sind in einem Entity
  Resolution Framework die Blocking Verfahren. Während einer Anfrage dauert die
  Berechnung der Ähnlichkeitswerte zwischen Attributen des Anfragedatensatzes
  und durch Blocking gefilterter Datensatzkandidaten am längsten. Um niedrige
  Latenzen zu erreichen, muss das Blocking die zu durchsuchende Kandidatenmenge
  stark reduzieren und kann außerdem versuchen den Aufwand von
  Ähnlichkeitsvergleichen, beispielsweise durch Caching, zu reduzieren.
* **Datenmodifikation zur Laufzeit**. Der Datenbestand kann mit jedem Ereignis
  modifiziert werden, sodass ständig neue Entitäten hinzukommen oder modifiziert
  werden. Das Entfernen wird nur in äußersten Ausnahmen vorgenommen und findet
  deshalb keine weitere Beachtung. Sobald eine Enitität hinzugekommen ist bzw.
  modifiziert wurde, muss die Änderung sofort für die darauffolgenden Anfragen
  zum Abgleich auffindbar sein. Werden die Anfragen stets sequentiell
  bearbeitet, ist diese Anforderung realtiv einfach zu erfüllen. Da aber auch
  bei hoher Last die Latenzen niedrig bleiben müssen, kann ein Entity Resolution
  Framework die Bearbeitung auf mehrere Threads, Prozesse oder Knoten skalieren,
  um die Anfragen parallel zu bearbeiten. Erhält das System nacheinander mehrere
  Anfragen, die derselben Entität zuzuweisen sind, dann ist die
  Wahrscheinlichkeit hoch, dass diese parallel vorarbeitet werden. Damit
  allerdings die zweite Anfrage, die die erste als Teil ihrer Ergebnissemenge
  enthält, müssen diese, bei Nutzung von Paralellisierung, sequentialisiert
  werden, ohne dadurch einen Flaschenhals für das Gesamtsystem zu bilden.
* **Hohe Trefferrate**. Unter den Latzen darf naturlich nicht die Qualität des
  Ergebnisses leiden. Das Ziel ist es zu einer Entität möglichst alle im
  Datenbestand existierenden Datensätze zu finden und zurückzugeben. Dazu wird
  die Kandidatenmenge untersucht, die durch ein Blocking Verfahren gebildet
  wurde. Da diese Menge auch Datensätze enthält die nicht derselben Entität
  entsprichen, müssen diese nach der Ähnlichkeitsberechnung von einem
  Klassifikator in *Matches* und *Non-Matches* klassifiziert werden. Damit
  dieser möglichst wenige Missklassifikationen unternimmt, d.h. Matches als
  Non-Matches deutet bzw. umgekehrt, muss der Ähnlichkeitswert eine hohe
  Aussagekraft haben. Deshalb ist es wichtig für jedes Attribut eine geeignete
  Ähnlichkeitsfunktion zu finden, die dem Klassifikator erlaubt, auf den
  Attributsähnlichkeiten, möglichst eindeutige Entscheidungen zu treffen.

Die Problemstellungen *niedrige Latenzen*, *Datenmodifikation zur Laufzeit* und
*hohe Trefferraten* führt zu den drei Problembereichen *Blocking*,
*Ähnlichkeitsberechnung* und *Klassifikation*. Für diese drei Bereiche werden in
@sec:relatedwork ähnliche Arbeit vorgestellt. Anschließend werden die Ansätze in
@sec:analysis diskitiert und alternative eigene Verfahren entwickelt.

# Ähnliche Arbeiten {#sec:relatedwork}

## Blocking {#sec:blocking}

Blocking dient der Reduzierung der quadratischen Komplexität eines ER
Verfahrens. Im Folgenden werden Verfahren unterschieden, die entwickelt wurden,
um auf statischen oder dynamischen Datenquellen angewandt zu werden.

### Statisches Blocking {#sec:staticblocking}

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|\cdot|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}\cdot|A|\cdot(|A|-1)$ Vergleiche durchgeführt werden. In beiden
Fällen ist die Anzahl der Vergleiche quadratisch zur Eingabemenge
[@Kol:Effiziente:14]. In der Studie [@KTR:Evaluation:10] zeigen Köpcke et al.,
dass das kartesische Produkt für große Datenmengen nicht skaliert. Aus diesem
Grund reduzieren moderne Entity Resolution Frameworks den Suchraum auf die
wahrscheinlichsten Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden
zur Reduzierung des quadratischen Suchraum werden übergreifend als
Blockingmethoden bezeichnet. Neben Blocking werden auch Windowing- und Indexing
Verfahren eingesetzt. Während Blockingverfahren die Anzahl der notwendigen
Vergleiche drastisch reduzieren, indem Non-Matches ausgeschlossen werden,
besteht dennoch die Gefahr, dass fälschlicherweise tatsächliche Matches
ausgefiltert werden. Daher ist es notwendig die Güte des Blockingverfahrens zu
bestimmen. Dazu werden zwei Kennziffern erhoben. Zum einen die *Reduction
Ratio*, welche die Reduzierung der Vergleiche im Gegensatz zum Kartesischen
Produkt ausdrückt, sowie die *Pairs Completeness*, welche den Anteil der
tatsächlich ausgewählten Duplikate, die sich nach dem Blocking in der
Kandidatenmenge befinden, beschreibt. Eine detaillierte Beschreibung der
Komplexitätsmaße für Blocking wird in @sec:measurements vorgenommen.

Prinzipiell erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" voneinander einfinden. Zur
Durchführung der Gruppierung oder Sortierung müssen sog. Block- bzw.
Sortierschlüssel für jeden Datensatz erzeugt werden. Diese Schlüssel werden von
den Attributswerten oder einem Teil der Attributswerte abgeleitet und stellen
eine Signatur des Datensatzes dar. Eine beliebte Variante für Schlüssel sind
etwa phoenitische Enkodierung.

#### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee des Verfahrens ist eine Menge von Datensätzen in
disjunkte Partitionen (genannt Blöcke) zu teilen. Anschließend werden nur die
Datensätze des jeweiligen Blocks miteinander verglichen. Dazu wird jedem
Datensatz ein Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens
hängt daher maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und
Größe der Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
@fig:sb_blocking zeigt die Ausführung des Blockingverfahrens beispielhaft an
einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein Blockschlüssel
(hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses Schlüssels gruppiert.
Die Größe der einzelnen Blöcke bestimmt die Reduktion Ratio. Diese hängt
allerdings immer von der Datenquelle ab und kann daher nicht pauschalisiert
werden. Bei der Generierung der Blockschlüssel können fehlerhafte Werte
einzelner Attribute dazu führen, dass Duplikate in unterschiedlichen Blöcken
landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden Datensatz
mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert werden.
Dieser Ansatz nennt sich Multi-pass Blocking [@Kol:Effiziente:14]. Im Folgenden
werden mit Q-gram Indexing und Suffix Array Indexing zwei Verfahren diskutiert,
die ein unscharfes Matching der Schlüssel erlauben und dadurch etwa Tippfehler
auflösen können.

![Beispielhafte Standard Blocking Ausführung nach [@Kol:Effiziente:14]. Für
jeden Datensatz in $S$ wird ein Blockschlüssel $K$ erzeugt. Anhand dessen werden
Blöcke erzeugt und innerhalb der Blöcke werden Paare gebildet.
](images/standard_blocking.pdf){#fig:sb_blocking}

#### Q-gram Indexing

Das Q-gram Indexing basiert auf der Idee Datensätze unterschiedlicher aber
ähnlicher Blockschlüssel miteinander zu vergleichen. Ein Blockschlüssel wird
dazu in eine Liste $G$ von q-Grammen überführt. Ein q-Gram ist ein Substring der
Länge $q$ des ursprünglichen Blockschlüssels. Beispielsweise erzeugt $q=2$
angewendet auf den Blockschlüssel `banana` die Liste $G = {ba, an, na}$. Alle
Kombinationen der q-Gram Liste mit einer Mindestlänge $l=max(1,\floor{\#G \cdot
t})$ werden konkateniert und dienen als Schlüssel der Blöcke, wobei $t$ ein
Schwellwert zwischen 0 und 1 ist. Für $t=0.9$ werden die Sublisten ${(ba, an),
(ba, na), (an, na), (ba, an, na)}$ der Längen 2 und 3 gebildet. Dabei werden
Datensätze mehreren Blöcken zugewiesen. Dieses Verfahren kann als Alternative
zum Multi-pass Verfahren beim Standard Blocking genutzt werden. Ist $t=1$ wird
lediglich ein Blockschlüssel erzeugt, was dem Standard Blocking entspricht. Der
große Nachteil ist der hohe Aufwand bei der Berechnung aller möglichen
Sublisten. Ein Blockschlüssel mit $n$ Zeichen muss in $k=n-q+1$ q-Gramme zerlegt
werden. Insgesamt müssen dadurch $\sum_{i=max\{1,[k \cdot t]\}}^{k} {k \choose
i}$ Sublisten berechnet werden [@Chr:Survey:12].

#### Suffix Array Indexing

Das Suffix Array Indexing [@AO:Fast:05a] leitet, ähnlich wie Q-gram Indexing,
mehrere Schlüssel aus einem Blockschlüssel ab. Grundidee ist es alle Suffixe mit
einer Mindestlänge von $l$ zu bestimmen. Ein Datensatz mit Blockschlüssellänge
$n$ wird in $n-l+1$ Blöcke eingeordnet. Ist $n<l$ wird der Ausgangsschlüssel als
einziger Schlüssel verwendet. Durch die größere Menge an Kandidatenpaaren ist
i.Allg. die *Pair Completeness* höher (vgl. Multi-pass). Zudem ist der Aufwand
der Berechnung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer. Im
Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren jedoch
deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze
unnötigerweise mehrfach miteinander verglichen werden hoch. Deshalb werden aus
Blöcken, welche einen bestimmten Schwellwert überschreiten alle Datensätze
entfernt, die min. einen weiteren längeren Blockschüssel haben.

#### Sorted Neighborhood

Das Sorted Neighborhood Verfahren, ist ein Sortierverfahren, welches 1995 von
Hernández & Stolfo [@HS:Merge:95] zur Erkennung von Duplikaten in
Datenbanktabellen vorgestellt wurde. Es besteht aus drei Phasen. Zunächst
bekommt jeder Datensatz einen Sortierschlüssel zugewiesen. Dabei muss der
Sortierschlüssel nicht einzigartig sein. Um die Berechnung des Schlüssels gering
zu halten, soll dieser durch Verkettung von Attributen bzw. Teilen der Attribute
bestimmt werden. Attribute die vorne im Schlüssel stehen haben dadurch eine
höhere Priorität. In der zweiten Phase werden die Datensätze anhand des
Schlüssels sortiert. In der dritten Phase wird ein Fenster (engl. Window) über
die sortierten Datensätze geschoben und alle Datensätze innerhalb des Windows
werden miteinander verglichen. Dieses Verfahren eignet sich besonders gut zur
Erkennung von Duplikaten innerhalb einer Datenquelle. Sollen Duplikate in
mehreren Datenquellen gefunden werden, müssen die Einträge beim Sortieren
gemischt werden. Dadurch besteht allerdings die Gefahr, dass vorrangig Datensätze
einer Datenquelle miteinander verglichen werden. Der Vorteil gegenüber dem
Standard Blocking ist, dass die Anzahl der Vergleiche lediglich von der Größe
der Datenquelle und der gewählten Fenstergröße abhängen. Ein großer Nachteil
ist, dass Datensätze die sich in der ersten Stelle des Schlüssels unterscheiden,
weit voneinander entfernt sind und dadurch nicht als Matches identifiziert
werden. Um dennoch eine hohe Pairs Completeness zu erreichen, werden mehrere
Schlüssel pro Datensatz generiert und ein Fenster mit kleiner Größe über die
verschieden sortierten Listen geschoben. Dieses Verfahren entspricht im Grunde
dem Multi-pass Verfahren beim Standard Blocking.

Ein großes Problem bei der klassischen und der Multi-pass Variante des Sorted
Neighborhood Verfahrens ist, dass die zu wählende Fenstergröße $w$ größer als
die Anzahl der Datensätze mit dem am häufigsten vorkommenden Sortierschlüssel
sein muss, um eine gute Pair Completeness zu erreichen. Sei $n$ die Menge an
Datensätzen mit dem am häufigsten vorkommenden Schlüssel $k$ und $m$ die Menge
der Datensätze des darauffolgenden Schlüssels $k+1$, dann ist $w=n+m$. Nur
dadurch kann sichergestellt werden, dass alle Datensätze aus $n$ mit den "nahen"
Datensätzen aus $m$ verglichen werden. Da Sortierschlüssel für gewöhnlich nicht
gleichverteilt sind, gibt es meist wenige große und viele kleine Mengen an
Datensätzen mit dem gleichen Sortierschlüssel. Dadurch werden Datensätze mit
seltenen Sortierschlüsseln unnötig oft mit "weit" entfernten Datensätzen
verglichen. Zudem dominiert der am häufigsten vorkommenden Schlüssel, genauso
wie beim Standard Blocking, die Ausführungszeit des Algorithmus.

In [@DN:comparison:09] schlagen Draisbach & Naumann eine optimierte Variante des
Sorted Neighborhood Verfahrens vor. Dabei zeigen Sie, dass Standard Blocking und
Sorted Neighborhood zwei extreme von Überlappungen bei Partitionen sind. Gegeben
sind zwei Partitionen $P_1$ und $P_2$, dann ist die Überlappung $U_{P_1,P_2} =
P_1 \cap P_2$ und $u = |U_{P_1,P_2}|$. Ihre Idee ist es diese Überlappung zu
optimieren. Dabei soll die Überlappung groß genug sein, um tatsächliche Matches
zu finden, aber gering genug, um die Menge der Vergleiche zu reduzieren.
Zunächst wird wie beim klassischen Verfahren sortiert. Danach werden angrenzende
Datensätze in disjunkte Partitionen zerlegt und schließlich wird ein
Überlappungsfaktor $u$ gewählt. Innerhalb jedes Blockes wird analog zum Standard
Blocking jeder Datensatz mit jedem anderen verglichen. Innerhalb des
Overlap-Window $w=u+1$, wird jeweils das erste Element mit allen anderen
verglichen. Ist $w=0$ entspricht das Verfahren dem Standard Blocking und hat
jede Partition nur ein Element entspricht es der Sorted Neighborhood Methode. Um
zu vermeiden, dass eine Partition dominiert, können größere Partitionen in
Subpartitionen geteilt werden.

#### Canopy Clustering

Cohen & Richman [@CR:Learning:02] schlagen ein Clustering-Verfahren zum Blocken,
auf Basis von Canopies, vor. Die Idee von Canopy Clustering ist es, Datensätze
anhand einer einfachen Vergleichsmetrik in überlappende Cluster (=Canopies) zu
partionieren. Zur Generierung wird eine Kandidatenliste gebildet, welche initial
als allen Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen
Clusters gewählte und alle Datensätze innerhalb des Mindestabstandes $d_1$
zugewiesen. Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren
Mindestabstandes $d_2 < d_1$ aus der Kandidatenliste entfernt. Dieser
Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die *Pair
Completeness* hängt hierbei stark der gewählten Abstandsfunktion ab.
Anschließend wird werden alle Datensätze eines Cluster miteinander verglichen.

### Dynamisches Blocking {#sec:dyblocking}

Für die Dupliaktserkennung in einer Datenquelle $A$, sind bei einer Anfrage
$|A|$ Vergleiche notwendig. Da dies zu ungewollt hohen Latenzen führen würde,
werden auch im dynamischen Fall Blocking Verfahren eingesetzt. Besonders wichtig
für dynamische Verfahren ist, dass Anfragen möglichst schnell beantwortet
werden. Dies wird erreicht, indem Verfahren einen Teil der Vergleiche
vorausberechnen. Des Weiteren sollen die Anwortzeiten möglichst gleich sein, um
zu verhindern das manche Anfragen ein vielfaches länger benötigen. Das bedeutet,
dass die Kandidatenpaare, die pro Anfrage in Frage kommen, möglichst gleich sein
müssen.

#### DySimII {#sec:dysimII}

![Ein DySimII-Index, welcher aus der Tabelle links erzeugt worden ist. Die
Beispieldatensätze enthalten das Namensattribut und eine Double-Metaphone
Enkodierung, welche als Blockingschlüssel genutzt wird. **RI** ist der Record
Identifier Index, **BI** der Block Index und **SI** der Similarity Index. Das
Beispiel ist aus [@RCL.EA:Dynamic:13] entnommen.
](images/dysim_example.png){#fig:dysim_example}

Der Dynamic Similarity-Aware Inverted Index [@RCL.EA:Dynamic:13] ist die
dynamische Erweiterung (Version 2) des Similarity-Aware Inverted Index von
Christen & Gayler [@CG:Scalable:08], deshalb abgekürzt DySimII. Die
Funktionalitäten beider Verfahren ist identisch, bis auf die Ausnahme, dass der
DySimII es erlaubt den Index während der Laufzeit zu erweitern. Dabei ist die
Grundidee die benötigten Ähnlichkeiten vorauszuberechnen, um während der
Laufzeit diese nur nachschlagen zu müssen.

Der Index besteht aus drei Teilen. Dem **Record Identifier Index (RI)**, welcher
alle Attribute speichert und diese ihren Datensätzen zuordnet, dem **Block Index
(BI)**, welcher Attribute anhand einer Enkodierungsfunktion gruppiert und
zuletzt dem **Similarity Index (SI)**, welcher dieselben Schlüssel wie der
Record Identifier Index verwendet und die Ähnlichkeiten der Attribute im
gleichen Block hält. @fig:dysim_example zeigt ein Beispiel eines DySimII Index.
Im RI wurden die Datensatzidentifier von Tony (r1, r3) und Cathrine (r2, r6) als
Attributsübereinstimmung gruppiert. Anschließend wurden im BI über die
Double-Metaphone Enkodierung, welche einen identischen String für
gleich klingende Wörter erzeugt, ähnliche Schreibweisen von Tony und Cathrine
zusammengeführt, was dem Standard Blocking entspricht. Im SI wurden die
Ähnlichkeiten von (Tony, Tonia und Tonya) bzw. (Cathrine, Kathryn), welche sich
in einen gemeinsamen Block befinden, untereinander mit ihren berechneten
Ähnlichkeiten verknüpft. Dabei wird für jedes Attribut eines Datensatzes ein
eigenes Tripel RI, BI und SI genutzt, um zu verhindern, dass sich Werte
unterschiedlicher Attribute vermischen.

```texalgo
#alg:dysim_insert DySimII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item Encoding functions: $E_i,i=1 \cdots n$
  \item Similarity functions: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $RI, BI, SI$
  \end{itemize}
}
\Statex
\State Initialize $RI = \{\}$, $BI = \{\}$, $SI = \{\}$
\For{records $r \in D$}
  \For{attributes $a = 1 \cdots n$}\label{alg:ds:1}
    \State Append $r.id$ into $RI[r.a]$\label{alg:ds:2}
    \If{$r.a \notin SI$}\label{alg:ds:2.1}
      \State $c = E_a(r.a)$\label{alg:ds:3}
      \State Append $r.a$ to $BI[c]$\label{alg:ds:4}
      \For{attribute $v \in BI[c]$}\label{alg:ds:5}
        \State $sim = S_a(r.a, v)$\label{alg:ds:6}
        \State Append $(r.a, sim)$ to $SI[v]$\label{alg:ds:7}
        \State Append $(v, sim)$ to $SI[r.a]$\label{alg:ds:8}
      \EndFor\label{alg:ds:9}
    \EndIf
  \EndFor
\EndFor
\State return $RI, BI, SI$

#alg:dysim_query DySimII - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item Encoding functions: $E_i,i=1 \cdots n$
  \item Similarity functions: $S_i,i=1 \cdots n$
  \item Indexes: $RI, BI, SI$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex
\State Initialize dictionary $M = \{\}$
\For{attributes $a = 1 \cdots n$}\label{alg:dq:1}
  \If{$q.a \notin RI$}\label{alg:dq:2}
    \State $Insert(q.a, q.id, E_a, S_a)$\label{alg:dq:3}
  \Else
    \State Append $q.id$ to $RI[q.a]$\label{alg:dq:4}
  \EndIf
  \For{$r.id \in RI[q.a]$}\label{alg:dq:5}
    \State $M[r.id] = M[r.id] + 1.0$\label{alg:dq:6}
  \EndFor
  \For{$(r.a, sim) \in SI[q.a]$}\label{alg:dq:7}
    \For{$r.id \in RI[r.a]$}\label{alg:dq:8}
      \State $M[r.id] = M[r.id] + sim$\label{alg:dq:9}
    \EndFor
  \EndFor
\EndFor
\State Sort $M$ according to similarities
\State return $M$
```

Das Verfahren unterscheidet zwei Phasen. Die Bauphase (engl. Build-Phase), in
welcher der Index, aus einem initialen Datenbestand, erzeugt wird (Algorithmus
\ref{alg:dysim_insert}) und die Anfragephase (engl. Query-Phase), welche
Anfragen aus einem Datenstrom beantwortet (Algorithmus \ref{alg:dysim_query}).

**Build Phase**. Das Einfügen von Datensätzen läuft nach folgendem Schema ab.
Die Einfügeprozudur wird für jedes Attribut eines Datensatzes $r$ wiederholt
(Zeile \ref{alg:ds:1}). Zuerst wird der Identifier $r.id$ unter dem aktuellen
Attribut $r.a$ in den RI eingefügt (Zeile \ref{alg:ds:2}). Anschließend wird
geprüft, ob sich $r.a$ bereits im SI befindet (Zeile \ref{alg:ds:2.1}). Ist dies
der Fall gibt es für dieses Attribut nichts weiter zu tun. Andernfalls wird für
das Attribut $r.a$ über die Enkodierungsfunktion $E_a$ ein Blockschlüssel
bestimmt (Zeile \ref{alg:ds:3}). Anhand dessen wird das Attribut in den
entsprechenden Block im Block Index eingefügt (Zeile \ref{alg:ds:4}). Beinhaltet
der Block mehr als ein Attribut, wird zu allen bereits im Block befindlichen
Attributen die Ähnlichkeit $sim$ bestimmt (Zeile \ref{alg:ds:6}). Diese
Ähnlichkeit wird zwei Mal in den Similarity Index eingefügt. Zunächst das Tupel
$(r.a, sim)$ unter dem verglichenen Attribute $v$ (Zeile \ref{alg:ds:7}) und
anschließend das Tupel $(v, sim)$ unter dem Attribut des einzufügenden
Datensatzes $r.a$ (Zeile \ref{alg:ds:8}). Die Schritte der Zeilen
\ref{alg:ds:2}-\ref{alg:ds:9}, werden für jedes Attribut jedes Datensatzes
wiederholt. Nachdem alle Datensätze in $RI$, $BI$ und $SI$ eingefügt wurden,
werden diese drei Indicies zurückgegeben.

**Query Phase**. Zur Beantwortung einer Anfrage werden ebenfalls alle Attribute
seperate betrachtet (Zeile \ref{alg:dq:1}). Zuerst wird die Ergebnisliste $M$
initalisiert. Für jedes Attribut wird geprüft, ob dieses bereits im RI existiert
(Zeile \ref{alg:dq:2}). Ist dies der Fall wird lediglich der Datensatzidentifier
$q.id$ in den RI unter dem Attribut $q.a$ eingefügt (Zeile \ref{alg:dq:4}).
Andernfalls wird das Attribute $q.a$ nach dem Verfahren aus Algorithmus
\ref{alg:dysim_insert} zu den Indicies hinzugefügt (Zeile \ref{alg:dq:3}).
Dadurch werden implizit die Ähnlichkeiten zu $q.a$ zu Attributen im gemeinsamen
Block berechnet. Anschließend werden aus dem RI alle Identifier ausgelesen
(Zeile \ref{alg:dq:5}), welche das selbe Attribut wie $q$ besitzen und werden in
die Ergebnisliste mit dem Ähnlichkeitswert 1 aufgenommen bzw. wenn dort schon
ein Eintrag für einen Datensatz vorhanden ist, wird die Ähnlichkeits mit 1
aufsummiert (Zeile \ref{alg:dq:6}). Danach wird zu $q.a$ im SI die Attribute
des selben Blocks und deren Ähnlichkeit $sim$ nachgeschlagen (Zeile
\ref{alg:dq:7}). Für jedes Attribut, werden aus dem RI, die Datensätze mit dem
Attribut $r.a$ selektiert (Zeile \ref{alg:dq:7}) und in die Ergebnisliste mit
ihrer Ähnlichkeit, gegenüber $q.a$, $sim$ aufgenommen bzw. wird $sim$ zur
Ähnlichkeit eines bestehenden Eintrags addiert.

Im Gegensatz zum Standard Blocking können Anfragen deutlich schneller
beantwortet werden, da im Optimalfall keine Ähnlichkeitsberechnung stattfinden
muss und lediglich Werte nachgeschlagen werden. Auf der negativen Seite steht
hingegen der deutlich erhöhte Speicherbedarf, welcher durch das Halten der
Ähnlichkeitswerte zurückzuführen ist.

#### Similarity-Aware Index with Local Sensitive Hashing (LSH)

Dieses Verfahren ist eine Erweiterung des DySimII durch LSH, welches von Li et
al. [@LLRo:Two:13] vorgestellt wurde. Die hier genutzte Variante des Local
Sensitive Hashing nutzt das Minhash Verfahren. Minhashing ist eine effiziente
Abschätzung der Überlappung zweier Mengen bekannt als Jaccard-Ähnlichkeit.
Mittels des Minhash-Algorithmus ist es möglich für jeden Datensatz $n$
Signaturen der Länge $k$ zu generieren. Dazu werden $n$ verschiedene zufällig
gewählte Hashfunktionen genutzt. Um die Wahrscheinlichkeit zu erhöhen, dass nur
gleiche Paar dieselbe Signatur haben wird eine Technik namens Banding genutzt.
Dazu werden $l$ Signaturen zu einem Band zusammengefügt und damit verundet.
Mehrere Bänder sind logisch gesehen eine Veroderung. Auch dieses Verfahren teilt
sich in Bau- und Anfragephase.

**Build Phase**. Beim Erzeugen des Index werden zunächst die Minhash Signaturen
erzeugt und zu Bändern verundet. Anschließend werden die Datensatzidenfier wird
mit den erzeugten Bändern verknüpft. Dazu wird ein Index erstellt, welcher als
Schlüssel zunächst die verschieden Bänder hat. Innerhalb der Bänder gibt es
weitere Subindicies, welche als Schlüssel die Minhash Signaturen haben. Den
jeweiligen Signaturen innerhalb der Bänder wird der Datensatzidentifier
zugewiesen. Dadurch sind gleiche Signaturen durch die Bänder getrennt, was die
Wahrscheinlichkeit erhöht, dass unähnliche Datensatze eine gemeinsame Signatur
im Index haben. Der LSH Index ersetzt dadurch den Record Index. Die Schritte zum
Einfügen in den Block Index bzw. den Similarity Index sind analog zum DySimII.

**Query Phase**. Für die Beantwortung einer Anfrage werden zunächst für den
neuen Datensatz die Minhash Signaturen und Bänder erzeugt und mit
Datensatzidentifier in den LSH Index eingefügt. Danach werden die
Datensatzidenfier mit gleichen Signaturen in den gleichen Bändern als
Kandidatenmenge ausgelesen. Nun müssen die Attribute der Kandidaten aus einer
Datenquelle geladen werden. Mit diesen Attributen können aus dem Similarity
Index die Ähnlichkeitswerte jedes Kandidaten bestimmt werden. Die Kandidaten,
welche aus dem LSH Index erhalten wurden haben allerdings nicht zwingend
Attribute in denselben Blöcken im Block Index wie der Anfragedatensatz.
Deshalb können zu einigen Attributen keine vorberechneten Ähnlichkeiten aus dem
Similarity Index bezogen werden. Da das Berechnen zur Laufzeit zu lange dauert,
werden diese mit dem Ähnlichkeitswert 0 miteinberechnet. Dies mindert zwar die
Genauigkeit etwas sorgt dennoch für gute Latenzen.

Im Gegensatz zum DySimII ist die Berechnung des Index aufwendiger, da für jeden
Datensatz die Minhash Signaturen und Bänder berechnet werden müssen. Allerdings
ist die Kandidatenmenge potentielle deutlich geringer als beim DySimII, wodurch
die Anfragen schneller beantwortet werden.

#### DySNI

Das DySNI Verfahren von Ramadan et al. [@RCLG:Dynamic:15] ist eine dynamische
Umsetzung des Sorted Neighborhood Verfahren aus dem statischen Blocking. Anstatt
eines Arrays wird eine Baumstruktur verwendet, um Datasätze möglichst effizient
zu selektieren. Der gewählte Baum ist ein BraidedTree (BRT), welcher eine
Erweiterung eines balancierter binären AVL-Baums ist. Dieser unterscheidet
sich, indem zusätzlich innerhalb des Baumes jeder Knoten jeweils einen Verweis
auf seinen Vorgänger und seinen Nachfolger hat. Die Sortierung erfolgt
alphabetisch nach einem gewählten Sortierschlüssel. Ein Knoten besteht dabei aus
einem Sortierschlüsselwert (Sorting Key Value. kurz: SKV) und einer Liste an von
Datensatzes mit diesem SKV.

**Build Phase**. Beim Einfügen eines neuen Datensatzes wird zunächst dessen SKV
erzeugt. Wenn der SKV noch nicht im BRT-Baum vorhanden ist, wird ein neuer
Knoten erzeugt und der Datensatzidentifier angehängt. Ist der Knoten bereits
vorhanden, wird lediglich der Datensatzidentifier zum existierenden Knoten
hinzugefügt. Zusätzlich wird der Datensatz in einen Inverted Index $D$
eingefügt, um ihn zum Attributsvergleich mit anderen Datensätzen schnell
selektieren zu können.

**Query Phase**. Zunächst wird der Anfragedatensatz, nach dem Vorgehen aus der
Build Phase, eingefügt. Der Knoten in welchen der Anfragedatensatz eingefügt
wurde, heißt Anfrageknoten $N_q$. Ausgehend von $N_q$ wird ein Fenster über die
benachbarten Knoten gespannt. Alle Datensätze, welche in Knoten innerhalb des
Fensters gespeichert sind, werden als Kandidatenmenge $C$ selektiert. Aus $D$
werden dann für jeden Kandidaten seine Attribute geholt und anschließend in
einem Paarvergleiche mit dem Anfragedatensatz die Ähnlichkeit ermittelt. Für die
Erzeugung des Fensters werden vier Methoden vorgestellt, welche sich an
Varianten des statischen Sorted Neighborhood Verfahrens orientieren.

* **Fixed Window Size (DySNI-f)** ist das einfachste Verfahren, bei welchem das
  Fenster um einen festen Wert $w$ in Vorgänger- und Nachfolgerichtung
  aufgespannt wird.
* **Candidates-Based Adaptive Window (DySNI-c)** erweitert das Fenster
  abwechselnd in Vorgänger- und Nachfolgerichtung, solange bis eine
  Mindestanzahl an Kandidaten gefunden wurde.
* **Similarity-Based Adaptive Window (DySNI-s)** nutzt die Ähnlichkeit
  zwischen SKVs. Dabei wird ein Fenster in eine Richtung solange erweitert bis
  die Ähnlichkeit zwischen dem SKV von $N_q$ und dem nächsten Vorgänger bzw.
  Nachfolger eine Mindestähnlichkeit $\Delta$ unterschreitet.
* **Duplicate-Based Adaptive Window (DySNI-d)** erweitert das Fenster aus Basis
  gefundener Matches in beide Richtungen unabhängig. Dabei wird das Fenster um
  jeweils einen Knoten erweitert und zwischen dem Anfragedatensatz und den
  Datensätzen des neuen Knoten der Ähnlichkeitswert ermittelt, sowie
  klassifiziert, ob es sich um ein Match oder Non-Match handelt. Sinkt der
  Anteil an gefunden Matches unter eine Schranke $\delta$, wird das Fenster in
  diese Richtung nicht weiter vergrößert.

Damit Ähnlichkeiten zwischen den Datensätzen nicht jedes Mal neu berechnet
werden müssen, wird je nach gewählter Fensterberechnung, die Ähnlichkeit der
SKVs zu berechnen und in den beteiligten Knoten abzuspeichern. Dadurch wird
allerdings die Auswahl an SKVs auf Konkatenation von Attributen beschränkt.
Attribute, die nicht im SKV genutzt wurden, müssen bei diesem Verfahren trotzdem
jedes Mal neu berechnet werden. Des Weiteren ist auch dieses Verfahren sensitiv
auf Fehler am Anfang des SKV. Um dies zu korrigieren wird, ähnlich zum
Multi-pass Verfahren des Sorted Neighborhood Verfahren, vorgeschlagen mehrere
BRT-Bäume mit unterschiedlichen SKVs zu erstellen.

In ihrer Auswertung zeigen die Ramadan et al., dass das *DySNI-d* Verfahren im
BRT-Baum nicht funktioniert, weil ein Großteil der Duplikate im Anfrageknoten
$N_q$ landet und damit eine Erweiterung des Fensters nicht zustande kommt. Die
besten Recall Ergenisse wurden mit dem *DySNI-s* Verfahren erreicht, da der Baum
nach den SKVs sortiert wurde und dieses Fenster sich am besten aufspannt. Die
beiden anderen Verfahren *DySNI-f* und *DySNI-c* erzielen, ebenfalls gute
Ergebnisse. Zusätzlich haben die Verfahren den Vorteil, dass sich das Fenster
und damit die Anzahl der Kandidaten gut kontrollieren lässt und dadurch auch die
Latenzen.

### Blocking Schema

Die Qualität aller Verfahren, ob statisch oder dynamisch, hängt maßgeblich von
der Auswahl der richtigen Blockschlüssel bzw. Sortierschlüssel ab. Ein Verbund
aus mindestens einem Blockschlüssel für einen Entitätstypen nennt man Blocking
Schema. Wie genau diese Schemata auszuwählen sind, wird von vielen Blocking
Verfahren offen gelassen. Die meisten Verfahren schränken jedoch ein, dass zu
einem Datensatz nur ein Blockschlüssel oder Sortierschlüssel erzeugt werden
darf. Bei der Verwendung von Multi-pass Ansätzen werden dementsprechend
verschiedene Schemata gefordert. Ein adäquates Schema zu finden ist oft, auch
von Domainexperten, nur durch ausprobieren herauszufinden. Oft genutzt werden
phonetische Enkodierung und Konkatenation von Attributen. Weitere Beispiele sind
Q-Gramme oder Suffixe aus den vorgestellten statischen Verfahren.

#### DNF-Blocking Schema {#sec:blk_scheme}

Um die Probleme des manuellen Auswählens von Block- und Sortierschlüsseln zu
umgehen schlagen Kejriwal & Miranker [@KM:Unsupervised:13] ein Verfahren vor,
welches ein Blocking Schema in disjunktiver Normalform erzeugt. Dieses Schema
besteht aus den folgenden vier Komponenten.

**Indizierungsfunktion**. Das kleinste Element eines Blocking Schema ist eine
*Indizierungsfunktion* $h_i(x_t)$. Diese akzeptiert einen Attributswert $x_t$
eines Datensatzes und erzeugt eine Menge $Y$, welche 0 oder mehr Blockschlüssel
(engl. *blocking key value*, kurz BKV) beinhaltet. Ein BKV identifiziert einen
Block, welchem ein Datensatz zugeordnet wird. Ein Beispiel einer
Indizierungsfunktion ist `Tokens`. Mit der Funktion Tokens wird ein
Eingabestring, beispielsweise 'Marios Pizza', in eine Menge von Token mittels
eines Trennzeichens getrennt, beispielsweise durch eine Leerzeichen in $Y =
\{$'Marios', 'Pizza'$\}$.

**General Blocking Predicate**. Das allgemeine Blockingprädikat (engl. *general
blocking predicate*) $p_i(x_{t_1}, x_{t_2})$ nimmt zwei Attribute
unterschiedlicher Datensatze $t_1$ und $t_2$ und nutzt die i^te^
Indizierungsfunktion, um zwei Mengen von BKV $Y_1$ und $Y_2$ zu erzeugen. Das
Prädikat ist wahr, wenn beiden Mengen eine gemeinsame Schnittmenge haben $Y_1
\cup Y_2 \neq \empty$. Angenommen die i^te^ Indizierungsfunktion ist `Tokens`,
dann ist das zugehörige Prädikat `EnthältGemeinsamenToken`, welches Wahr ist,
wenn zwei Attribute mindestens einen gemeinsamem Token haben. Beispielsweise
$p_{egt}('Marios\ Pizza', 'Tonys\ Pizza') = Wahr$, weil $Y_1 = \{$'Marios',
'Pizza'$\}$ und $Y_2 = \{$'Tonys', 'Pizza'$\}$ woraus folgt das $Y_1 \cup Y_2 =
\{$'Pizza'$\}$ und damit ist das Prädikat erfüllt.

**Specific Blocking Predicate**. Das spezifische Blockingprädikat (engl.
*specific blocking predicate*) ist ein Paar $(p_i, f)$, dass ein allgemeines
Blockingprädikat $p_i$ mit einem Attribute $f$ verbindet. Dazu nimmt das
spezifische Blockingprädikat zwei Datensätze $t_1$ und $t_2$ und wendet $p_i$
auf die entsprechenden Attribute $f_1$ und $f_2$ der Datensätze an. Ein solches
Paar ist beispielsweise (`EnthältGemeinsamenToken`, `PLZ`). Dieses spezifische
Predikat ist wahr, wenn die Postleitzahl zweier Datensätze einen gemeinsamem Token
enthält.

**DNF Blocking Schema**. Das DNF Blocking Schema $f_P$ ist eine Funktion, welche
in der Disjunktiven Normalform ohne Negation durch eine Menge von $P$ Ausdrücken
erzeugt wird. Jeder Ausdruck in $f_P$ besteht aus mindestens einem spezifischen
Blockingpredikat, beispielsweise (`EnthältGemeinsamenToken`, `Name`) $\lor$
(`ExakteÜbereinstimmung`, `Stadt`). Mehrere spezifische Blockingpredikat in
einem Ausdruck werden durch eine Konjunktion verbunden. Das DNF Blocking Schema
ist dementsprechend Wahr, wenn einer seiner Ausdrücke Wahr ist. Ist ein Blocking
Schema für zwei Datensätze Wahr, haben beide mindestens einen gemeinsamen
Blockschlüsselwert. Für den Blockschlüssel entspricht die Konjunktion eines
Ausdrucks dem Konkatenieren von Strings. Gegeben sei folgendes Blocking Schema
$f_P =$ (`ExakteÜbereinstimmung`, `Name`) $\land$ (`ExakteÜbereinstimmung`,
`Stadt`) und der Datensatz $r =$ ('Peter', 'Frankfurt'). Daraus erzeugt $f_P$
den Blockschlüssel 'PeterFrankfurt'. Die Disjunktion der Ausdrücke kann bei
vielen Verfahren als Multi-pass Ansatz implementiert werden. Des Weiteren ist zu
beachten, dass das Blocking Schema potentiell mehrere Schlüssel pro Datensatz
erzeugt.

#### Lernen des DNF-Blocking Schema

Bevor das Verfahren von Kejriwal & Miranker [@KM:Unsupervised:13] automatisiert
ein Blocking Schema bestimmen kann, wird angenommen, dass eine Menge von
bekannten Matches und Non-Matches vorhanden ist. Der erste Schritt zum Lernen
eines DNF-Blocking Schema ist, eine Liste an spezifischen Blockingprädikaten zu
benennen. Beispielsweise (`EnthältGemeinsamenToken`, `name`),
(`ExakteÜbereinstimmung`, `stadt`) für Stringattribute und (`Erste3Ziffern`,
`Postleitzahl`) für ein nummerisches Attribut. Anschließend wird für jedes Paar,
der Matches bzw. Non-Matches, ein boolscher Featurevektor pro spezifischem
Blockingprädikat erzeugt. Ein Wert ist Wahr, wenn ein Paar das Prädikat erfüllt.
Dabei wird die Menge positiver Vektoren mit $P_f$ und negativer Vektoren mit
$N_f$ bezeichnet.

```tex
\begin{figure}
\centering
\begin{forest}
  for tree={
    grow=north,
    font=\itshape,
    if n children=0 {
      tier=terminal,
    }{},
  }
  [(EnthältGemeinsamenToken\, name) $\vee$ (ExakteÜbereinstimmung\, stadt) $\vee$ (Erste3Ziffern\, plz), calign=edge midpoint
    [(EnthältGemeinsamenToken\, name) $\vee$\\(ExakteÜbereinstimmung\, stadt), align=left, calign=first
        [(EnthältGemeinsamenToken\, name)]
        [(ExakteÜbereinstimmung\, stadt)]
    ]
    [(Erste3Ziffern\, plz)]
  ]
\end{forest}
\caption{Konjunktion der drei Ausdrücke (\texttt{EnthältGemeinsamenToken},
\texttt{name}), (\texttt{ExakteÜbereinstimmung}, \texttt{stadt}) und
(\texttt{Erste3Ziffern}, \texttt{plz}) zu einem zweistelligen und dreistelligen
Ausdruck.}\label{fig:dnf_conjunktion}
\end{figure}
```

Anschließend werden die Ausdrücke des Blocking Schema erzeugt. Da exponentiell
viele Ausdrücke erzeugt werden können, muss der Anwender die Tiefe, d.h. Anzahl
der Prädikate pro Ausdruck, beschränken. @fig:dnf_conjunktion zeigt wie aus den
Einzelausdrücken (`EnthältGemeinsamenToken`, `name`), (`ExakteÜbereinstimmung`,
`stadt`) und (`Erste3Ziffern`, `plz`) ein zweistelliger und ein dreistelliger
Ausdruck erzeugt werden. Zwei Ausdrücke $a_1$ und $a_2$ mit ihren
Featurevektoren $P_{f,a_1}, N_{f,a_1}$ und $P_{f,a_2}, N_{f,a_2}$ werden zu
einem neuen Ausdruck $a_{1-2}$ konjugiert, indem die Vektoren verundet werden
$P_{f,a_1-2} = P_{f,a_1} \land P_{f,a_2}$, respektive $N_{f,a_{1-2}} = N_{f,a_1}
\land N_{f,a_2}$. Dadurch müssen die teuren Prädikatsoperationen lediglich auf
die einstelligen Ausdrücken anwendet werden. Danach wird die Qualität der
einzelnen Ausdrücke bewertet. Dazu nutzen Kejriwal & Miranker die Fisher-Score.
Die Idee der Fisher-Score, nach [@GLH:Generalized:12], ist, eine Untermenge von
Features (hier: Ausdrücke) zu finden, sodass die Datenpunkte der Klassen (hier:
Matches und Non-Matches) möglichst weit voneinander entfernt und gleichzeitig
die Datenpunkte innerhalb der Klasse möglichst nahe zusammen sind. Die Formel
zur Berechnung der Fisher-Score des i^ten^ Ausdrucks sieht folgendermaßen aus
$$\rho_i = \frac{|P_{f,i}|(\mu_{p,i} - \mu_i)^2 + |N_{f,i}|(u_{\mu,i} -
\mu_i)^2}{|P_{f,i}|\sigma^2_{p,i} + |N_{f,i}|\sigma^2_{n,i}},$$ dabei ist
$\mu_{p,i}$ bzw. $\mu_{n,i}$ der Anteil der wahren Werte in $P_{f,i}$ und
$N_{f,i}$. Weiterhin ist $\mu_i$ der Anteil wahrer Werte in $P_{f,i}$ und
$N_{f,i}$ zusammen und $\sigma$ ist die positive bzw. negative Varianz.

```{.texalgo #alg:dnfbs caption="DisjunctiveBlockingScheme()"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Set of positively labeled feature vectors $P_f$
  \item List of conjuctive Terms $T$
  \item Maximum positive pairs that may be left uncovered $\epsilon$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Disjunctive Blocking Scheme $DBS$
  \end{itemize}
}
\Statex
\State Initialize set $DBS$ to be empty
\State Initialize vector $P_{dbs}$ as NULL vector
\For{term $t \in T$}
  \State Calculate the Fisher-Score\label{alg:dbs:1}
\EndFor
\State Sort $T$ according to scores\label{alg:dbs:2}
\While{more than $\epsilon$ Matches in $P_{DBS}$ are uncovered}\label{alg:dbs:3}
  \State Let $i$ be feature in $T$ with highest Fisher-Score
  \State Remove $i$ from $T$
  \If{$P_f,i \lor P_{dbs}$ covers at least 1 new pair}
    \State Add $i$ to $DBS$
    \State $P_{dbs} = P_{f,i} \lor P_{DBS}$
  \EndIf
\EndWhile\label{alg:dbs:4}
\State Return disjunction of Terms $DBS$
```

Algorithmus \ref{alg:dnfbs} beschreibt vereinfacht die Auswahl des DNF Blocking
Schema. Dabei wurde ein Teil des Algorithmuses weggelassen, der anhand der
negatvien Featurevektoren schlechte Ausdrücke filtert. Zunächst werden die
Ausdrück anhand der Fisher-Score bewertet (Zeile \ref{alg:dbs:1}). Anschließend
wird die Liste der Ausdrücke $T$ nach ihrer Fisher-Score sortiert (Zeile
\ref{alg:dbs:2}). Das zusammenfügen des Disjunktiven Blocking Schema erfolgt
durch hinzugefügen des jeweils besten Ausdrucks $i \in T$, solange ein Ausdruck
mindestens ein weiteres Match erfasst (Zeilen \ref{alg:dbs:3}-\ref{alg:dbs:4}).
Dazu werden die positiven Featurevektoren des Ausdrucks $P_{f,i}$ und der
$P_{DBS}$ verodert. Dies wird wiederholt, bis ein Minimum an Matches noch nicht
erfasst wurde oder alle Ausdrücke verarbeitet sind.

## Ähnlichkeitsmaße {#sec:similarity}

In @sec:blocking wurde beschrieben wie Kandidaten für einen Vergleich gruppiert
und selektiert werden. Über Ähnlichkeitsmaße (engl. similarity measures) wird
die Ähnlichkeit zweier Datensätze bestimmt. Genauer wird die Ähnlichkeit der
einzelnen Attribute bestimmt, aus welcher sich die Gesamtähnlichkeit der
Datensätze bestimmen lässt. Die meisten Fehler, welche zu unterschiedlichen
Datensätzen führen sind typographische Variationen von Strings. Weshalb sich
entsprechend viele Ansätze für den Vergleich von Stringattributen finden.
Attributsähnlichkeiten werden nach Elmagarmid et al. [@EIV:Duplicate:07] in vier
Kategorien geordnet:

* zeichenbasierend
* tokenbasierend
* phonetisch
* nummerisch

zusätzlich werden noch die kernelbasierend Methoden betrachtet.

#### Zeichenbasierende Ähnlichkeit

Wie sich die Ähnlichkeit von Strings bestimmen lässt, wird seit den 60er Jahren
[@Lev:Binary:66] intensiv erforscht. Die Stärke von zeichenbasierten
Ähnlichkeiten sind das Erkennen von typografischen Fehlern.

Der älteste und wohl auch bekannteste Algorithmus ist die Levenshtein Distanz
[@Lev:Binary:66]. Die Levenshtein Distanz ist eine **Editierdistanzen**, welche
die minimalen Schritte berechnet, die benötigt werden um einen String $\sigma_1$
in einen anderen $\sigma_2$ umzuwandeln. Diese Schritte beinhalten das Einfügen,
das Löschen, das Ersetzen und mit der Modifikation von Damerau
[@Dam:technique:64] auch das Vertauschen von Zeichen. Je weniger Schritte für
die Transformation notwendig sind, desto ähnlicher sind zwei Strings. Da
potentiell alle Zeichen beider Strings miteinander vergleichen werden ist die
Komplexität $O(|\sigma_1|\cdot|\sigma_2|)$. Needleman und Wunsch
[@NW:general:70] erweitern die originale Editierdistanz dahingehend, dass
bestimmte Operationen anders gewichtet werden. Dazu können die Kosten für die
einzelnen Schritte, welche in der einfachsten Form 1 sind, auf einen beliebigen
Gleitkommawert angepasst werden. Beispielsweise können Transpositionen, welche
auf einen Tippfehler zurückgeführt werden können, niedriger gewichtet werden. In
dieser Variante entspricht das Lösen der Editierdistanz dem Traveling Salesmen
Problem und ist daher NP-schwer.

Eine weitere Modifikation der Editierdistanz ist es Lücken zu erkennen
[@WSB:biological:76], beispielsweise wenn ein Wort abgekürzt wurde, etwa *John
R. Smith* statt *Johnathan Richard Smith*. Dementsprechend können Kosten für das
Öffnen und das Erweitern einer Lücke festgelegt werden. Eine andere ist Fehler
am Anfang oder am Ende des String mit geringeren Kosten zu versehen
[@SW:Identification:81].

Eine weitere gängige Alternative ist die Jaro-Distanz, welche die Anzahl der
gemeinsamen Zeichen $m$ berechnet, wobei eine Verschiebung von $\frac{1}{2}\cdot
min(|\sigma_1|, |\sigma_2|)$ zugelassen wird. Von den gemeinsamen Zeichen werden
anschließend die Transpositionen $t$ berechnet, d.h. wie viele gemeinsame
Zeichen nicht in der gleiche Reihenfolge sind. Daraus berechnet sich die
Jaro-Distanz $d_j =\frac{1}{3}\left(\frac{m}{|s_1|} + \frac{m}{|s_2|} +
\frac{m-t}{m}\right)$.

#### Tokenbasierende Ähnlichkeit

Die tokenbasierende Ähnlichkeit bietet, im Gegensatz zu den zeichenbasierenden,
den Vorteil, dass Vertauschungen von Wörtern erkannt werden. Beispielsweise bei
Vorname und Nachname.

Monge und Elkan [@MEo:Field:96] schlagen einen Algorithmus auf Basis atomarer
Strings vor. Für zwei Strings $\sigma_1, \sigma_2$ werden alle Token aus
$\sigma_1 = (\sigma_{1_{t_1}}, \dots, \sigma_{1_{t_i}})$ mit allen Token aus
$\sigma_2 = (\sigma_{2_{t_1}}, \dots, \sigma_{2_{t_j}})$ vergleichen. Zum
Vergleich wird eine beliebige zeichenbasierende Ähnlichkeit $sim$ gewählt
werden. Dadurch kann dieser Algorithmus auch typographische Fehler erkennen.
Anschließend wird für jeden Token $t$ in $\sigma_1$ die Ähnlichkeit mit $s_t =
max(sim(\sigma_{1_t}, \sigma_{2_{t_1}}), \dots, sim(\sigma_{1_t},
\sigma_{2_{t_j}}))$ berechnet. Die Gesamtähnlichkeit ist der Durchschnitt der
Tokenähnlichkeiten $m = avg(s_{t_1}, \dots, s_{t_i})$. Das Problem dieses
Algorithmus ist seine Komplexität, welche quadratisch zur Tokenmenge und damit
zu $sim$ ist.

Eine weitere Möglichkeit Token zu bilden sind Q-Gramme. Diese erlauben es
ebenfalls neben Vertauschungen von Wörtern auch typographische Fehler zu
erkennen. Über Q-Gramme wird ein String $\sigma$ in $k$ überlappende Token der
Länge $n$ zerlegt. Dazu wird ein Fenster der Länge $n$ von Position 1 bis
$|\sigma|-(n-1)$ geschoben. Um aus Q-Grammen eine Ähnlichkeit zu bestimmen gibt
es viele Möglichkeiten.

Eine deutlich einfachere und schnellere Möglichkeit die Ähnlichekit von Token zu
berechnen, ist der *Jarcard-Koeffizienten*. Dieser gibt die Ähnlichkeit zweier
Mengen $A, B$ mit $J(A,B) = {{|A \cap B|}\over{|A \cup B|}}$ an. Ein ähnliches
Maß bietet der *Simpson-Koeffizienten*, welcher die Überlappung zweier Mengen
$S(A,B) = \frac{| A \cap B | }{\min(|A|,|B|)}$ bestimmt.

Ein weiteres Vorgehen auf Basis atomarer Strings ist WHIRL von Cohen
[@Coh:WHIRL:00]. Es kombiniert die Kosinus-Ähnlichkeit mit dem TF/IDF
Gewichtungsschema. Dabei ist TF die Vorkommenshäufigkeit (engl. term frequency)
und gibt an wie häufig ein Token in einem String vorkommt. IDF ist die Inverse
Dokumenthäufigkeit (engl. inverse document frequency) und gibt an wie oft ein
Token in den bekannten Strings eines Vokabulars $D$ vorkommnt. Für alle atomaren
Strings $w$ wird ein Gewicht berechnet $$\nu_\sigma(w) = log(tf_w + 1) \cdot
log(idf_w).$$ Die Kosinus-Ähnlichkeit zweier String $\sigma_1$, $\sigma_2$ ist
dementsprechend definiert als $$sim(\sigma_1, \sigma_2) =
\frac{\sum_{i=1}^{|D|}{\nu_{\sigma_1}(j) \cdot
\nu_{\sigma_2}(j)}}{\|\nu_{\sigma_1}\| \|\nu_{\sigma_2}\|}$$ Mit WHIRL ist es
möglich vertauschungssicher die Ähnlichkeit von Strings zu bestimmen. Ein großer
Vorteil dieses Algorithmus ist, dass nach erheben des TF/IDF Index, die
Ähnlichkeit in $O(1)$ berechnet werden kann, da lediglich Werte nachgeschlagen
werden müssen. Dem entgegen steht, dass typographische Fehler nicht erkannt
werden. Deshalb haben Gravano et. al [@GIKS:Text:03] WHIRL erweitert und nutzen
statt atomare Strings Q-Gramme. Dadurch lassen sich bei gleicher Komplexität
auch Rechtschreibfehler erkennen, da ein Großteil der Q-Gramme gleich ist.
Allerdings geht zu zugunsten von Speicherkosten, da der TF/IDF Index
dementsprechend größer wird.

#### Phonetische Ähnlichkeit

Die phonetische Ähnlichkeit ist sowohl zeichen- als auch tokenbasiert. Es
werden jedoch nicht die Zeichen oder Token miteinander verglichen, sondern die
Sprechlaute von Wörtern. So ist es möglich gleich gesprochene Wörter mit
unterschiedlicher Schreibweise zu finden. Dies ist vor allen Dingen bei Namen
sehr nützlich, da es hier eine besonders hohe Dichte an gleich klingenden
Worten mit kleinen Variationen in der Schreibweise gibt. Phonetische
Enkodierungsschemata funktionieren allerdings meist nur für eine Sprache oder
einen Akzent. Bekannt Beispiele für englisch Sprache sind Soundex und NYSIIS,
sowie Metaphone und Double Metaphone, welche sich zum Teil auch auf nicht
englische Sprachen anwenden lassen. Ein Methode für die deutsche Sprache ist die
Kölner Phonetik.


#### Nummerische Ähnlichkeit

Während es für Strings eine Vielzahl ein Vergleichmöglichkeiten gibt ist die
Anzahl bei den nummerischen Überschaubar. Die offensichtlichste Methode ist eine
Nummer als String zu behandeln und entsprechende Algorithmen zu verwenden.
Andere eher primitive Vorgehensweisen sind etwa, die ersten $n$ oder letzten $m$
Ziffern miteinander zu vergleichen, beispielsweise bei einer Postleitzahl. Für
Mengenangaben zwischen zwei Werten $n_1$ und $n_2$ kann die maximale absolute
Differenz genutzt werden $sim_{d_{max}} = 1.0 - \left(\frac{|n_1 - n_2|}{d_{max}
}\right)$.

#### Kernel Ähnlichkeit

Anhand zweier gegebener Strings gibt es keine offensichtliche Antwort auf die
Frage: Wie ähnlich sind $\sigma_1$ und $\sigma_2$? Im Gegensatz dazu kann dies
für Vektoren in $\mathbb{R}^d$ eindeutig, beispielsweise über die
Kosinus-Ähnlichkeit $\frac{\sigma_1 \cdot \sigma_2}{\|\sigma_1\|\|\sigma_2\|}$
berechnet werden [@SRR:Large:07]. Kernelfunktionen werden unter anderem in
Support Vector Machines (SVM) eingesetzt. Diese Kernel weisen eine Reihe
statistischer interessanter Eigenschaften auf, beispielsweise dass ihre
Performanz unabhängig von der Dimensionalität ist, auf welcher die Berechnung
stattfindet. Dadurch ist es möglich in hohen Dimensionalitäten zu arbeiten ohne
Überanzupassen [@LSS.EA:Text:02].

Der einfachste und meist genutzte String Kernel ist der Bag-of-Words Kernel.
Dabei werden die Anzahl der vorkommenden Worte in $\sigma$ gezählt und in einem
dünnbesetzten Vektor, über die Menge aller bekannten Worte aller bekannten
Strings, erzeugt. Wie bereits bekannt sind atomare String anfällig für
typographische Fehler. Aus diesem Grund gibt es auch Variationen des Kernels,
welcher Q-Gramme nutzen, um dieses Problem zu umgehen.

Lodhi et al. stellen eine Kernelfunktion vor, um Strings im Feature Space
miteinander zu vergleichen, ohne diese vorher in Vektoren zu zerlegen. Der
sogenannte String Subsequence Kernel (SSK) vergleicht Strings, indem er
Stringvektoren erzeugt, welche einen bestimmten Substring beinhalten oder nicht.
Dabei wird jedes Vorkommen eines Substrings anhand der Übereinstimmung
gewichtet. Die Übereinstimmung erlaubt beispielsweise auch Lücken, sodass der
Substring 'c-a-r' in den beiden Wörtern '**car**d' und ' **c**ust**ar**d' mit
unterschiedlicher Gewichtung vorkommt.

## Klassifikatoren {#sec:classifier}

Die Aufgabe von Klassifikatoren oder Matching-Strategien (vgl. Köpcke & Rahm
[@KR:Frameworks:10]) ist es, Datensatzpaare in zwei Mengen Matches und
Non-Matches kategorisieren. Im Gegensatz zu den Attributesähnlichkeitsmaßen
bewerten diese einen kompletten Datensatz, welcher im Normalfall aus mehreren
Attributen besteht. Klassifikatoren können nach Elmagarmid et al.
[@EIV:Duplicate:07] in zwei Kategorien einordnen werden.

* Vorgehen die *Trainingsdaten* benötigen, um zu Lernen welche Datensätze
  übereinstimmen. Hierzu gehören überwachte, semi-überwachte, aktive und
  unüberwachte Lernstrategien.
* Vorgehen die *Domänenwissen* oder *generische Distanzmaße* nutzen, um
  Übereinstimmungen zu finden.

### Distanzbasierende Verfahren

Nachdem die Ähnlichkeit zwischen den Attributen der Kandidatenpaaren berechnet
wurden, gibt es für jedes Kandidatenpaar $(t_1, t_2)$ einen Ähnlichkeitsvektor
$(sim(t_{1,a_1},t_{2,a_1}), \dots, sim(t_{1,a_n},t_{2,a_n}))$ über die Attribute
$a_1, \dots, a_n$ beider Datensätze. Dieser Vektor wird von den nachfolgenden
Verfahren genutzt, um eine Match-Entscheidung zu treffen.

#### Schwellenwertbasierend

Die naivste Art und Weise zu klassifizieren sind nach Christen [@Chr:Data:12,
Kap. 6] Schwellenwerte. Dazu werden die Ähnlichkeitsvektoren zu einer
Gesamtähnlichkeit $g$ aufsummiert. Anschließend werden je nach Ausprägung bis zu
zwei Schwellen festgelegt. In der Variante mit einer Schwelle $t$, werden die
Kandidatenpaare in zwei Klassen mit $g \geq> t$ als Matches und $g < t$ als
Non-Matches klassifiziert. Werden zwei Schranken $t1$ und $t2$ genutzt, wird in
drei Klassen gegliedert, Matches mit $g \geq t1$,  Non-Matches mit $g \leq t2$
und zusätzlich gibt es noch den Bereich $t2 < g < t1$, welcher ein potentielles
Match bedeutet und manuelle klassifiziert werden muss. Der Nachteil dieser
Methodik ist, dass beim Mitteln des Vektors alle Attribute mit gleichem Gewicht
zum endgültigen Wert beitragen. Dadurch wird die Wichtigkeit der
unterschiedlichen Attribute und ihre Wertstellung innerhalb des Datensatzes
verworfen. Um dem entgegenzuwirken, können für jedes Attribut Gewichte vergeben
werden, mit welchen die Wertstellung der Attribute angegeben werden kann.
Dennoch gehen beim Aufsummieren von Ähnlichkeiten detaillierte Informationen
über die einzelnen Ähnlichkeiten verloren.

#### Regelbasierend

Christen [@Chr:Data:12, Kap. 6] beschreibt die regelbasierende Klassifikation
als Anwendung der Prädikatenlogik erster Stufe (PL1). Dabei wird ein
Klassifikationspredikat in konjunktiver Normalfrom mit disjunktiven Ausdrücken
geschrieben. $$\begin{aligned}((sim(name)[r_i, r_j] \geq 0.9) \land
(sim(stadt)[r_i, r_j] = 1.0))&\\ \lor (sim(plz)[r_i, r_j] \geq 0.7)& \implies
[r_i, r_j] \to Match \end{aligned} $$ {#eq:cls_rule} @eq:cls_rule zeigt eine
beispielhafte Regel, wobei $sim(\cdot)$ einer Attributsähnlichkeit entspricht
und $r_i$, $r_j$ zwei Datensätze sind. Diese Regel klassifiziert ein Paar als
Match, wenn entweder der Ähnlichkeitswert für Name größer 0.9 und die Stadt
gleich ist, oder der Ähnlichkeitswert der Postleitzahl größer 0.7 ist. Der
Vorteil gegenüber den schwellenbasierenden Verfahren ist, dass Ausdrücke auf
Attribute angewendet werden und dadurch die Informationen der einzelnen
Attributsähnlichkeiten nicht verloren gehen. Mit der regelbasierten
Klassifikation kann in beliebig viele Klassen kategorisiert werden.
Typischerweise werden entweder zwei Klassen Match und Non-Match oder zusätzlich
potentielles Match klassifiziert. Bei Match und Non-Match ist nur ein Prädikat
$P_m$ notwendig, da alle wahren Paare als Matches und alle falschen Paare als
Non-Matches klassifiziert werden. Für potentielle Matches wird ein weiteres
Prädikat $P_{pm}$ benötigt, dementsprechend sind Attribute Non-Matches, wenn
sowohl $P_m$ als auch $P_pm$ falsch ist. Für die Bestimmung der Prädikate gibt
es zwei Möglichkeiten. Die erste ist einen Domänexperten das Prädikat festlegen
zu lassen. Dies ist allerdings ein sehr zeitintensiver Prozess, welcher bei
aller Expertise in den meisten Fällen durch ausprobieren gelöst werden muss. Die
Alternative ist ein Prädikat zu Lernen, was ähnlich zum Lernen eines Blocking
Schema (vgl. @sec:blk_scheme) funktioniert.

### Überwachtes bzw. semi-überwachtes Lernen

Die Verfahren für überwachtes und semi-überwachtes Lernen benötigen eine Menge
von klassifizierten Daten in der Form von Matches und Non-Matches. Anhand dieser
Trainingsdaten kann ein Klassifikationsmodell erstellt werden. Soll das Model
Datensätze nur in die zwei Klassen Matches und Non-Matchses ordnen, wird ein
binärer Klassifikatoren gesucht.

<!--\TODO{Mehr ausführen!}-->

#### Decision Trees

Decision Trees sind als Klassifikatoren sehr beliebt, da ihre Funktionsweise
anschaulich ist. Zudem kann ein Modell übersichtlich visualisiert werden, sodass
es intuitiv,  auch von Laien, interpretiert werden kann. Ähnlich zu dem
regelbasierten Verfahren prüft auch der Decision Tree den Ähnlichkeitswert eines
bestimmten Attributes, welches einem Wert im Vektor entspricht. Dementsprechend
kann ein Model eines Decision Tree direkt in einem Prädikat formuliert werden.
@fig:decision_tree zeigt ein Beispiel eines Decision Tree. An jedem Knoten,
ausgehend von Wurzelknoten, wird der Ähnlichkeitswert eines bestimmten
Attributes über die Funktion $s(\cdot)$ geprüft. Weißt das Geburtdatum zweier
Datensätze eine Ähnlichkeit kleiner 0.5 auf wird diese durch den darauffolgeden
Blattknoten als Non-Match klassifiziert. Damit ein Blattknoten einen Datensatz
als Match klassifiziert müssen zusätzlich noch der Vorname ähnlicher als 0.15
und die Stadt ähnlicher als 0.8 sein.

```tex
\begin{figure}
\centering
\begin{forest}
  for tree={
    inner sep=0.7em,
    font=\itshape,
    if n children=0 {
      tier=terminal,
    }{},
  }
  [s(Geburtsdatum) $\leq$ 0.5, calign=edge midpoint, for tree=draw
    [s(Vorname) $>$ 0.15, edge label={node[midway,left,font=\scriptsize]{ja}}
        [s(Stadt) $>$ 0.8, edge label={node[midway,left,font=\scriptsize]{ja}}
            [Match, edge label={node[midway,left,font=\scriptsize]{ja}},rounded corners]
            [Non-Match, edge label={node[midway,right,font=\scriptsize]{nein}},rounded corners]
        ]
        [Non-Match, edge label={node[midway,right,font=\scriptsize]{nein}},rounded corners]
    ]
    [Non-Match, edge label={node[midway,right,font=\scriptsize]{nein}},rounded corners]
  ]
\end{forest}
\caption{Beispiel eines Decision Tree. Der Baum tested an jedem Knoten den
Ähnlichkeitswert eines bestimmten Attributes, durch die Funktion $s(\cdot)$.
Die Blattknoten bestimmen das Klassifikationsergebnis Match oder Non-Match.}
\label{fig:decision_tree} \end{figure}
```

#### Support Vector Machines

Support Vector Machines wurden von Boser et al. [@BGV:Training:92] eingeführt.
In ihrer einfachsten Form lernen SVMs eine separierende Hyperebene zwischen
einer Menge von Punkten, welche den Abstand zwischen der Hyperebene und den
nähesten Punkten der jeweiligen Klassen maximiert. Eine Kernelfunktion berechnet
das innere Produkt zwischen Punkten im Hyperraum (Feature Space)
[@LSS.EA:Text:02].

Bilenko & Mooney [@BM:Adaptive:03] stellen ein Lernverfahren auf Basis der
TF/IDF Ähnlichkeit von Cohen vor, welches einen SVM-Klassifikator nutzt. Dazu
wird ein Vektor erzeugt, indem die Kosinus-Ähnlichkeit zwischen den
Attributen eines Datensatzpaares berechnet wird. Dabei werden die Komponenten
der bekannten Summe $\frac{{x_i \cdot y_i}}{\|x\|\|y\|}$, welche zum i^ten^
Element des Vokabulars gehören, einem $d$-dimensionalen Vektor zugeordnet
$\mathbf{p}(x, y) = \langle \frac{{x_i \cdot y_i)}}{\|x\|\|y\|} \rangle$. In
@fig:bilenkosvm sind zwei Datensätze gezeigt. Jedes Attributspaar ist dabei ein
Teil eines Vektors. Die Vektoren werden dann von einem SVM-Model in Matches und
Non-Matches klassifiziert. Trainiert wird das SVM-Model anhand von Match und
Non-Match Vektoren.

![Datensatzklassifikation nach [@BM:Adaptive:03]. Der Featurevektor für die
Klassifikation wird aus den Attributsähnlichkeiten von Name, Address, City und
Cuisine erzeugt. Eine SVM klassifiziert diesen Vektor anschließend in Match
(duplicate records) oder Non-Match (Non-duplicate records).
](images/bilenko_class_svm.png){#fig:bilenkosvm}

Christen [@Chr:Automatic:08] erweitert dieses Verfahren, indem mehrere SVM
Modelle trainiert werden. Die initiale SVM wird mit der Mengen der
offensichtlichen Matches und Non-Matches der Gesamttrainingsmenge trainiert. Bei
offensichtlichen Matches sind die Werte der Vektoren sehr nahe an 1 und bei
offensichtlichen Non-Matches sehr nahe bei 0. Alle nicht offensichtlichen
Vektoren der Trainingsmenge werden anschließend mit der initialen SVM
klassifiziert. Je nach Klassifizierungsergebnis werden diejenigen, welche
am weitesten von der seperierenden Hyperebene entfernt sind, zur Menge der
offensichtlichen Matches bzw. Non-Matches hinzugefügt. Die zweite SVM wird dann
mit den erweiterten Trainingsmengen trainiert. Diese Schritte werden solange
wiederholt, bis ein Stopkriterium erfüllt ist.

### Aktives Lernen

Ein großer Nachteil der überwachten Lernverfahren ist, dass die Trainingsmenge
viele Beispiele benötigt und das diese repräsentativ für die zu Gesamtmenge von
Entitäten sein muss. Wie Trainingsdaten akquiriert werden wird in
@sec:measurements diskutiert. Als Alternative dazu gibt es die aktiven
Lernverfahren, welche initial nur eine sehr kleine Trainingsmenge (*seed*)
benötigen. Auf Basis des initialen Models werden in Interaktion mit einem
erfahrenen Benutzer Datensatzpaare selektiert, die helfen das
Klassifikationsmodell zu verbessern. Eine initiales Modell kann relativ einfach
über offensichtliche Matches und Non-Machtes erzeugt werden. Anschließend ist
aktives Lernen ein iterativer Prozess. Zunächst wird die Trainingsmenge mit dem
Modell klassifiziert. Anschließend ist aktives Lernen ein iterativer Prozess.
Zunächst wird die Trainingsmenge mit dem Model klassifiziert. Aus der Menge
klassifizierte Daten werden die Interessantesten ausgewählt, die manuelle von
einem Benutzer klassifiziert werden. Anschließend werden diese zu den initialen
Daten hinzugefügt und es wird ein neues Model trainiert. Diese Schleife wird
solange wiederholt bis ein Stopkriterum (Anzahl von Iterationen oder minimale
Genauigkeit) erreicht wurde.

Arasu et al. [@AGK:active:10] kombinieren eine aktives Lernvorgehen mit einem
Blockingmechanismus, welcher entweder mit einem Decision Tree oder einer SVM
funktioniert. Dabei gibt der Benutzer als Stopkriterium die Mindestpräzision
(siehe @sec:measurements) an, welcher das finale Modell entsprechen muss. Der
Lernprozess versucht dann ein Model zu finden, welches einen hohen Recall
liefert und gleichzeitig die Anzahl der manuell zu klassifizierenden Paare
gering hält.

## Messen von Qualität- und Komplexität[^1] {#sec:measurements}

[^1]: Dieser Abschnitt bezieht sich auf Analysen und Erklärungen zu Qualität und
  Komplexität von Entity Resolution Systemen aus Christen [@Chr:Data:12].

Aus den bis hier vorgestellten Verfahren zu Entity Resolution stellt sich die
Frage: Wie kann die Qualität und Komplexität dieser Verfahren gemessen werden?
Dies soll dazu dienen, ein Verfahren zu bewertet und gleichzeitig eine
Vergleichbarkeit zwischen anderen Verfahren bieten. Damit die Qualität und die
Komplexität der ER-Verfahren überprüft werden kann, ist es unerlässlich über die
Ground Truth Daten (auch Gold Standard Daten) zu verfügen. Die Ground Truth
beschreibt eine Menge gekennzeichneter Daten, welcher einer oder mehreren
Klassen angehören. Für Entity Resolution sind die Daten Datensatzpaare und die
Klassen Matches und Non-Matches. Damit die Ground-Truth repräsenativ für die zu
überprüfenden Daten ist, sollte diese möglichst deren Charakteristik
wiederspiegeln. Daraus entsteht die nächste Frage: Woher kommen die Ground Truth
Daten?

* Wird versucht einen entwickelten Algorithmus/Verfahren zu bewerten, dann
  empfiehlt es sich einen der frei verfügbaren Datensätze zu nehmen, zu welchen
  bereits Ground Truth Daten existieren und beispielsweise von Wissenschaftlern
  oder Domainexperten manuell klassifiziert wurden. Das Problem ist, dass viele
  dieser Datensätze nur wenige Einträge (meist $< 10.000$) haben und daher kaum
  Bezug zu Realdaten haben. Diese Datensätze werden in @sec:datasets diskutiert.

* Soll ein Verfahren auf Daten einer Domäne angewendet werden, zu welcher keine
  Ground Truth existiert, müssen diese manuell erzeugt werden. Dabei werden
  Datensatzpaare zufällig erzeugt und müssen anschließend von einem Prüfer in
  Matches und Non-Matches klassifiziert. Ein großer Nachteil dieser Methode ist,
  selbst wenn ein Blockingverfahren angewendet wurde, dass die Zahl der zu
  klassifizierenden Paare riesig ist. Hinzu kommt, dass die Anzahl der Matches
  nur einen Bruchteil der Paare betrifft, weshalb die Ground Truth ein
  deutliches Ungleichgewicht aufweisen wird. Ein weiteres Problem ist, dass in
  diesem Prozess Fehler gemacht werden. Dabei entstehen die Fehler nicht bei den
  offensichtlichen Match und Non-Matches, sondern meist in Paaren, die auch für
  den Menschen nur schwer zu bewerten sind. Des Weiteren kann es zu
  unterschiedlichen Klassifizierungen je nach Prüfer kommen und auch derselbe
  Prüfer kann je nach Gemütslage und Konzentrationslevel unterschiedliche
  Aussagen über dasselbe Paar treffen.

Vogel et al. [@VHD.EA:Reach:14] haben deshalb einen sogenannten *Annealing
Standard* entwickelt, welcher das Erstellen einer Ground Truth über einen
iterativen Prozess vereinfachen sollen. Dabei wird zunächst mit einem
Klassifikatoren eine Baseline erzeugt, die den Annealing Standard darstellt.
Anschließend werden mit einem weiteren Klassifikatoren, welcher der vorherige
mit anderen Parametern sind kann, Paare erzeugt und mit der Baseline verglichen.
Die Übereinstimmung der beiden bildet den neuen Annealing Standard. Die übrigen
Paare werden zu manuellen Inspektion Prüfern vorgelegt und die dadurch erzeugten
Matches und Non-Matches werden mit dem Annealing Standard verschmolzen. Diese
Iteration wird solange wiederholt, bis das Delta der Klassifikatoren einen
bestimmten Maximalwert an Paaren unterschreitet.

```{.texalgo #alg:weaklabels
    caption="WeakTrainingSet w\textbackslash o Ground Truth"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Upper Threshold: $ut$
  \item Lower Threshold: $lt$
  \item Blocking Window Size: $c$
  \item Maximum Duplicate Pairs: $max_p$
  \item Maximum Non-Duplicate Pairs: $max_n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item A set of positive samples: $P$
  \item A set of negative samples: $N$
  \end{itemize}
}
\Statex
\State Initialize set $P = ()$, set $N = ()$
\State Initialize set of tuple pairs $C = ()$
\State Generate TFIDF statistics of $D$\label{alg:wl:tfidf}
\For{fields $f \in D$} \label{alg:wl:tks}
    \For{records $r \in D$}
        \State Tokenize $r_f$ into $BKV_f$
        \State Block $r$ on generate tokens for field $f$
    \EndFor
\EndFor \label{alg:wl:tke}
\For{block $B$ generate in previous step}\label{alg:wk:cs}
    \State  Slide a window of size c over tupels in $B$
    \StatexIndent[1] Generate all possible pairs within window and
    \StatexIndent[1] add to $C$
\EndFor \label{alg:wk:ce}
\For{pairs $(t_1, t_2) \in C$}
    \State Compute TFIDF similarity $sim$ of $(t_1, t_2)$
    \If{$sim \geq ut$}\label{alg:wk:ps}
        \If{$|P| < d$}
            \State add $(t_1, t_2)$ to $P$
        \ElsIf{$sim >$ lowest $sim$ in $P$}\label{alg:wk:pcks}
            \State Replace pair with lowest $sim$ in $P$ with $(t_1, t_2)$
        \EndIf\label{alg:wk:pcke}
    \EndIf\label{alg:wk:pe}
    \If{$sim < lt$}\label{alg:wk:ns}
        \If{$|N| < nd$}
            \State add $(t_1, t_2)$ to $N$
        \ElsIf{$sim >$ lowest $sim$ in $N$}\label{alg:wk:ncks}
            \State Replace pair with lowest $sim$ in $N$ with $(t_1, t_2)$
        \EndIf\label{alg:wk:ncke}
    \EndIf\label{alg:wk:ne}
\EndFor\label{alg:wk:sime}
\State Return $P$ and $N$
```

Ein weiteres Verfahren haben Kejriwal & Mirankern [@KM:Unsupervised:13]
entwickelt. Ihre Idee ist es, eine Menge schwach klassifizierter Datenpaare zu
generieren. Dabei werden sowohl positive (Matches), als auch negative
(Non-Matches) Datensatzpaare klassifiziert. Über zwei Schranken kann der
Benutzer festlegen wie ähnlich (obere Schranke $ut$) bzw. wie verschieden
(untere Schranke $lt$) die Paare sein sollen. Paare größer $ut$ werden dadurch
als Matches und Paare kleiner $lt$ als Non-Matches klassifiziert. In Algorithmus
\ref{alg:weaklabels} wird die Funktionsweise erklärt. Damit vor allem die
erzeugten Ground Truth Non-Matches, der klassifizierten Paare, nicht beliebig
groß werden, kann der Anwender festlegen, wie viele Matches $max_p$ bzw.
Non-Matches Paare $max_n$ maximal erzeugt werden sollen. Zunächst wird die
TF/IDF Statistik über $D$ erzeugt (Zeile \ref{alg:wl:tfidf}), welche für einen
späteren Paarvergleich benötigt wird. Die in diesem Algorithmus genannten
$fields$ beschreiben die Positionen eines Attributes im Tupel eines Datensatzes,
beispielsweise für ein Tupel (`Kevin`, `Sapper`, `Hochschule RheinMain`) hat der
Nachname die Position 2. Anschließend wird ein Blocking der Daten per Standard
Blocking und Sorted Neighborhood durchgeführt. Jeder Datensatz wird (pro
Attribute) in Token zerlegt (Zeilen \ref{alg:wl:tks}-\ref{alg:wl:tke}). Anhand
der Token wird das Standard Blocking durchgeführt, wobei jeder Datensatz in
mehreren Blöcken vertreten sein kann. Die Menge von Blöcken sind jeweils nach
Attributen gruppiert, sodass Token unterschiedlicher Attribute nicht als
Blockschüssel desselben Blockes genutzt werden, da die Datensätze in den
entsprechenden Blocken, trotz übereinstimmenden Token, vermutlich wenig
Ähnlichkeit haben. Danach wird die Kandidatenmenge $C$ möglicher Matches bzw.
Non-Matches anhand der gruppierten Datensätze generiert (Zeile
\ref{alg:wk:cs}-\ref{alg:wk:ce}). Um innerhalb der Blöcke den
Paarvergleichsaufwand zu reduzieren, wird die Sorted Neighborhood verwendet und
ein Fenster der Größe $c$ über den Block geschoben. Nachdem das Fenster über
jeden Block geschoben wurde, steht die Menge möglicher Kandidatenpaare fest.
Diese Paare werden nun mit der TF/IDF-Ähnlichkeit $sim$ (aus Cohen
[@Coh:WHIRL:00]) verglichen (Zeilen \ref{alg:wk:ps}-\ref{alg:wk:pe}). Aufgrund
der, über die kompletten Daten erfassen, TF/IDF Statistik beträgt die
Komplexität des Vergleiches $O(1)$, da lediglich die entsprechenden TF und IDF
Werte, der Attribute eines Paares, nachgeschlagen werden müssen. Ist die
Ähnlichkeit $sim \geq ut$, wird das Paar als Match klassifiziert und zu $P$
hinzugefügt (Zeilen \ref{alg:wk:ps}-\ref{alg:wk:pe}). Analog, ist $sim < lt$
wird das Paar als Non-Match klassifiziert und zu $N$ hinzugefügt (Zeilen
\ref{alg:wk:ps}-\ref{alg:wk:pe}). Von allen Matches werden jeweils die $max_p$
mit der höchsten Ähnlichkeit $sim$ ausgewählt (Zeilen
\ref{alg:wk:pcks}-\ref{alg:wk:pcke}). Analog werden ebenfalls die $max_n$
Non-Matches mit der höchsten Ähnlichkeit $sim$ gewählt (Zeilen
\ref{alg:wk:ncks}-\ref{alg:wk:ncke}). Bei den Non-Matches soll dadurch
verhindern, dass lediglich Paare mit $sim \approx 0.0$ ausgewählt werden, da
diese für gewöhnlich zu niedrigen Klassifikationsraten führen.

* Werden schnell große Datensätze mit entsprechender Ground Truth benötigt,,
  bieten sich synthetisch generierte Datensätze an. Damit diese repräsentativ
  sind, sollten Sie die gleichen Attribute haben wie die echten Datensätze. Dazu
  wird eine Datenbank möglicher Attributswerte benötigt, welche der Generator
  verwenden soll. Zusätzlich gibt es Parameter, um die Größe des Datensatzes und
  Anzahl der Duplikate, die Häufigkeitsverteilung der einzelnen Attribute und
  die Modifikationen der Duplikate gegenüber dem Original, in typographische,
  OCR oder phonetisch Fehler, zu bestimmen. Beispiele solcher Datensätze finden
  sich in @sec:datasets.

* Anstatt synthetische Datensätze zu generieren und anschließend Fehler
  einzufügen, ist stattdessen auch möglich in einen bestehenden Datensatz Fehler
  einzubauen und diese als entsprechende Ground Truth zu verwenden. Dadurch
  werden allerdings die tatsächlichen Matches unterschlagen, was zu Konflikten
  bei der Entity Resolution führen kann.

### Qualitätsmaße

Ist zu einem Datensatz die Ground-Truth verfügbar, so können die klassifizierten
Datensätze einer der Kategorien in @tbl:cls_pred_matrix zugeordnet werden.

* True Positives (TP), sind alle Paare, die als Matches klassifiziert wurden und
  nach Ground Truth tatsächlich Matches sind.
* False Positives (FP), sind alle Paare, die als Matches klassifiziert wurden
  aber keine sind.
* False Negatives (FN), sind alle Paare, die als Non-Matches klassifiziert
  wurden aber tatsächlich Matches sind.
* True Negatives (TN), sind alle Paare, die als Non-Matches klassifiziert wurden
  und auch tatsächlich zwei verschiedene Entitäten identifizieren.

```tex
\begin{table}[]
\centering
\renewcommand{\arraystretch}{1.5} % Default value: 1
\resizebox{\textwidth}{!}{%
\begin{tabular}{ll|c|c|}
\cline{3-4}
                                                                                                &           & \multicolumn{2}{c|}{Predicted classes}      \\ \cline{3-4}
                                                                                                &           & Match                & Non-Match            \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Actual\\ Matches\end{tabular}}} & Match     & True Positives (TP)  & False Negatives (FN) \\ \cline{2-4}
\multicolumn{1}{|c|}{}                                                                          & Non-Match & False Positives (FP) & True Negatives (TN)  \\ \hline
\end{tabular}%*
}
\caption{Matrix mit den vier Klassifikationszuständen. TP wenn tatsächliches und
klassifiziertes Match, FN wenn tatsächlich Non-Match, aber klassifiziert als
Match, FP wenn tatsächlich Match, aber klassifiziert als Non-Match und TN wenn
tatsächliches und klassifiziertes Non-Match.}
\label{tbl:cls_pred_matrix}
\end{table}
```

Das Ergebnis eines idealen Klassifikatoren ist, dass soviele Matches wie möglich
True Positives sind und die Anzahl der False Positives, sowie False Negatives
klein ist. Auf Basis der vier Klassifikationsklassen können Qualitätsmaße
bestimmt werden. Die folgende Liste zeigt die beliebtesten Methoden und erklärt
ihre Stärken und Schwächen.

* *Accuracy*.
  $$acc = \frac{TP+TN}{TP+FP+TN+FN}$$ {#eq:accuracy}
  Die Genauigkeit ist ein weit verbreitetes Qualitätsmaß für Binär- und
  Multi-Klassen Probleme im Maschine-Learning Bereich. Die Accuracy ist nützlich
  in Situationen, in welchen die Klassen möglichst gleich verteilt sind. Für
  Entity Resolution ist dieses Maß daher nur bedingt geeignet, da zwischen
  Matches und Non-Matches fast immer ein Ungleichgewicht zugunsten von
  Non-Matches besteht. Daher sind die meisten klassifizierten Ergebnisse True
  Negatives, welche die Gleichung dominieren. Ein naiver Klassifikator der
  ausschließlich Non-Matches klassifiziert erhält dadruch eine hohe Genauigkeit.

* *Precision*.
  $$prec = \frac{TP}{TP+FP}$$ {#eq:precision}
  Precision wird oft als Qualitätsmaß vor Suchergebnisse genommen, da es den
  Anteil der True Positives in den Matches berechnet. Für Entity Resolution
  misst die Precision den Anteil von korrekt bestimmten Matches.

* *Recall*.
  $$rec = \frac{TP}{TP+FN}$$ {#eq:recall}
  Recall misst den Anteil der tatsächlichen Matches (TP + FN), welche korrekt
  (TP) als Matches klassifiziert wurden. Zwischen Recall und Precision gibt es
  einen Kompromiss. Beispielsweise kann der Recall verbessert werden, indem die
  Precision abgesenkt bzw. die Precision verbessert indem der Recall gesenkt
  wird.

* *F-measure*. Auch bekannt als *f-score* oder *f_1-score*.
  $$fmeas = 2 \cdot \left(\frac{prec \cdot rec}{prec + rec}\right)$$ {#eq:fscore}
  Das F-measure berechnet das harmonische Mittel zwischen Precision und Recall.
  Eine guter F-measure Wert ist daher ein Kompromiss zwischen den beiden.

Die oben genannten Qualitätsmaße berechnen alle einen exakten Wert für die
Qualität eines Klassifikators. Aus den bekannten Verfahren für Blocking,
Vergleich und Klassifizierung geht hervor, dass diese eine Reihe von Parametern
haben, um das Ergebnis zu kalibrieren. Deshalb ist es sinnvoll eine Reihe von
Werten zu erzeugen, um diese miteinander zu vergleichen. Ein solcher Vergleich
funktioniert am einfachsten per Visualisierung. Die folgenden drei
Visualisierungen werden dazu oft verwendet:

* *Precision-recall Graph*. Diese Visualisierung zeigt den Kompromiss zwischen
  Precision und Recall (siehe @fig:quality_graphs\[b]). Für jedes klassifizierte
  Match wird eine Reihenfolge, beispielsweise anhand der Summe der
  Ähnlichkeiten, bestimmt. Die Ähnlichkeiten dienen als Schwellwerte und für
  jede Schwelle wird Precision und Recall berechnet. Dabei ist die X-Achse stets
  der Recall und die Y-Achse die Precision. Durch den Kompromiss startet die
  Kurve meist in der oberen rechte Ecke mit hoher Precision und niedrigen Recall
  und endet in der linken unteren Ecke mit entgegengesetzten Werten. Dabei ist
  das Ziel die Kurve möglichst Nahe an die linke obere Ecke zu bekommen, in
  welcher Precision und Recall maximal sind.
* *Average Precision*. Der Precision-recall Graph kann effektiv die Qualität
  eines Verfahrens oder Klassifikators darstellen. Automatisiert eine
  Entscheidung anhand eines Graphen zu treffen ist jedoch schwierig, weshalb mit
  der Average Precsion die Fläche unter Precision-recall Kurve im
  Recallintervall 0 bis 1 ermittelt wird. Die Average Precision wird in der
  Praxis berechnet durch
  $$\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k),$$ {#eq:avep}
  wobei $k$ die Schwelle ist, $n$ die Anzahl
  der insgesamt klassifizierten Matches, $P(k)$ ist die Precision für die
  Schwelle $k$ und $\Delta r(k)$ ist die Differenz des Recalls zwischen $k-1$
  und $k$.
* *F-measure Graph*. Anstatt zwei Qualitätmaße gegeneinander zu zeichen, kann
  man diese auch zusammen im Bezug auf einen bestimmten Parameter darstellen
  (siehe @fig:quality_graphs\[c]). Der F-measure Graph, beispielsweise plottet
  Precision, Recall und F-measure gegen einen Parameter, wie etwa die
  akkumulierte Gesamtwahrscheinlichkeit bei den schwellenbasierten
  Klassifikatoren genutzt. Darüber kann dann abgelesen werden, bei welchem Wert
  (z.B. Schwelle) die beste Precision, der beste Recall und das beste F-measure
  erreicht wird.
* *ROC Kurve*. Wie der Precision-recall Graph vergleicht die
  Receiver-Operating-Characteristic (ROC) Kurve zwei Qualitätmaße. In diesem
  Fall die auf der X-Achse die False-Positve-Rate und auf der Y-Achse der Recall
  (siehe @fig:quality_graphs\[d]). Obwohl die ROC Kurve robust gegen
  ungleichgewichtige Klassen ist, so ist diese mit Vorsicht für Entity
  Resolution zu genießen, da die False Positive Rate die True Negatives
  miteinbezieht, hat die Kurve das Problem etwas zu optimistisch zu sein.
  Verschiedene ROC Kurven verschiedener Klassifikatoren mit unterschiedlichen
  Parametern zu vergleichen, kann dennoch nützlich sein, um deren Qualität zu
  bewerten.

![Beispiele von Qualitätsgraphen aus [@Chr:Data:12]. **b**. Precision-Recall,
**c**. F-measure, **d**. ROC Kurve. \TODO{Ersetzen durch eigene Graphen!}
](images/quality_graphs.png){#fig:quality_graphs}

### Effizienzmaße

Neben der Qualität bestimmt auch die Effizienz wie gut Entity Resolution Systeme
funktionieren. Die offensichtlichste Art Effektivität zu messen ist, die
Laufzeit des Verfahrens zu messen und miteinander zu vergleichen. Allerdings ist
dieser Ansatz abhängig von der genutzten Hardware und bietet keinen
plattformübergreifenden Vergleich. Für die folgenden Maße müssen zunächst einige
Mengen definiert werden. Zunächst wird in die Menge aller tatsächlichen Matches
$n_M$ und die Menge aller tatsächlichen Non-Matches $n_N$ gegliedert.
Dementsprechend ist $n_M + n_N = m \cdot n$ für Entity-Linking und $n_M + n_N =
m(m-1)/2$ für Deduplizierung. Die Menge der durch Blocking gruppierten
Datensatzpaare wird ebenso in Matches und Non-Matches geteilt und mit $s_M$ bzw.
$s_N$ bezeichnet, wobei $s_M + s_N \leq n_M + n_N$.

* *Reduction Ratio*. Dieses Maß gibt an wie viele Datensatzpaare von einem
  Blockingverfahren generiert worden sind und setzt diese ins Verhältnis mit der
  Anzahl aller möglichen Datensatzpaaren, welche ohne Blocking generiert worden
  wären. Das Reduction Ratio ist definert als
  $$rr = 1 - \left(\frac{s_M + s_N}{n_M + n_N}\right).$$ {#eq:reductionratio}
* *Pairs completeness*. Dieses Maß berechnet den Anteil der möglichen Matches.
  Es wird berechnet mit
  $$pc = \frac{s_M}{n_M}.$$ {#eq:pairscompletness}
  Pairs completeness ist mit dem Recall aus @eq:recall verwandt. Je geringer die
  Pairs completeness ist, desto geringer ist auch die Matchingqualität, da
  dieses Maß eine Obergrenze für einen möglichen Recall bestimmt. Denn
  tatsächliche Matches, die von einem Blocking Mechanismus nicht selektiert
  werden, können auch nicht klassifiziert werden. Zwischen Reduction Ratio und
  Pairs Completness gibt es einen offensichtlichen Kompromiss, je mehr
  Datensatzpaare erzeugt werden, desto mehr tatsächliche Matches können gefunden
  werden.
* *Pairs quality*. Dieses Maß berücksichtigt die Qualität eines
  Blockingverfahren, indem es selektierten tatsächlichen Matches in Relation mit
  mit allen selektierten Paaren stellt. Es wird berechnet mit
  $$pq = \frac{s_M}{s_M + s_N}.$$ {#eq:pairsquality}
  Die Pairs quality ist verwandt mit der Precision aus @eq:precision. Eine hohe
  Pairs qualtiy bedeutet, dass ein Blockingverfahren hauptsächlich Paare
  erzeugt, welche tatsächlich Matches sind. Auch hier gibt es ähnlich zu
  Precision und Recall einen Kompromiss zwischen Pairs completness und Pairs
  quality.
