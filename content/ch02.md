# Grundlagen

## Blocking

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|*|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}*|A|*(|A|-1)$ durchgeführt Vergleiche werden. In beiden Fällen ist
die Anzahl der Vergleiche quadratisch zur Eingabemenge  [@Kol:Effiziente:14]. In
der Studie [@KTR:Evaluation:10] zeigen Köpcke et al., dass das kartesische
Produkt für größe Datenmengen nicht skaliert. Aus diesem Grund reduzieren
moderne Entity Resolution Frameworks den Suchraum auf die wahrscheinlichten
Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden zur Reduzierung des
quatratischen Suchraum werden übergreifened als Blockingmethoden bezeichnet.
Neben Blocking werden Windowing- und Clustering-Methoden eingesetzt. Während
Blockingverfahren die Anzahl der notwendigen Vergleiche drastisch reduzieren,
indem Non-Matches ausgeschlossen werden, besteht dennoch die Gefahr, dass
fälschlicherweise tatsächliche Matches rausgefiltert werden. Daher ist es
notwendig die Güte des Blockingverfahrens zu bestimmen. Dazu werden zwei
Kennziffern erhoben. Zum einen die *Reduction Ratio*, welche die Reduzierung der
Vergleiche im Gegensatz zum Kartesischen Produkt ausdrückt, sowie die *Pairs
Completeness*, welche den Anteil der tatäschlich ausgewählten Duplikate, die
sich nach dem Blocking in der Kandidatenmenge befinden, beschreibt.

Prinzipelle erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" befinden. Zur Durchführung der
Gruppierung oder Sortierung müssen sog. Block- bzw. Sortierschlüssel für jeden
Datensatz erzeugt werden. Diese Schlüssel werden von den Attributwerten oder
einem Teil der Attributwerte abgeleitet und stellen eine nicht einzigartige
Signatur des Datensatzes dar.

### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee ist eine Menge von Datensätzen in disjunkte
Partitionen (gennant Blöcke) zu teilen. Anschließend werden nur die Datensätze
des jeweiligen Blocks miteinander verglichen. Dazu wird jedem Datensatz ein
Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens hängt daher
maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und Größe der
Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
Abbildung @fig:sb_blocking zeigt die Ausführung des Blockingverfahrens
beispielhaft an einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein
Blockschlüssel (hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses
Schlüssels gruppiert. Die größe der einzelnen Blöcke bestimmt die Reduction
Ratio. Diese hängt allerdings immer von der Datenquelle ab und kann daher nicht
pauschalisiert werden. Bei der Generierung der Blockschlüssel können fehlerhafte
Werte einzelner Attribute dazu führen, dass Dupliakte in unterschiedlichen
Blöcken landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden
Datensatz mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert
werden. Dieser Ansatz nennt sich Multi-pass Blocking.

![Beispielhafte Standard Blocking Ausführung. Nach [@Kol:Effiziente:14].](pictures/standard_blocking.pdf){#fig:sb_blocking}

### Sorted Neighborhood
