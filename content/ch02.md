# Grundlagen

## Entity Resolution

Die Methoden zur Duplikatserkennung stammen ursprünglich aus dem
Gesundheitsbereich und wurden erstmal 1969 von Felegi & Sunter [@FS:Theory:69]
formal formuliert. Je nach Fachgebiet gibt es unterschiedliche Fachbegriffe.
Statistiker und Epidemiologen sprechen von *record* oder *data linkage*, während
Informatiker das Problem unter *entity resolution*, *data* oder *field
matching*, *duplicate detection*, *object identification* oder *merge/purge*
kennen. Identifiziert werden sollen dabei beliebige Entitäten. Die Schwierigkeit
dabei ist allerdings, dass Entitäten nicht durch ein einzigartiges Attribut
identifiziert werden können. Zudem sind die Datensätze oft fehlerhaft,
beispielsweise durch Rechtschreibfehler oder unterschiedliche Konventionen. Die
Methoden zur Entity Resolution (ER) arbeiten meist auf Datensatzpaaren und
liefern als Ergebnis eine Menge von Übereinstimmungen. Damit eine
Übereinstimmung zwischen zwei oder mehr Entitäten festgestellt werden kann
müssen diese Verglichen werden und ein Ähnlichkeitswert (engl. similarity score)
bestimmt werden. Dieser Ähnlichkeitswert gibt die Intensität der Übereinstimmung
an.

Zur Bestimmung der Ähnlichkeit eines Datensatzpaares unterscheiden Elmagarmid et
al. [@EIV:Duplicate:07] zwischen Attributsvergleichs- (engl. field matching) und
Datensatzvergleichsmethoden (engl. record matching). Methoden zum
Attributsvergleich sind zeichenbasierend (edit distance, affine gap distance,
Jaro distance metric oder Q-gram distance), tokenbasierend (atomic strings,
Q-grams mit tf.idf), phonetisch (soundex) oder nummerisch. Die
Datensatzvergleichsmethoden sind probabilistisch (Naive Bayes), überwachtes bzw.
semi-überwachtes Lernen (SVMLight [@Joa:Svmlight:99], Markov Chain Monte Carlo
[@SD:Entity:06]), aktives Lernen (ALIAS [@SB:Interactive:02]), distanzebasierend
(siehe Attributsvergleich - Datensatz als konkatenierter String) oder
regelbasierend (AJAX [@GFS.EA:Declarative:01]). Damit verschieden
Vergleichmethoden miteinander kombiniert werden können, wird der
Ähnlichkeitswert für gewöhnlich auf einen Wert zwischen 0 und 1 normiert. Wobei
1 totale Übereinstimmung und 0 keine Übereinstimmung bedeutet.

Da es keine Methode zur Entity Resolution gibt, welche allen anderen überlegen
ist, wurden Ende der 90er Jahre begonnen Frameworks zu entwickeln, welche
verschiedene Methoden miteinander kombinieren. Einen Vergleich dieser Frameworks
wurde durch Köpcke & Rahm 2010 [@KR:Frameworks:10] durchgeführt. Ein Framework
besteht aus verschiedenen Matchern. Ein Matcher ist dabei ein Algorithmus,
welcher die Ähnlichkeit zweier Datensätze ermittelt. Ähnlich wie Elmagarmid et
al. unterscheiden Köpcke & Rahm zwischen attributs- und kontextbasierenden
Matchern. Als Kontext bezeichnen Sie die semantische Beziehung bzw. Hierarchie
zwischen den Attributen. Um die Matcher miteinander zu kombinieren nutzen die
Frameworks min. eine Matching Strategie. Eine Strategie ist, die
Ähnlichkeitswerte verschiedener Matcher nummerisch zu kombinieren,
beispielsweise durch eine gewichtete Summe oder einen gewichteten Durchschnitt.
Ein anderer Ansatz ist regelbasierend. Eine einfache Regel besteht aus einer
logischen Verbindung und einer Match-Kondition, beispielsweise einem
Schwellenwert. Die dritte und komplexeste Strategie ist workflow-basierend.
Hierbei kann beispielsweise eine Sequenz von Matchern die Ergebnisse iterativ
einschränken. Grundsätzlich können Workflows beliebig komplex werden. Einen
passenden Workflow zu finden kann selbst Domainexperten vor eine große
Herausforderung stellen. Daher gibt es trainingbasierende Ansätze passende
Parameter für Matcher oder Kombinationsfunktionen (z.B. Gewicht für Matcher) zu
bestimmen. Solche Ansätze sind etwa, Naive Bayes, Logistic Regression, Support
Vector Maschine oder Decision Trees. Durch die Match-Strategie sollen
verglichene Datensätze in Paare von Matches und None-Matches klassifiziert
werden.

In der klassischen Variante arbeitet Entity Resolution auf statischen Daten.
D.h. das während des ER-Prozesses keine neuen Daten hinzukommen. Hierbei werden
die zwei Disziplinen Deduplizierung und Entity-Linking unterschieden. Die
Deduplizierung wird auf einer Datenquelle durchgeführt und hat den Zweck alle
Duplikate in dieser Datenquelle zu finden. Anschließend werden die gefundenen
Duplikate automatisch oder manuell zusammengeführt. Entity Linking hingegen wird
auf mindestens zwei verschiedenen Datenquellen durchgeführt. Das Ziel ist es,
nicht Duplikate zusammenzuführen, sondern Entitäten unter den Datenquellen zu
verlinken. Damit die Links eindeutig sind, wird vorausgesetzt, dass die
einzelnen Datenquellen dedupliziert sind.

Ein Großteil der Forschung in Entity Resolution konzentriert sich auf die
Qualität der Vergleichsergebnisse. Die von Köpcke & Rahm verglichenen Frameworks
konzentrieren sich alle Samt darauf zwei statische Mengen zu miteinander
vergleichen. Bei großen Datenmengen kann dies durchaus mehrere Stunden dauern.
Daher gibt es in den letzten Jahre einige Ansätze und Frameworks, welche
MapReduce Algorithmen zum Skalieren nutzen [@KR:Parallel:13, @MAS:Graph:14].
Einen weiteren Ansatz die Laufzeit für nahe Echtzeit Anwendungen zu optimieren
präsentieren Whang et al. [@WMG:PayAsYouGo:13]. Anstatt eine
Übereinstimmungsmenge nach Abschluss eines Algorithmus zu liefern, zeigen Sie
Möglichkeiten partielle Ergebnisse während der Laufzeit des Algorithmus zu
erhalten. Dabei werden die Suchalgorithmen so modifiziert, dass zunächst die
Wahrscheinlichsten Kandidaten miteinander verglichen werden. Dabei wird in
relativ kurzer Zeit ein Großteil der Entitäten gefunden.

Zudem gibt es immer mehr Bedarf, Vergleichsergebnisse in nahe Echtzeit zu
liefern. Im Gegensatz zum statischen ER gibt es das dynamische ER. Der
Ausgangspunkt für diese Disziplin ist eine deduplizierte Datenquelle. Das Finden
gleicher Enitäten erfolgt hierbei auf Anfrage, weshalb die gesamte Datenmenge
vorab nicht bekannt ist. Ein dynamisches ER System ist ähnlich einer
Suchmaschine, doch anstatt einer gewerteten Liste möglicher Treffer, soll es
alle gleichen Entitäten finden, welche zur Anfrage passen. Das bedeutet
insbesondere, dass die Anfrage die gleiche Datenstruktur haben muss, wie die zu
durchsuchende Datenquelle. Zudem ergeben sich Anforderungen an die Antwortzeit,
welche bei vielen System im Subsekundenbereich liegt. Erste Ergebnisse Entity
Resolution in nahe Echtzeit zu erreichen, präsentieren Christen & Gayler in
[@CG:Scalable:08], unter Verwendung von Inverted Indexing Techniken,
welche normalerweise bei der Websuche Anwendung finden.

Die Ausführung der Vergleichsmethoden ist enorm teuer, da diese das Kreuzprodukt
zweier Mengen bilden müssen. Dies führt zu einer quadratischen Komplexität,
welche dafür sorgt, dass bei großen Datenmengen die Ausführungszeit im
statischen Fall bzw. die Latenzen im dynamischen Fall unakzeptabel groß werden.
Um die Ausführungszeit zu reduzieren wird versucht den Suchraum auf die
wahrscheinlichsten Duplikatsvorkommen zu begrenzen. Diese Vorgehen werden als
Blocking oder Indexing bezeichnet.

![Vereinfachter Entity Resolution Workflow. Nach [@Kol:Effiziente:14].](pictures/entity_resolution_workflow.png){#fig:er_workflow}

@fig:er_workflow zeigt einen vereinfachten typischen Entity Resolution Workflow
für statisches ER. Zunächst werden die Datensätze vorverarbeitet, um typische
Fehler zu entfernen. Dazu gehört das Korrigieren von Rechtschreibfehler,
ignorieren von Groß- bzw. Kleinschreibung und ersetzten von bekannten
Abkürzungen. Durch die Vorverarbeitung kann die Qualität des Matchings
verbessert, indem verhindert wird, dass offensichtliche Abweichungen den
Ähnlichkeitswert beeinflussen. Der nächste Schritt das Blocking, teilt die
Gesamtmenge in Submengen zur Reduzierung der Komplexität. Auf das Blocking folgt
das Matching hierbei werden Ähnlichkeitswerte bestimmt und Matches sowie
Non-Matches klassifiziert. Abschließend findet noch die Berechnung der
transitiven Hülle statt, um beispielsweise aus Paaren von Matches Gruppen von
Datensätzen zu finden, welche derselben Entität entsprechen.

## Statisches Blocking

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|*|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}*|A|*(|A|-1)$ Vergleiche durchgeführt werden. In beiden Fällen ist
die Anzahl der Vergleiche quadratisch zur Eingabemenge [@Kol:Effiziente:14]. In
der Studie [@KTR:Evaluation:10] zeigen Köpcke et al., dass das kartesische
Produkt für größe Datenmengen nicht skaliert. Aus diesem Grund reduzieren
moderne Entity Resolution Frameworks den Suchraum auf die wahrscheinlichsten
Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden zur Reduzierung des
quadratischen Suchraum werden übergreifend als Blockingmethoden bezeichnet.
Neben Blocking werden auch Windowing- und Indexing Verfahren eingesetzt. Während
Blockingverfahren die Anzahl der notwendigen Vergleiche drastisch reduzieren,
indem Non-Matches ausgeschlossen werden, besteht dennoch die Gefahr, dass
fälschlicherweise tatsächliche Matches ausgefiltert werden. Daher ist es
notwendig die Güte des Blockingverfahrens zu bestimmen. Dazu werden zwei
Kennziffern erhoben. Zum einen die *Reduction Ratio*, welche die Reduzierung der
Vergleiche im Gegensatz zum Kartesischen Produkt ausdrückt, sowie die *Pairs
Completeness*, welche den Anteil der tatsächlich ausgewählten Duplikate, die
sich nach dem Blocking in der Kandidatenmenge befinden, beschreibt.

Prinzipelle erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" voneinander einfinden. Zur
Durchführung der Gruppierung oder Sortierung müssen sog. Block- bzw.
Sortierschlüssel für jeden Datensatz erzeugt werden. Diese Schlüssel werden von
den Attributwerten oder einem Teil der Attributwerte abgeleitet und stellen eine
Signatur des Datensatzes dar. Eine beliebte Variante für Schlüssel sind etwa
phoenitische Enkodierung.

### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee ist eine Menge von Datensätzen in disjunkte
Partitionen (genannt Blöcke) zu teilen. Anschließend werden nur die Datensätze
des jeweiligen Blocks miteinander verglichen. Dazu wird jedem Datensatz ein
Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens hängt daher
maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und Größe der
Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
@fig:sb_blocking zeigt die Ausführung des Blockingverfahrens beispielhaft an
einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein Blockschlüssel
(hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses Schlüssels gruppiert.
Die Größe der einzelnen Blöcke bestimmt die Reduktion Ratio. Diese hängt
allerdings immer von der Datenquelle ab und kann daher nicht pauschalisiert
werden. Bei der Generierung der Blockschlüssel können fehlerhafte Werte
einzelner Attribute dazu führen, dass Duplikate in unterschiedlichen Blöcken
landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden Datensatz
mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert werden.
Dieser Ansatz nennt sich Multi-pass Blocking.

![Beispielhafte Standard Blocking Ausführung. Nach [@Kol:Effiziente:14].](pictures/standard_blocking.pdf){#fig:sb_blocking}

### Q-gram Indexing

Das Q-gram Indexing basiert auf der Idee Datensätze unterschiedlicher aber
ähnlicher Blockschlüssel miteinander zu vergleichen. Ein Blockschlüssel wird
dazu in eine Liste $G$ von q-Grammen überführt. Ein q-Gram ist ein Substring der
Länge q des ursprünglichen Blockschlüssels. Alle Permutationen der q-Gram Liste
mit einer Mindestlänge $l=max(1,\floor{\#G*t})$ werden konkateniert und dienen
als Schlüssel der Blöcke, wobei $t$ ein Schwellwert zwischen 0 und 1 ist. Dabei
werden Datensätze mehreren Blöcken zugewiesen. Ist $t=1$ entspricht Q-gram
Indexing dem Standard Blocking. Dieses Verfahren kann als Alternative zum
Multi-pass Verfahren beim Standard Blocking genutzt werden. Der große Nachteil
ist der hohe Aufwand bei der Berechnung aller möglichen Sublisten. Ein
Blockschlüssel mit $n$ Zeichen muss in $k=n-q+1$ q-Gramme zerlegt werden.
Insgesamt müssen dadurch $\sum_{i=max\{1,[k*t]\}}^{k} {k \choose i}$ Sublisten
berechnet werden [@Chr:Survey:12].

### Suffix Array Indexing

Das Suffix Array Indexing [@AO:Fast:05a] leitet, ähnlich wie Q-gram Indexing,
mehrere Schlüssel aus einem Blockschlüssel ab. Grundidee ist es alle Suffixe mit
einer Mindestlänge von $l$ zu bestimmen. Ein Datensatz mit Blockschlüssellänge
$n$ wird in $n-l+1$ Blöcke eingeordnet. Ist $n<l$ wird der Ausgangsschlüssel als
einziger Schlüssel verwendet. Durch die größere Menge an Kandidatenpaaren ist
i.Allg. die *Pair Completeness* höher (vgl. Multi-pass). Zudem ist der Aufwand
der Berechnung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer. Im
Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren jedoch
deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze
unnötigerweise mehrfach miteinander verglichen werden hoch. Deshalb werden aus
Blöcken, welche einen bestimmten Schwellwert überschreiten alle Datensätze
entfernt, die min. einen weiteren längeren Blockschüssel haben.

### Sorted Neighborhood

Das Sorted Neighborhood Verfahren, ist ein Sortierverfahren, welches 1995 von
Hernández & Stolfo [@HS:Merge:95] zur Erkennung von Dupliakten in
Datenbanktabellen vorgestellt wurde. Es besteht aus drei Phasen. Zunächst
bekommt jeder Datensatz einen Sortierschlüssel zugewiesen. Dabei muss der
Sortierschlüssel nicht einzigartig sein. Um die Berechnung des Schlüssels gering
zu halten, soll dieser durch Verkettung von Attributen bzw. Teilen der Attribute
bestimmt werden. Attribute die vorne im Schlüssel stehen haben dadurch eine
höhere Priorität. In der zweiten Phase werden die Datensätze anhand des
Schlüssels sortiert. In der dritten Phase wird ein Fenster (engl. Window) über
die sortierten Datensätze geschoben und alle Datensätze innerhalb des Windows
werden miteinander verglichen. Dieses Verfahren eignet sich besonders gut zur
Erkennung von Duplikaten innerhalb einer Datenquelle. Sollen Duplikate in
mehreren Datenquellen gefunden werden, müssen die Einträge beim Sortieren
gemischt werden. Dadurch besteht allerdings die Gefahr, das vorrangig Datensätze
einer Datenquelle miteinander verglichen werden. Vorteil zum Standard Blocking
ist, dass die Anzahl der Vergleiche lediglich von der Größe der Datenquelle und
der gewählten Fenstergröße abhängen. Ein großer Nachteil ist, dass Datensätze
die sich in der ersten Stelle des Schlüssels unterscheiden, weit voneinander
entfernt sind und dadurch nicht als Matches identifiziert werden. Um dennoch
eine hohe Pair Completeness zu erreichen, werden mehrere Schlüssel pro Datensatz
generiert und ein Fenster mit kleiner Größe über die verschieden sortierten
Listen geschoben. Dieses Verfahren entspricht im Grunde dem Multi-pass Verfahren
beim Standard Blocking.

Ein großes Problem bei der klassischen und der Multi-pass Variante des Sorted
Neighborhood Verfahrens ist, dass die zu wählende Fenstergröße $w$ größer als
die Anzahl der Datensätze mit dem am häufigsten vorkommenden Sortierschlüssel
sein muss, um eine gute Pair Completeness zu erreichen. Sei $n$ die Menge an
Datensätzen mit dem am häufigsten vorkommenden Schlüssel $k$ und $m$ die Menge
der Datensätze des darauffolgenden Schlüssels $k+1$, dann ist $w=n+m$. Nur
dadurch kann sichergestellt werden, dass alle Datensätze aus $n$ mit den "nahen"
Datensätzen aus $m$ verglichen werden. Da Sortierschlüssel für gewöhnlich nicht
gleich verteilt sind, gibt es meist wenige Größe und viele kleine Mengen an
Datensätzen mit dem gleichen Sortierschlüssel. Dadurch werden Datensätze mit
seltenen Sortierschlüsseln unnötig oft mit "weit" entfernten Datensätzen
verglichen. Zudem dominiert der am häufigsten vorkommenden Schlüssel, genauso
wie beim Standard Blocking, die Ausführungszeit des Algorithmus.

In [@DN:comparison:09] schlagen Draisbach & Naumann eine optimierte Variante des
Sorted Neighborhood Verfahrens vor. Dabei zeigen Sie, dass Standard Blockung und
Sorted Neighborhood extreme von Overlapping bei Partitionen sind. Ihre Idee ist
es diese Überlappung zu optimieren. Dabei soll die Überlappung groß genug sein,
um tatsächliche Matches zu finden, aber gering genug, um die Menge der
Vergleiche zu reduzieren. Zunächst wird wie beim klassischen Verfahren zu
sortiert. Danach werden angrenzende Datensätze in disjunkte Partitionen zerlegt
und schließlich wird ein Überlappungsfaktor (genannt Overlap) $u$ gewählt.
Innerhalb jedes Blockes wird analog zum Standard Blocking jeder Datensatz mit
jedem anderen verglichen. Innerhalb des Overlap-Window $w=u+1$, wird jeweils das
erste Element mit allen anderen Verglichen. Ist $w=0$ entspricht das Verfahren
Standard Blocking und hat jede Partition nur ein Element entspricht es der
Sorted Neighborhood Methode. Um zu vermeiden, dass eine Partition dominiert,
können größere Partitionen in Subpartitionen geteilt werden.

Weitere Varianten verändern die Fenstergröße anhand identifizierte Duplikate
[@DNSW:Adaptive:12].

### Canopy Clustering

Die Idee von Canopy Clustering ist es, Datensätze anhand einer einfachen
Vergleichsmetrik in überlappende Cluster (=Canopies) zu partionieren. Zur
Generierung wird eine Kandidatenliste gebildet, welche initial als allen
Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen Clusters
gewählte und alle Datensätze innerhalb des Mindestabstandes $d_1$ zugewiesen.
Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren
Mindestabstandes $d_2 < d_1$ aus der Kandidatenliste entfernt. Dieser
Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die *Pair
Completeness* hängt hierbei stark der gewählten Abstandsfunktion ab.
Anschließend wird werden alle Datensätze eines Cluster miteinander verglichen.

## Dynamisches Blocking

Für die Dupliaktserkennung in einer Datenquelle $A$, sind bei einer Anfrage $|A|
- 1$ Vergleiche notwendig. Da dies zu ungewollt hohen Latenzen führen würde,
  werden auch im dynamischen Fall Blocking Verfahren eingesetzt.

### DySimII

DySimII [@RCL.EA:Dynamic:13] ist die dynamische Variante des Similarity Aware
Index von Christen & Gayler [@CG:Scalable:08], welcher es zusätzlich erlaubt den
Index während der Laufzeit zu erweitern. Dabei ist die Grundidee die benötigten
Ähnlichkeiten vorauszuberechnen, um während der Laufzeit diese nur nachschlagen
zu müssen.

Der Index besteht aus drei Teilen. Dem **Record Index (RI)**, welcher alle
Attribute speichert und diese ihren Datensätzen zuordnet, dem **Block Index
(BI)**, welcher Attribute anhand einer Enkodierungsfunktion gruppiert und
zuletzt dem **Similarity Index (SI)**, welcher dieselben Schlüssel wie der
Record Index verwendet und die Ähnlichkeiten der Attribute im gleichen Block
hält. @fig:dysim_example zeigt ein Beispiel eines DySimII Index. Im Record Index
wurden die Datensatzidentifier von Tony (r1, r3) und Cathrine (r2, r6) als
Attributsübereinstimmung gruppiert. Anschließend wurden im Block Index über die
Double-Metaphone Enkodierung ähnliche Schreibweisen von Tony und Cathrine
zusammengeführt, was dem Standard Blocking entspricht. Im Similarity Index
wurden die Ähnlichkeiten von (Tony, Tonia und Tonya) bzw. (Cathrine, Kathryn),
welche sich in einem gemeinsamem Block befinden, untereinander mit ihren
berechneten Ähnlichkeiten verknüpft.

![DySimII Beispiel. Aus [@RCL.EA:Dynamic:13].](pictures/dysim_example.png){#fig:dysim_example}

Das Einfügen von Datensätzen läuft nach folgendem Schema ab. Zunächst werden
alle Attribute mit Verweis auf den Datensatzidentifier im Record Index
gespeichert. Falls ein Attribut dort schon existiert wird lediglich der
Identifier angefügt. Anschließend wird für jedes Attribut eine Enkodierung
bestimmt. Anhand dieser Enkodierung werden die Attribute in jeweils einen Block
im Block Index eingefügt. Beinhaltet der Block mehr als ein Attribut wird zu
allen Attributen die Ähnlichkeit bestimmt und die Ähnlichkeit des eingefügten
Attributes mit der verglichenen Attribut in den Similarity Index eingefügt.
Gleichzeitig wird die bestimmte Ähnlichkeit des eingefügten Attributes auch zu
dem verglichenen Attribute im Similarity Index ergänzt.

Bei einer Anfrage wird zunächst der neue Datensatz dem Index hinzugefügt.
Anschließend werden aus dem Record Index alle Identifier ausgelesen, welche ein
gleiches Attribute besitzen und werden in einen Akkumulator mit dem
Ähnlichkeitswert 1 aufgenommen. Bei mehreren gleichen Identifiern werden die
Ähnlichkeitswerte addiert. Anschließend werden die Attribute des
Anfragedatensatzes im Similarity Index nachgeschlagen und alle Attribute des
gleichen Blockes mit ihrer Ähnlichkeit ausgelesen. Zu diesen Attributen werden
aus dem Record Index die Identifier abgefragt und mit ihrer Ähnlichkeit aus dem
Similarity Index im Akkumulator aufsummiert.

Im Gegensatz zum Standard Blocking können Anfragen deutlich schneller
beantwortet werden, da im Optimalfall keine Ähnlichkeitsberechnung stattfinden
muss und lediglich Werte nachgeschlagen werden. Auf der negativen Seite steht
hingegen der deutlich erhöhte Speicherbedarf, welcher durch das Halten der
Ähnlichkeitswerte zurückzuführen ist.

### Similarity-Aware Index with Local Sensitive Hashing (LSH)

Dieses Verfahren ist eine Erweiterung des DySimII durch LSH, welches von Li et
al. [@LLRo:Two:13] vorgestellt wurde. Die hier genutzte Variante des Local
Sensitive Hashing nutzt das Minhash Verfahren. Minhashing ist eine effiziente
Abschätzung der Überlappung zweier Mengen bekannt als Jaccard-Ähnlichkeit.
Mittels des Minhash-Algorithmus ist es möglich für jeden Datensatz $n$
Signaturen der Länge $k$ zu generieren. Dazu werden $n$ verschieden zufällig
gewählte Hashfunktionen genutzt. Um die Wahrscheinlichkeit zu erhöhen, dass nur
gleiche Paar die selbe Signatur haben wird eine Technik namens Banding genutzt.
Dazu werden $l$ Signaturen zu einem Band zusammengefügt und damit verundet.
Mehrere Bänder sind logisch gesehen eine Veroderung.

Beim Erzeugen des Index werden zunächst die Minhash Signaturen erzeugt und zu
Bändern verundet. Anschließend werden die Datensatzidenfier wird mit den
erzeugten Bändern verknüpft. Dazu wird ein Index erstellt, welcher als Schlüssel
zunächst die verschieden Bänder hat. Innerhalb der Bänder gibt es weitere
Subindicies, welche als Schlüssel die Minhash Signaturen haben. Den jeweiligen
Signaturen innerhalb der Bänder wird der Datensatzidentifier zugewiesen. Dadurch
sind gleiche Signaturen durch die Bänder getrennt, was die Wahrscheinlichkeit
erhöht, dass unähnliche Datensatze eine gemeinsame Signatur im Index haben. Der
LSH Index ersetzt dadurch den Record Index. Die Schritte zum Einfügen in den
Block Index bzw. den Similarity Index sind analog zum DySimII.

Für die Beantwortung einer Anfrage werden zunächst für den neuen Datensatz die
Minhash Signaturen und Bänder erzeugt und mit Datensatzidentifier in den LSH
Index eingefügt. Dannach werden die Datensatzidenfier mit gleichen Signaturen in
den gleichen Bändern als Kandidatenmenge ausgelesen. Nun müssen die Attribute
der Kandidaten aus einer Datenquelle geladen werden. Mit diesen Attributen
können aus dem Similarity Index die Ähnlichkeitswerte jedes Kandidaten bestimmt
werden. Die Kandidaten, welche aus dem LSH Index erhalten wurden haben
allerdings nicht zwingend Attribute in denselben Blöcken im Block Index wie der
Anfragedatensatzes. Deshalb können zu einigen Attributen keine vorberechneten
Ähnlichkeiten aus dem Similarity Index bezogen werden. Da das berechnen zur
Laufzeit zu lange dauert, werden diese mit dem Ähnlichkeitswert
0 miteinberechnet. Dies mindert zwar die Genauigkeit etwas sorgt dennoch für
gute Latenzen.

Im Gegensatz zum DySimII ist die Berechnung des Index aufwendiger, da für jeden
Datensatz die Minhash Signaturen und Bänder berechnet werden müssen. Allerdings
ist die Kandidatenmenge potentielle deutlich geringer als beim DySimII, wodurch
die Anfragen schneller beantwortet werden.

### DySNI

## DNF-Blocking Schema nach [@KM:Unsupervised:13]

Das kleinste Element eines Blocking Schema ist eine *Indizierungsfunktion*
$h_i(x_t)$. Diese akzeptiert einen Attributwert eines Datensatzes und erzeugt
eine Menge $Y$, welche 0 oder mehr Blockschlüssel (engl. *blocking key value*,
kurz BKV) beinhaltet. Ein BKV identifiziert einen Block, welchem ein Datensatz
zugeordnet wird. (Beispiel)

Daraus folgt das  allgemeine Blockingpredikat (engl. *general blocking
predicate*). Das allgemeine Blockingpredikat $p_i(x_{t_1}, x_{t_2})$ nimmt zwei
Attribute unterschiedlicher Datensatze $t_1$ und $t_2$ und nutzt die $i^te$
Indizierungsfunktion, um zwei Mengen $Y_1$ und $Y_2$ zu erzeugen. Das Predikat
ist wahr, wenn die beiden Mengen eine gemeinsame Schnittmenge haben. (Beispiel)

Das spezifische Blockingpredikat (engl. *specific blocking predicate*) ist ein
Paar $(p_i, f)$, dass ein allgemeine Blockingpredikat $p_i$ mit einem Attribute
$f$ verbindet. Dazu nimmt das spezifische Blockingpredikat zwei Datensätze $t_1$
und $t_2$ und wendet $p_i$ auf die entsprechenden Attribute $f_1$ und $f_2$ der
Datensätze an. (Beispiel)

Das DNF Blocking Schema $f_P$ ist eine Funktion, welche in der Disjunktiven
Normalform ohne Negation durch einer Menge von $P$ spezifischen
Blockingpredikaten erzeugt wird. Jeder Term in $f_P$ kann aus einer Konjunktion
von spezifischen Blockingpredikaten bestehen.

## Weak Labels

Das Lernen von BKVs, funktionierenden Ähnlichkeitsmaßen per Attribute und
Klassifikation von Paaren in Matches und None-Matches erfordert eine Menge von
klassifizierten Daten. Diese sind allerdings oft nicht vorhanden und das
Erstellen ist ein sehr aufwendiger Prozess, welcher von einem Domainexperten
durchgeführt werden muss.

Um dieses Problem zu lösen schlagen Kejriwal & Mirankern [@KM:Unsupervised:13]
ein Schema vor, um eine Menge schwach klassifizierter Daten zu generieren. Dabei
werden sowohl positive, als auch negative Datenpaare klassifiziert. Über zwei
Schranken kann der Benutzer festlegen wie ähnlich (obere Schranke $ut$) bzw. wie
verschieden (untere Schranke $lt$) die Paare sein sollen. Zunächst werden dazu
alle Attribute jedes Datensätzen in Token zerlegt. Anhand der Token wird ein
Standard Blocking durchgeführt, wobei jeder Datensatz in mehreren Blöcken
vertreten sein kann. Um innerhalb der Blöcke den Paarvergleichsaufwand zu
reduzieren, wird zusätzlich ein Fenster der Größe $c$ über den Block geschoben.
Dieses Verfahren ist ähnlich des Sorted Block Verfahren [@DN:generalization:11].
Dabei empfiehlt es sich die Daten vorher zu sortieren, um deterministische
Ergebnisse zu erzielen. Nachdem das Fenster über jeden Block geschoben wurde
steht die Menge möglicher Kandidatenpaare fest. Diese Paare werden nun mit der
TF/IDF-Ähnlichkeit ($sim$) [@AO:Fast:05a] verglichen. Diese ermöglicht, nachdem
die TF/IDF Statistik über die kompletten Daten erfasst worden ist, eine
Komplexität von $O(1)$, da lediglich die Werte des Paares nachgeschlagen werden
müssen. Ist $sim \geq ut$ wird das Paar positiv klassifiziert. Ebenso, ist $sim
< lt$ wird ein Paar negativ klassifiziert. Damit die Menge der klassifizierten
Daten nicht beliebig groß wird, kann der Benutzer festlegen, wie viele positive
bzw. negative Paare maximal erzeugt werden sollen. Dabei werden sowohl bei den
positiven als auch bei den negativen Paaren die besten $d$ bzw. $nd$ Paare
ausgewählt. Bei den negativen Paaren, soll dadurch verhindern, dass lediglich
Paare mit $sim \approx 0.0$ ausgewählt werden, da diese für gewöhnlich zu
niedrigen Klassifikationsraten führen. Die Gesamtkomplexität des Algorithmus ist
$O(n + nm + nm)$. Die Bestandteile sind $O(n)$ zur Erzeugung der TF/IDF
Statistik, $O(nm)$ zur Erzeugung der Blöcke ($m$ Anzahl der Attribute) und
$O(nm)$ zur Erzeugung der Kandidatenpaare.
