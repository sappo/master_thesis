# Grundlagen

## Entity Resolution

Die Methoden zur Duplikatserkennung stammen ursprünglich aus dem
Gesundheitsbereich und wurden erstmal 1969 von Felegi & Sunter [@FS:Theory:69]
formal formuliert. Je nach Fachgebiet gibt es unterschiedliche Fachbegriffe.
Statistiker und Epidemiologen sprechen von *record* oder *data linkage*, während
Informatiker das Problem unter *entity resolution*, *data* oder *field
matching*, *duplicate detection*, *object identification* oder *merge/purge*
kennen. Identifiziert werden sollen dabei beliebige Entitäten. Die Schwierigkeit
dabei ist allerdings, dass Entitäten nicht durch ein einzigartiges Attribut
identifiziert werden können. Zudem sind die Datensätze oft fehlerhaft,
beispielsweise durch Rechtschreibfehler oder unterschiedliche Konventionen. Die
Methoden zur Entity Resolution (ER) arbeiten meist auf Datensatzpaaren und
liefern als Ergebnis eine Menge von Übereinstimmungen. Damit eine
Übereinstimmung zwischen zwei oder mehr Entitäten festgestellt werden kann
müssen diese Verglichen werden und ein Ähnlichkeitswert (engl. similarity score)
bestimmt werden. Dieser Ähnlichkeitswert gibt die Intensität der Übereinstimmung
an.

Zur Bestimmung der Ähnlichkeit eines Datensatzpaares unterscheiden Elmagarmid et
al. [@EIV:Duplicate:07] zwischen Attributsvergleichs- (engl. field matching) und
Datensatzvergleichsmethoden (engl. record matching). Methoden zum
Attributsvergleich sind zeichenbasierend (edit distance, affine gap distance,
Jaro distance metric oder Q-gram distance), tokenbasierend (atomic strings,
Q-grams mit tf.idf), phonetisch (soundex) oder nummerisch. Die
Datensatzvergleichsmethoden sind probabilistisch (Naive Bayes), überwachtes bzw.
semi-überwachtes Lernen (SVMLight [@Joa:Svmlight:99], Markov Chain Monte Carlo
[@SD:Entity:06]), aktives Lernen (ALIAS [@SB:Interactive:02]), distanzebasierend
(siehe Attributsvergleich - Datensatz als konkatenierter String) oder
regelbasierend (AJAX [@GFS.EA:Declarative:01]). Damit verschieden
Vergleichmethoden miteinander kombiniert werden können, wird der
Ähnlichkeitswert für gewöhnlich auf einen Wert zwischen 0 und 1 normiert. Wobei
1 totale Übereinstimmung und 0 keine Übereinstimmung bedeutet.

Da es keine Methode zur Entity Resolution gibt, welche allen anderen überlegen
ist, wurde Ende der 90er Jahre begonnen Frameworks zu entwickeln, welche
verschiedene Methoden miteinander kombinieren. Einen Vergleich dieser Frameworks
wurde durch Köpcke & Rahm [@KR:Frameworks:10] durchgeführt. Ein Framework
besteht aus verschiedenen Matchern. Ein Matcher ist dabei ein Algorithmus,
welcher die Ähnlichkeit zweier Datensätze ermittelt. Ähnlich wie Elmagarmid et
al. unterscheiden Köpcke & Rahm zwischen attributs- und kontextbasierenden
Matchern. Als Kontext bezeichnen Sie die semantische Beziehung bzw. Hierarchie
zwischen den Attributen. Um die Matcher miteinander zu kombinieren nutzen die
Frameworks min. eine Matching Strategie. Durch die Match-Strategie sollen
verglichene Datensätze in Paare von Matches und None-Matches klassifiziert
werden. Eine Strategie ist, die Ähnlichkeitswerte verschiedener Matcher
nummerisch zu kombinieren, beispielsweise durch eine gewichtete Summe oder einen
gewichteten Durchschnitt. Ein anderer Ansatz ist regelbasierend. Eine einfache
Regel besteht aus einer logischen Verbindung und einer Match-Kondition,
beispielsweise einem Schwellenwert. Die dritte und komplexeste Strategie ist
workflow-basierend. Hierbei kann beispielsweise eine Sequenz von Matchern die
Ergebnisse iterativ einschränken. Grundsätzlich können Workflows beliebig
komplex werden. Einen passenden Workflow zu finden kann selbst Domainexperten
vor eine große Herausforderung stellen. Daher gibt es trainingbasierende Ansätze
passende Parameter für Matcher oder Kombinationsfunktionen (z.B. Gewicht für
Matcher) zu bestimmen. Solche Ansätze sind etwa, Naive Bayes, Logistic
Regression, Support Vector Maschine oder Decision Trees.

In der klassischen Variante arbeitet Entity Resolution auf statischen Daten.
D.h. das während des ER-Prozesses keine neuen Daten hinzukommen. Hierbei werden
die zwei Disziplinen Deduplizierung und Entity-Linking unterschieden. Die
Deduplizierung wird auf einer Datenquelle durchgeführt und hat den Zweck alle
Duplikate in dieser Datenquelle zu finden. Anschließend werden die gefundenen
Duplikate automatisch oder manuell zusammengeführt. Entity Linking hingegen wird
auf mindestens zwei verschiedenen Datenquellen durchgeführt. Das Ziel ist es,
nicht Duplikate zusammenzuführen, sondern Entitäten unter den Datenquellen zu
verlinken. Damit die Links eindeutig sind, wird vorausgesetzt, dass die
einzelnen Datenquellen dedupliziert sind.

Die Ausführung der Vergleichsmethoden ist enorm teuer, da diese das Kreuzprodukt
zweier Mengen bilden müssen. Dies führt zu einer quadratischen Komplexität,
welche dafür sorgt, dass bei großen Datenmengen die Ausführungszeit im
statischen Fall bzw. die Latenzen im dynamischen Fall unakzeptabel groß werden.
Um die Ausführungszeit zu reduzieren wird versucht den Suchraum auf die
wahrscheinlichsten Duplikatsvorkommen zu begrenzen. Diese Vorgehen werden als
Blocking oder Indexing bezeichnet.

![Vereinfachter Entity Resolution Workflow. Nach [@Kol:Effiziente:14].](pictures/entity_resolution_workflow.png){#fig:er_workflow}

@fig:er_workflow zeigt einen vereinfachten typischen Entity Resolution Workflow
für statisches ER. Zunächst werden die Datensätze vorverarbeitet, um typische
Fehler zu entfernen. Dazu gehört das Korrigieren von Rechtschreibfehler,
ignorieren von Groß- bzw. Kleinschreibung und ersetzten von bekannten
Abkürzungen. Durch die Vorverarbeitung kann die Qualität des Matchings
verbessert, indem verhindert wird, dass offensichtliche Abweichungen den
Ähnlichkeitswert beeinflussen. Der nächste Schritt, das Blocking, teilt die
Gesamtmenge in Submengen zur Reduzierung der Komplexität. Auf das Blocking folgt
das Matching hierbei werden Ähnlichkeitswerte bestimmt und Matches sowie
Non-Matches klassifiziert. Abschließend findet noch die Berechnung der
transitiven Hülle statt, um beispielsweise aus Paaren von Matches Gruppen von zu
bilden, welche derselben Entität entsprechen.

Ein Großteil der Forschung in Entity Resolution konzentriert sich auf die
Qualität der Vergleichsergebnisse. Die von Köpcke & Rahm verglichenen Frameworks
konzentrieren sich alle Samt darauf zwei statische Mengen zu miteinander
vergleichen. Bei großen Datenmengen kann dies durchaus mehrere Stunden dauern.
Daher gibt es in den letzten Jahre einige Ansätze und Frameworks, welche
MapReduce Algorithmen zum Skalieren nutzen [@KR:Parallel:13, @MAS:Graph:14].
Einen weiteren Ansatz die Laufzeit für nahe Echtzeit Anwendungen zu optimieren
präsentieren Whang et al. [@WMG:PayAsYouGo:13]. Anstatt eine
Übereinstimmungsmenge nach Abschluss eines Algorithmus zu liefern, zeigen Sie
Möglichkeiten partielle Ergebnisse während der Laufzeit des Algorithmus zu
erhalten. Dabei werden die Suchalgorithmen so modifiziert, dass zunächst die
Wahrscheinlichsten Kandidaten miteinander verglichen werden. Dabei wird in
relativ kurzer Zeit ein Großteil der Entitäten gefunden.

Neben den statischen Verfahren gibt es zunehmend Bedarf an dynamischen
Verfahren. Das Finden gleicher Enitäten erfolgt hier auf Anfrage, weshalb die
gesamte Datenmenge vorab nicht bekannt ist. Ramadan et al. [@RCLG:Dynamic:15]
formulieren die Problemstellung für dynamische ER-Verfahren folgendermaßen:

$$M_{q_j} = \{r_i|r_i.id = q_j.id, r_i \in R\}, M_{q_j} \subseteq R, q_j \in Q$$ {#eq:dyer}

@eq:dyer beschreibt, dass für jeden Anfragedatensatz $q_j$ eines Anfragestroms
$Q$ alle Datensätze $M_{q_j}$ in $R$ gefunden werden sollen, welche die selbe
Enität wie $q_j$ beschreiben. Beispielsweise mussen Kreditauskunfteien auf
Anfrage prüfen, ob ein Kunde kreditwürdig ist. Dazu müssen die passendend
Entitäten möglichst schnell gefunden werden, um eine Entscheidung treffen zu
können. Zudem ist es notwending eine Historie der unveränderten Anfragen aller
Entität vorzuhalten, da die Beweise über frühere Anfragen liefern. Die
Herausforderung für dynamische ER-Verfahren ist weiter nach Ramadan et al.
Indexing-Verfahren zu entwicklen, welche es erlauben den Index dynamische zu
erweitern und eine kleine Zahl qualitativer Ergebnisse in nahe Echtzeit
(Subsekundenbereich) zu liefern. Ein dynamisches ER System ist ähnlich einer
Suchmaschine, doch anstatt einer gewerteten Liste möglicher Treffer, soll es
alle gleichen Entitäten finden, welche zur Anfrage passen. Das bedeutet
insbesondere, dass die Anfrage die gleiche Datenstruktur haben muss, wie die zu
durchsuchende Datenquelle. Erste Ergebnisse Entity Resolution in nahe Echtzeit
zu erreichen, präsentieren Christen & Gayler in [@CG:Scalable:08], unter
Verwendung von Inverted Indexing Techniken, welche normalerweise bei der
Websuche Anwendung finden.

## Statisches Blocking

Für die Duplikatserkennung in zwei Datenquellen $A$ und $B$ sind $|A|*|B|$
Paarvergleiche notwendig. Bei einer einzelnen Datenquelle $A$ müssen
$\dfrac{1}{2}*|A|*(|A|-1)$ Vergleiche durchgeführt werden. In beiden Fällen ist
die Anzahl der Vergleiche quadratisch zur Eingabemenge [@Kol:Effiziente:14]. In
der Studie [@KTR:Evaluation:10] zeigen Köpcke et al., dass das kartesische
Produkt für größe Datenmengen nicht skaliert. Aus diesem Grund reduzieren
moderne Entity Resolution Frameworks den Suchraum auf die wahrscheinlichsten
Kandidaten, die sogenannten Match-Kandidaten. Diese Methoden zur Reduzierung des
quadratischen Suchraum werden übergreifend als Blockingmethoden bezeichnet.
Neben Blocking werden auch Windowing- und Indexing Verfahren eingesetzt. Während
Blockingverfahren die Anzahl der notwendigen Vergleiche drastisch reduzieren,
indem Non-Matches ausgeschlossen werden, besteht dennoch die Gefahr, dass
fälschlicherweise tatsächliche Matches ausgefiltert werden. Daher ist es
notwendig die Güte des Blockingverfahrens zu bestimmen. Dazu werden zwei
Kennziffern erhoben. Zum einen die *Reduction Ratio*, welche die Reduzierung der
Vergleiche im Gegensatz zum Kartesischen Produkt ausdrückt, sowie die *Pairs
Completeness*, welche den Anteil der tatsächlich ausgewählten Duplikate, die
sich nach dem Blocking in der Kandidatenmenge befinden, beschreibt.

Prinzipelle erfolgt Blocking entweder durch Gruppierung oder Sortierung. Dadurch
sollen sich mögliche Duplikate in der "Nähe" voneinander einfinden. Zur
Durchführung der Gruppierung oder Sortierung müssen sog. Block- bzw.
Sortierschlüssel für jeden Datensatz erzeugt werden. Diese Schlüssel werden von
den Attributwerten oder einem Teil der Attributwerte abgeleitet und stellen eine
Signatur des Datensatzes dar. Eine beliebte Variante für Schlüssel sind etwa
phoenitische Enkodierung.

### Standard Blocking

Standard Blocking ist eine der ersten und populärsten Blockingmethoden
[@FS:Theory:69]. Die Idee ist eine Menge von Datensätzen in disjunkte
Partitionen (genannt Blöcke) zu teilen. Anschließend werden nur die Datensätze
des jeweiligen Blocks miteinander verglichen. Dazu wird jedem Datensatz ein
Blockschlüssel zugeordnet. Die Qualität des Blockingverfahrens hängt daher
maßgeblich vom gewählten Blockschlüssel ab, da dieser die Anzahl und Größe der
Partitionen bestimmt. In einer Menge von Personen ist ein schlechter
Blockschlüssel etwa das Geschlecht. Da dieser die Menge lediglich in zwei große
Partitionen teilt. Ein besserer Blockschlüssel ist beispielsweise die
Postleitzahl oder die ersten Ziffern der Postleitzahl [@DN:comparison:09].
@fig:sb_blocking zeigt die Ausführung des Blockingverfahrens beispielhaft an
einer Datenquelle $S$. Zunächst wird jedem Datensatz (a - i) ein Blockschlüssel
(hier 1, 2, 3) zugeordnet. Anschließend wird anhand dieses Schlüssels gruppiert.
Die Größe der einzelnen Blöcke bestimmt die Reduktion Ratio. Diese hängt
allerdings immer von der Datenquelle ab und kann daher nicht pauschalisiert
werden. Bei der Generierung der Blockschlüssel können fehlerhafte Werte
einzelner Attribute dazu führen, dass Duplikate in unterschiedlichen Blöcken
landen. Damit diese Duplikate dennoch gefunden werden, kann für jeden Datensatz
mehrere Blockschlüssel, anhand unterschiedlicher Attribute, generiert werden.
Dieser Ansatz nennt sich Multi-pass Blocking.

![Beispielhafte Standard Blocking Ausführung. Nach [@Kol:Effiziente:14].](pictures/standard_blocking.pdf){#fig:sb_blocking}

### Q-gram Indexing

Das Q-gram Indexing basiert auf der Idee Datensätze unterschiedlicher aber
ähnlicher Blockschlüssel miteinander zu vergleichen. Ein Blockschlüssel wird
dazu in eine Liste $G$ von q-Grammen überführt. Ein q-Gram ist ein Substring der
Länge q des ursprünglichen Blockschlüssels. Alle Permutationen der q-Gram Liste
mit einer Mindestlänge $l=max(1,\floor{\#G*t})$ werden konkateniert und dienen
als Schlüssel der Blöcke, wobei $t$ ein Schwellwert zwischen 0 und 1 ist. Dabei
werden Datensätze mehreren Blöcken zugewiesen. Ist $t=1$ entspricht Q-gram
Indexing dem Standard Blocking. Dieses Verfahren kann als Alternative zum
Multi-pass Verfahren beim Standard Blocking genutzt werden. Der große Nachteil
ist der hohe Aufwand bei der Berechnung aller möglichen Sublisten. Ein
Blockschlüssel mit $n$ Zeichen muss in $k=n-q+1$ q-Gramme zerlegt werden.
Insgesamt müssen dadurch $\sum_{i=max\{1,[k*t]\}}^{k} {k \choose i}$ Sublisten
berechnet werden [@Chr:Survey:12].

### Suffix Array Indexing

Das Suffix Array Indexing [@AO:Fast:05a] leitet, ähnlich wie Q-gram Indexing,
mehrere Schlüssel aus einem Blockschlüssel ab. Grundidee ist es alle Suffixe mit
einer Mindestlänge von $l$ zu bestimmen. Ein Datensatz mit Blockschlüssellänge
$n$ wird in $n-l+1$ Blöcke eingeordnet. Ist $n<l$ wird der Ausgangsschlüssel als
einziger Schlüssel verwendet. Durch die größere Menge an Kandidatenpaaren ist
i.Allg. die *Pair Completeness* höher (vgl. Multi-pass). Zudem ist der Aufwand
der Berechnung der Schlüssel im Gegensatz zu Q-grammen deutlich geringer. Im
Gegensatz zum Standard Blocking ist die Menge an Kandidatenpaaren jedoch
deutlich höher. Dadurch ist auch die Wahrscheinlichkeit, dass zwei Datensätze
unnötigerweise mehrfach miteinander verglichen werden hoch. Deshalb werden aus
Blöcken, welche einen bestimmten Schwellwert überschreiten alle Datensätze
entfernt, die min. einen weiteren längeren Blockschüssel haben.

### Sorted Neighborhood

Das Sorted Neighborhood Verfahren, ist ein Sortierverfahren, welches 1995 von
Hernández & Stolfo [@HS:Merge:95] zur Erkennung von Dupliakten in
Datenbanktabellen vorgestellt wurde. Es besteht aus drei Phasen. Zunächst
bekommt jeder Datensatz einen Sortierschlüssel zugewiesen. Dabei muss der
Sortierschlüssel nicht einzigartig sein. Um die Berechnung des Schlüssels gering
zu halten, soll dieser durch Verkettung von Attributen bzw. Teilen der Attribute
bestimmt werden. Attribute die vorne im Schlüssel stehen haben dadurch eine
höhere Priorität. In der zweiten Phase werden die Datensätze anhand des
Schlüssels sortiert. In der dritten Phase wird ein Fenster (engl. Window) über
die sortierten Datensätze geschoben und alle Datensätze innerhalb des Windows
werden miteinander verglichen. Dieses Verfahren eignet sich besonders gut zur
Erkennung von Duplikaten innerhalb einer Datenquelle. Sollen Duplikate in
mehreren Datenquellen gefunden werden, müssen die Einträge beim Sortieren
gemischt werden. Dadurch besteht allerdings die Gefahr, das vorrangig Datensätze
einer Datenquelle miteinander verglichen werden. Vorteil zum Standard Blocking
ist, dass die Anzahl der Vergleiche lediglich von der Größe der Datenquelle und
der gewählten Fenstergröße abhängen. Ein großer Nachteil ist, dass Datensätze
die sich in der ersten Stelle des Schlüssels unterscheiden, weit voneinander
entfernt sind und dadurch nicht als Matches identifiziert werden. Um dennoch
eine hohe Pair Completeness zu erreichen, werden mehrere Schlüssel pro Datensatz
generiert und ein Fenster mit kleiner Größe über die verschieden sortierten
Listen geschoben. Dieses Verfahren entspricht im Grunde dem Multi-pass Verfahren
beim Standard Blocking.

Ein großes Problem bei der klassischen und der Multi-pass Variante des Sorted
Neighborhood Verfahrens ist, dass die zu wählende Fenstergröße $w$ größer als
die Anzahl der Datensätze mit dem am häufigsten vorkommenden Sortierschlüssel
sein muss, um eine gute Pair Completeness zu erreichen. Sei $n$ die Menge an
Datensätzen mit dem am häufigsten vorkommenden Schlüssel $k$ und $m$ die Menge
der Datensätze des darauffolgenden Schlüssels $k+1$, dann ist $w=n+m$. Nur
dadurch kann sichergestellt werden, dass alle Datensätze aus $n$ mit den "nahen"
Datensätzen aus $m$ verglichen werden. Da Sortierschlüssel für gewöhnlich nicht
gleich verteilt sind, gibt es meist wenige Größe und viele kleine Mengen an
Datensätzen mit dem gleichen Sortierschlüssel. Dadurch werden Datensätze mit
seltenen Sortierschlüsseln unnötig oft mit "weit" entfernten Datensätzen
verglichen. Zudem dominiert der am häufigsten vorkommenden Schlüssel, genauso
wie beim Standard Blocking, die Ausführungszeit des Algorithmus.

In [@DN:comparison:09] schlagen Draisbach & Naumann eine optimierte Variante des
Sorted Neighborhood Verfahrens vor. Dabei zeigen Sie, dass Standard Blockung und
Sorted Neighborhood extreme von Overlapping bei Partitionen sind. Ihre Idee ist
es diese Überlappung zu optimieren. Dabei soll die Überlappung groß genug sein,
um tatsächliche Matches zu finden, aber gering genug, um die Menge der
Vergleiche zu reduzieren. Zunächst wird wie beim klassischen Verfahren zu
sortiert. Danach werden angrenzende Datensätze in disjunkte Partitionen zerlegt
und schließlich wird ein Überlappungsfaktor (genannt Overlap) $u$ gewählt.
Innerhalb jedes Blockes wird analog zum Standard Blocking jeder Datensatz mit
jedem anderen verglichen. Innerhalb des Overlap-Window $w=u+1$, wird jeweils das
erste Element mit allen anderen Verglichen. Ist $w=0$ entspricht das Verfahren
Standard Blocking und hat jede Partition nur ein Element entspricht es der
Sorted Neighborhood Methode. Um zu vermeiden, dass eine Partition dominiert,
können größere Partitionen in Subpartitionen geteilt werden.

Weitere Varianten verändern die Fenstergröße anhand identifizierte Duplikate
[@DNSW:Adaptive:12].

### Canopy Clustering

Die Idee von Canopy Clustering ist es, Datensätze anhand einer einfachen
Vergleichsmetrik in überlappende Cluster (=Canopies) zu partionieren. Zur
Generierung wird eine Kandidatenliste gebildet, welche initial als allen
Datensätzen besteht. Dann wird zufällig ein Zentroid eines neuen Clusters
gewählte und alle Datensätze innerhalb des Mindestabstandes $d_1$ zugewiesen.
Zusätzlich werden alle Datensätze dieses Clusters mit einem weiteren
Mindestabstandes $d_2 < d_1$ aus der Kandidatenliste entfernt. Dieser
Algorithmus wird wiederholt, bis die Kandidatenliste leer ist. Die *Pair
Completeness* hängt hierbei stark der gewählten Abstandsfunktion ab.
Anschließend wird werden alle Datensätze eines Cluster miteinander verglichen.

## Dynamisches Blocking

Für die Dupliaktserkennung in einer Datenquelle $A$, sind bei einer Anfrage $|A|
- 1$ Vergleiche notwendig. Da dies zu ungewollt hohen Latenzen führen würde,
  werden auch im dynamischen Fall Blocking Verfahren eingesetzt.

### DySimII

DySimII [@RCL.EA:Dynamic:13] ist die dynamische Variante des Similarity Aware
Index von Christen & Gayler [@CG:Scalable:08], welcher es zusätzlich erlaubt den
Index während der Laufzeit zu erweitern. Dabei ist die Grundidee die benötigten
Ähnlichkeiten vorauszuberechnen, um während der Laufzeit diese nur nachschlagen
zu müssen.

Der Index besteht aus drei Teilen. Dem **Record Index (RI)**, welcher alle
Attribute speichert und diese ihren Datensätzen zuordnet, dem **Block Index
(BI)**, welcher Attribute anhand einer Enkodierungsfunktion gruppiert und
zuletzt dem **Similarity Index (SI)**, welcher dieselben Schlüssel wie der
Record Index verwendet und die Ähnlichkeiten der Attribute im gleichen Block
hält. @fig:dysim_example zeigt ein Beispiel eines DySimII Index. Im Record Index
wurden die Datensatzidentifier von Tony (r1, r3) und Cathrine (r2, r6) als
Attributsübereinstimmung gruppiert. Anschließend wurden im Block Index über die
Double-Metaphone Enkodierung ähnliche Schreibweisen von Tony und Cathrine
zusammengeführt, was dem Standard Blocking entspricht. Im Similarity Index
wurden die Ähnlichkeiten von (Tony, Tonia und Tonya) bzw. (Cathrine, Kathryn),
welche sich in einem gemeinsamem Block befinden, untereinander mit ihren
berechneten Ähnlichkeiten verknüpft.

![DySimII Beispiel. Aus [@RCL.EA:Dynamic:13].](pictures/dysim_example.png){#fig:dysim_example}

Das Verfahren unterscheitet zwei Phasen. die Bauphase (Build-Phase), in welcher
der Index aus einem initiallen Datenbestand erzeugt wird und die Anfragephase
(Query-Phase), welche Anfragen aus einem Datenstrom beantwortet.

**Build Phase**. Das Einfügen von Datensätzen läuft nach folgendem Schema ab.
Zunächst werden alle Attribute mit Verweis auf den Datensatzidentifier im Record
Index gespeichert. Falls ein Attribut dort schon existiert wird lediglich der
Identifier angefügt. Anschließend wird für jedes Attribut eine Enkodierung
bestimmt. Anhand dieser Enkodierung werden die Attribute in jeweils einen Block
im Block Index eingefügt. Beinhaltet der Block mehr als ein Attribut wird zu
allen Attributen die Ähnlichkeit bestimmt und die Ähnlichkeit des eingefügten
Attributes mit der verglichenen Attribut in den Similarity Index eingefügt.
Gleichzeitig wird die bestimmte Ähnlichkeit des eingefügten Attributes auch zu
dem verglichenen Attribute im Similarity Index ergänzt.

**Query Phase**. Bei einer Anfrage wird zunächst der neue Datensatz dem Index
hinzugefügt. Anschließend werden aus dem Record Index alle Identifier
ausgelesen, welche ein gleiches Attribute besitzen und werden in einen
Akkumulator mit dem Ähnlichkeitswert 1 aufgenommen. Bei mehreren gleichen
Identifiern werden die Ähnlichkeitswerte addiert. Anschließend werden die
Attribute des Anfragedatensatzes im Similarity Index nachgeschlagen und alle
Attribute des gleichen Blockes mit ihrer Ähnlichkeit ausgelesen. Zu diesen
Attributen werden aus dem Record Index die Identifier abgefragt und mit ihrer
Ähnlichkeit aus dem Similarity Index im Akkumulator aufsummiert.

Im Gegensatz zum Standard Blocking können Anfragen deutlich schneller
beantwortet werden, da im Optimalfall keine Ähnlichkeitsberechnung stattfinden
muss und lediglich Werte nachgeschlagen werden. Auf der negativen Seite steht
hingegen der deutlich erhöhte Speicherbedarf, welcher durch das Halten der
Ähnlichkeitswerte zurückzuführen ist.

### Similarity-Aware Index with Local Sensitive Hashing (LSH)

Dieses Verfahren ist eine Erweiterung des DySimII durch LSH, welches von Li et
al. [@LLRo:Two:13] vorgestellt wurde. Die hier genutzte Variante des Local
Sensitive Hashing nutzt das Minhash Verfahren. Minhashing ist eine effiziente
Abschätzung der Überlappung zweier Mengen bekannt als Jaccard-Ähnlichkeit.
Mittels des Minhash-Algorithmus ist es möglich für jeden Datensatz $n$
Signaturen der Länge $k$ zu generieren. Dazu werden $n$ verschieden zufällig
gewählte Hashfunktionen genutzt. Um die Wahrscheinlichkeit zu erhöhen, dass nur
gleiche Paar die selbe Signatur haben wird eine Technik namens Banding genutzt.
Dazu werden $l$ Signaturen zu einem Band zusammengefügt und damit verundet.
Mehrere Bänder sind logisch gesehen eine Veroderung. Auch diese Verfahren teilt
sich in Bau- und Anfragephase.

**Build Phase**. Beim Erzeugen des Index werden zunächst die Minhash Signaturen
erzeugt und zu Bändern verundet. Anschließend werden die Datensatzidenfier wird
mit den erzeugten Bändern verknüpft. Dazu wird ein Index erstellt, welcher als
Schlüssel zunächst die verschieden Bänder hat. Innerhalb der Bänder gibt es
weitere Subindicies, welche als Schlüssel die Minhash Signaturen haben. Den
jeweiligen Signaturen innerhalb der Bänder wird der Datensatzidentifier
zugewiesen. Dadurch sind gleiche Signaturen durch die Bänder getrennt, was die
Wahrscheinlichkeit erhöht, dass unähnliche Datensatze eine gemeinsame Signatur
im Index haben. Der LSH Index ersetzt dadurch den Record Index. Die Schritte zum
Einfügen in den Block Index bzw. den Similarity Index sind analog zum DySimII.

**Query Phase**. Für die Beantwortung einer Anfrage werden zunächst für den
neuen Datensatz die Minhash Signaturen und Bänder erzeugt und mit
Datensatzidentifier in den LSH Index eingefügt. Dannach werden die
Datensatzidenfier mit gleichen Signaturen in den gleichen Bändern als
Kandidatenmenge ausgelesen. Nun müssen die Attribute der Kandidaten aus einer
Datenquelle geladen werden. Mit diesen Attributen können aus dem Similarity
Index die Ähnlichkeitswerte jedes Kandidaten bestimmt werden. Die Kandidaten,
welche aus dem LSH Index erhalten wurden haben allerdings nicht zwingend
Attribute in denselben Blöcken im Block Index wie der Anfragedatensatzes.
Deshalb können zu einigen Attributen keine vorberechneten Ähnlichkeiten aus dem
Similarity Index bezogen werden. Da das berechnen zur Laufzeit zu lange dauert,
werden diese mit dem Ähnlichkeitswert 0 miteinberechnet. Dies mindert zwar die
Genauigkeit etwas sorgt dennoch für gute Latenzen.

Im Gegensatz zum DySimII ist die Berechnung des Index aufwendiger, da für jeden
Datensatz die Minhash Signaturen und Bänder berechnet werden müssen. Allerdings
ist die Kandidatenmenge potentielle deutlich geringer als beim DySimII, wodurch
die Anfragen schneller beantwortet werden.

### DySNI

Das DySNI Verfahren von Ramadan et al. [@RCLG:Dynamic:15] ist ein dynamische
Umsetzung des Sorted Neighborhood Verfahren aus dem statischen Blocking. Anstatt
eines Arrays verwenden Sie eine Baumstruktur, um Datasätze möglichst schnell zu
selektieren. Der gewählte beim ist ein BraidedTree (BRT), welcher ein
balanzierter binärer AVL-Baum ist. Innerhalb des Baumes hat jeder Knoten jeweils
einen Verweis auf seinen Vorgänger und seinen Nachfolger. Die Sortierung erfolgt
alphabetisch nach einem gewählten Sortierschlüssel. Ein Knoten besteht dabei aus
einem Sortierschlüsselwert (Sorting Key Value. kurz: SKV) und einer Liste an von
Datensätzes mit diesem SKV. Ein Anfrageknoten, ist derjenige in welchen ein
Anfragedatensatz eingefügt wurde, und wird mit $N_q$ bezeichnet.

**Build Phase**. Beim Einfügen eines neuen Datensatzes wird zunächst dessen SKV
erzeugt. Wenn der SKV noch nicht im BRT-Baum vorhanden ist, wird ein neuer
Knoten erzeugt und der Datensatzidentifier angehängt. Ist der Knoten bereits
vorhanden wird lediglich der Datensatzidentifier zum existierenden Knoten
hinzugefügt. Zusätzlich wird der Datensatz in einen Inverted Index $D$
eingefügt, um ihn zum Attributsvergleich mit anderen Datensätzen schnell
selektieren zu können.

**Query Phase**. Zunächst wird der Anfragedatensatz, nach dem Vorgehen aus der
Build Phase, eingefügt. Dannach ist bekannt welchem Knoten der Anfragedatensatz
zugeordnet wurde und das Fenster mit den benachbarten Knoten kann erzeugt
werden. Alle Datensätze, welche in Knoten innerhalb des Fenster gespeichert
sind, werden als Kandidatenmenge $C$ selektiert. Aus $D$ werden dann für jeden
Kandidaten seine Attribute geholt und anschließend in einem Paarvergleiche mit
dem Anfragedatensatz die Ähnlichkeit ermittelt. Für Erzeugung des Fenster werden
vier Methoden vorgestellt, welche sich an Varianten des statischen Sorted
Neighborhood Verfahrens orientieren.

* **Fixed Window Size** ist das einfachste Verfahren, bei welchem das Fenster um
  einen festen Wert $w$ in Vorgänger- und Nachfolgerrichtung aufgespannt wird.
* **Candidates-Based Adaptive Window** erweitert das Fenster abwechselnd in
  Vorgänger- und Nachfolgerrichtung, solange bis eine Mindestanzahl an
  Kandidaten gefunden wurde.
* **Similarity-Based Adaptive Window** nutzt die Ähnlichkeit zwischen SKVs.
  Dabei wird ein Fenster in eine Richtung solange erweitert bis die Ähnlichkeit
  zwischen dem SKV von $N_q$ und dem nächsten Vorgänger bzw. Nachfolger eine
  mindestähnlichkeit $\Delta$ unterschreitet.
* **Duplicate-Based Adaptive Window** erweitert das Fenster aus Basis gefundener
  Matches in beide Richtungen unabhängig. Dabei wird das Fenster um jeweils
  einen Knoten erweitert und zwischen dem Anfragedatensatz und den Datensätzen
  des neuen Knoten der Ähnlichkeitswert ermittelt, sowie klassifiziert, ob es
  sich um ein Match oder None-Match handelt. Sinkt die Anzahl an gefunden
  Matches unter eine Schranke $\delta$ wird das Fenster in diese Richtung nicht
  weiter vergrößert.

Damit Ähnlichkeiten zwischen den Datensätzen nicht jedes Mal neuberechnet werden
müssen. Schlagen die Authoren vor, je nach gewählter Fensterberechunung, die
Ähnlichkeit der SKVs zu berechnen und in den beteiligten Knoten abzuspeichern.
Dadurch wird allerdings die Auswahl an SKVs auf Konkatenationen von Attributen
beschränkt. Attribute die nicht im SKV genutzt wurden müssen bei diesem
Verfahren trotzdem jedes Mal neuberechnet werden.

Des Weiteren ist auch dieses Verfahren sensitiv auf Fehler am Anfang des SKV. Um
dies zu korrigieren wird, ähnlich zum Multi-pass Verfahren des Sorted
Neighborhood Verfahren, vorgeschlagen mehrere BRT-Bäume mit unterschiedlichen
SKVs zu erstellen.

In ihrer Auswertung zeigen die Authoren, dass das Duplikatsbasierende Verfahren
im BRT-Baum nicht funktioniert, weil ein Großteil der Duplikate im
Ausgangsknoten landet und damit eine Erweiterung des Fenster nicht zustande
kommt. Die besten Recall Ergenisse wurden mit Ähnlichkeitsbasierenden Verfahren
erreicht, da der Baum nach den SKVs sortiert wurde und dieses Fenster damit am
Besten passt. Das beiden anderen Verfahren ebenfalls gute Ergenisse und haben
den Vorteil, dass sich das Fenster und damit die Anzahl der Kandidaten gut
kontrollieren lässt und damit auch die Latenzen.

## Blocking Schema

Die Qualität aller Verfahren, ob statisch oder dynamische, hängt maßgeblich von
der Auswahl des richtigen Blocking Schema ab. Wie genau diese Schema auszuwählen
ist, wird von vielen Verfahren offen gelassen. Die meisten Verfahren schränken
jedoch ein, dass zu einem Datensatz nur ein Blockschlüssel oder Sortierschlüssel
erzeugt werden darf. Bei der Verwendung von Multi-pass Ansätzen werden
demensprechend verschiedene Schema gefordert. Ein funktionierendes Schema zu
finden ist oft, auch von Domainexperten, nur durch auspropieren herauszufinden.
Oft genutzt werden phonetische Enkodierung und Konkatenationen von Attributen.
Weitere Beispiele sind Q-Grame oder Suffixe aus den vorgestellten statischen
Verfahren.

### DNF-Blocking Schema

Um ein gutes Blocking Schema automatisch zu lernen schlagen Kejriwal & Miranker
[@KM:Unsupervised:13] ein Verfahren vor, welches ein Blocking Schema in
Disjunktiver Normalform erzeugt. Dieses Schema wird folgendermaßen definiert.

Das kleinste Element eines Blocking Schema ist eine *Indizierungsfunktion*
$h_i(x_t)$. Diese akzeptiert einen Attributwert eines Datensatzes und erzeugt
eine Menge $Y$, welche 0 oder mehr Blockschlüssel (engl. *blocking key value*,
kurz BKV) beinhaltet. Ein BKV identifiziert einen Block, welchem ein Datensatz
zugeordnet wird. Ein Beispiel einer Indizierungsfunktion ist *Tokens*. Tokens
zerlegt einen Eingabestring in eine Menge von Token mittels eines Trennzeichens
(z.B. Komma oder Leerzeichen).

Daraus folgt das allgemeine Blockingpredikat (engl. *general blocking
predicate*). Das allgemeine Blockingpredikat $p_i(x_{t_1}, x_{t_2})$ nimmt zwei
Attribute unterschiedlicher Datensatze $t_1$ und $t_2$ und nutzt die $i^{te}$
Indizierungsfunktion, um zwei Mengen $Y_1$ und $Y_2$ zu erzeugen. Das Predikat
ist wahr, wenn die beiden Mengen eine gemeinsame Schnittmenge haben. Angenommen
die $i^{te}$ Indizierungsfunktion ist *Tokens*, dann haben wir das Predikat
*EnthältGemeinsamenToken*, welche Wahr ist wenn zwei Attribute mindestens einen
gemeinsamem Token haben.

Das spezifische Blockingpredikat (engl. *specific blocking predicate*) ist ein
Paar $(p_i, f)$, dass ein allgemeines Blockingpredikat $p_i$ mit einem Attribute
$f$ verbindet. Dazu nimmt das spezifische Blockingpredikat zwei Datensätze $t_1$
und $t_2$ und wendet $p_i$ auf die entsprechenden Attribute $f_1$ und $f_2$ der
Datensätze an. Ein solche Paar ist beispielsweise (*EnthältGemeinsamenToken*,
*name*). Dieses spezifische Predikat ist wahr, wenn der Name zweier Datensätze
einen gemeinsamem Token enthält.

Das DNF Blocking Schema $f_P$ ist eine Funktion, welche in der Disjunktiven
Normalform ohne Negation durch eine Menge von $P$ Termen erzeugt wird. Jeder
Term in $f_P$ besteht aus mindestens einem spezifischen Blockingpredikat.
Mehrere spezifische Blockingpredikat in einem Term werden durch eine Konjunktion
verbunden. Das DNF Blocking Schema ist demensprechend Wahr, wenn einer seiner
Terme Wahr ist. Wahr für zwei Datensätze bedeutet, dass diese von einem
Blockingverfahren selektiert und verglichen werden würden. Dabei entspricht die
Konjunktion eines Terms dem Konkatenieren von Strings. Die Disjunktion der Terme
kann bei vielen Verfahren als Multi-pass Ansatz implementiert werden. Des
Weiteren ist zu beachten, dass das Blocking Schema potentiell mehrere Schlüssel
pro Datensatz erzeugt. Ein Beispiel eines solchen Schema ist
(*EnthältGemeinsamenToken*, Name) $\wedge$ (*ExakteÜbereinstimmung*, Stadt).

### Weak Labels

Das Lernen eines Blocking Schema erfordert eine Menge von klassifizierten Daten.
Diese sind allerdings oft nicht vorhanden und das Erstellen ist ein sehr
aufwendiger, langwieriger und teurer Prozess, welcher von Domainexperten
durchgeführt werden muss.

Deshalb haben Kejriwal & Mirankern [@KM:Unsupervised:13] ein Verfahren
entwickelt, um eine Menge schwach klassifizierter Daten zu generieren. Dabei
werden sowohl positive, als auch negative Datensatzpaare klassifiziert. Über
zwei Schranken kann der Benutzer festlegen wie ähnlich (obere Schranke $ut$)
bzw. wie verschieden (untere Schranke $lt$) die Paare sein sollen. Anschließend
wird ein Blocking der Daten per Standard Blocking und Sorted Neighborhood
durchgeführt. Zunächst werden dazu alle Attribute jedes Datensätzen in Token
zerlegt. Anhand der Token wird das Standard Blocking durchgeführt, wobei jeder
Datensatz in mehreren Blöcken vertreten sein kann. Um innerhalb der Blöcke den
Paarvergleichsaufwand zu reduzieren, wird zusätzlich ein Fenster der Größe $c$
über den Block geschoben, was der Sorted Neighborhood entspricht. Nachdem das
Fenster über jeden Block geschoben wurde steht die Menge möglicher
Kandidatenpaare fest. Diese Paare werden nun mit der TF/IDF-Ähnlichkeit ($sim$)
[@AO:Fast:05a] verglichen. Diese ermöglicht, nachdem die TF/IDF Statistik über
die kompletten Daten erfasst wurde, eine Komplexität von $O(1)$, da lediglich
die Werte des Paares nachgeschlagen werden müssen. Ist $sim \geq ut$ wird das
Paar positiv klassifiziert. Ebenso, ist $sim < lt$ wird ein Paar negativ
klassifiziert. Damit die Menge der klassifizierten Daten nicht beliebig groß
wird, kann der Anwender festlegen, wie viele positive $max_p$ bzw. negative
Paare $max_n$ maximal erzeugt werden sollen. Von allen positiven Paaren werden
abschließend die besten $d$ ausgewählt, aber maximal $max_p$. Analog werden
ebenfalls die besten $nd$ negative Paare gewählt. Bei den negativen Paaren soll
dadurch verhindern, dass lediglich Paare mit $sim \approx 0.0$ ausgewählt
werden, da diese für gewöhnlich zu niedrigen Klassifikationsraten führen. Die
Gesamtkomplexität des Algorithmus ist $O(n + nm + nm)$, welcher sich in die
Erzeugung der TF/IDF Statistik ($O(n)$) , die Erzeugung der Blöcke über $m$
Attribute ($O(nm)$) und die Erzeugung der Kandidatenpaare $O(nm)$ gliedert.

### Lernen des DNF-Blocking Schema

Der erste Schritt zum Lernen eines DNF-Blocking Schema ist, eine Liste an
spezifischen Blockingpredikaten zu benennen. Beispielsweise
*EnthältGemeinsamenToken* und *ExakteÜbereinstimmung* für Strings und
*Erste3Ziffern* für nummerische Attribute. Anschließend wird für jedes
spezifische Blockingpredikat, auf Basis der schwach klassifizierten Daten, ein
positiver $P_f$ und ein negativer $N_f$ boolscher Featurevektor erzeugt. Dabei
ist ein Wert innerhalb des Vektors Wahr, wenn das entsprechende Predikat für das
gewählte Datensatzpaar Wahr ist.

Anschließend werden die Terme des Blocking Schema erzeugt. Da potentiell
beliebig viele Terme erzeugt werden können, muss der Anwender die Tiefe, d.h.
Anzahl der Predikate pro Term, festlegen. Die Featurevektoren der Terme können
durch Konjunktion der Featurevektoren der einzelnen Predikate ohne große
Aufwände berechnet werden. Danach wird die Qualität der einzelnen Terme
bewertet. Dazu nutzen Kejriwal & Miranker die Fisher-Score. Die Idee der
Fisher-Score nach [@GLH:Generalized:12] ist, ein Untermenge von Features (hier:
Terme) zu finden, sodass die Datenpunkte der Klassen (hier: positive und
negative Labels) möglichst weit voneinander entfernt und gleichzeitig die
Datenpunkte innerhalb der Klasse möglichst nahe zusammen sind. Die Formel zur
Berechnung der Fisher-Score des $i^{ten}$ Terms sieht folgendermaßen aus:
$\rho_i = \frac{|P_{f,i}|(\mu_{p,i} - \mu_i)^2 + |N_{f,i}|(u_{\mu,i} -
\mu_i)^2}{|P_{f,i}|\sigma^2_{p,i} + |N_{f,i}|\sigma^2_{n,i}}$. Dabei ist
$\mu_{p,i}$ bzw. $\mu_{n,i}$ die Anzahl der wahrer Werte in $P_{f,i}$ und
$N_{f,i}$. Weiterhin ist $\mu_i$ die Anzahl wahrer Werte in $P_{f,i} \cup
N_{f,i}$ und $\sigma$ ist die positive bzw. negative Varianz.

Anhand der bewerteten Terme wird das DNF Blocking Schema gebildet. Dazu werden
die Terme nach ihrer Fisher-Score sortiert. Anschließend werden die Terme
absteigend zur DNF Blocking hinzugefügt, wenn ein Term mindestens ein weiteres
Paar im positiven Featurevektor erfasst. Dies geschieht solange bis ein Minimum
an positiven Paaren noch nicht erfasst worden sind oder keine Terme mehr
verfügbar sind.

## Ähnlichkeitsmaße

Über Ähnlichkeitsmaße (engl. similarity measures) wird die Ähnlichkeit zweier
Datensätze bestimmt. Genauer wird die Ähnlichkeit der einzelnen Attribute
bestimmt, aus welcher sich die Gesamtähnlichkeit der Datensätze bestimmen lässt.
Wie sich die Ähnlichkeit von Strings bestimmen lässt wird seit den 60er Jahren
intensiv erforscht. Der älteste und wohl auch bekannteste Algorithmus ist die
Levenshtein Distanz [@Lev:Binary:66]. Die Levenshtein Distanz ist Begründer der
Editierdistanzen, welche die Schritte berechnet, die benötigt werden um einen
String in einen anderen umzuwandeln. Diese Schritte beinhalten das Einfügen, das
Löschen, das Ersetzen und mit der Modifiaktion von Damerau [@Dam:technique:64]
auch das Vertauschen von Zeichen. Das Berechnen der Editierdistanzen entspricht
dem Traveling Salesmen Problem und ist daher NP-schwer. Weitere beliebte
Ähnlichkeitsmaße sind Koeffizienten, da diese oft eine geringere Komplexität wie
Editierdistanzen haben. Der bekannteste Koeffizient ist der Jaccard-Koeffizient,
welcher die Überlappung zweier Mengen oder beim Stringvergleich die Menge
gleicher Substringe angibt. Eine weitere Möglichkeit Stringähnlichkeit
auszudrücken bieten Kernelfunktionen. Diese messen die Ähnlichkeit von
Stringpaaren unterschiedlicher Länge: je ähnlicher zwei Strings sind, desto
höher ist der Wert, den der Kernel zurückgibt [@Wiki:String:16]. Des Weiteren
gibt es noch viele andere Algorithmen und noch viel mehr Variationen von
Algorithmen, um die Ähnlichkeit eines Stringpaares zu bestimmen.

Aus der Vielfalt der möglichen Ähnlichkeitsmaße gibt es keines das allen anderen
klar überlegen ist. Es ist daher sehr domainabhängig, welcher Algortihmus gute
Ergebnisse liefert. Beim Vergleich von Datensätzen sind diese Domänen meist
durch die unterschiedlichen Attribute getrennt. Daher ist es notwendig
herauszufinden, für welches Attribute welche Ähnlichkeitsmetric besonders gut
funktioniert. Daraus folgt das Problem der Vergleichbarkeit der
Ähnlichkeitsmetriken. Für zwei Strings $a$ und $b$ liefert der
Jaccard-Koeffizient beispielweise Werte zwischen 0 und 1, die
Levenshtein-Distanz hingegen Werte zwischen 0 und $maxlen(a, b)$. Deshalb ist es
notwendig die verschiedenen Ähnlichkeitsmaße zu normalisieren. Dafür wird das
Intervall von 0 bis 1 gewählt, wobei 1 totale Übereinstimmung und 0 keine
Übereinstimmung bedeutet.

## Klassifizierer

Nachdem die Ähnlichkeit zwischen den Kandidatenpaaren berechnet wurde, ist es
die Aufgabe der Klassifizierer, die Menge der Kandidatenpaare in Match und
None-Matches zu trennen. Bei den Klassifizieren kann man grundsätzlich
unterscheiden zwischen benutzerdefinierten und gelernten Strategien.

#### Benutzerdefiniert {-}









