# Analyse {#sec:analysis}

## Generieren von Labels {#sec:ana_lbl}

Damit die Qualität und die Effizienz eines Verfahrens bzw. einer Kette von
Verfahren bewertet werden kann, wird eine Ground Truth benötigt, um die
Ergenisse durch Metriken in Relation zu setzten. Für den Fall, dass zu einem
Datensatz keine Ground Truth existiert, ist die Bewertung des Ergebnisses
entsprechend schwierig. Der bereits vorgestellt Algorithmus vom Kejriwal
& Miranker [@KM:Unsupervised:13] zur Erzeugung von schwachen Labels bietet
hierfür die Möglichkeit, zumindest eine Tendenz zu bekommen, wie gut Qualität
und Effizienz eines Verfahrens sind.

Der Algorithmus \ref{alg:weaklabels} nutzt dazu Token, in Form von Wörtern, für
das Blocking der Datensätze, sowie zum Ermittlen der Ähnlichkeiten. Im Gegensatz
zu einem Vollvergleich der Zeichenketten, beispielsweise durch eine
Editierdistanz, wird dadurch die Komplexität, auf Kosten der Genauigkeit,
reduziert. Die Genauigkeit ist gegenüber den Editierdistanzen niedriger, da
lediglich auf Wörterebene miteinander vergleichen wird. Je mehr gemeinsame
Wörter, desto ähnlicher ist ein Datensatzpaar. Dadurch ist es beispielsweise
nicht möglich Rechtschreibfehler zu erkennen, da der Algorithmus diese als zwei
unterschiedliche Wörter behandelt. Die Gesamtkomplexität des Algorithmus beträgt
$O(n + nm + nm)$, welche sich in die Erzeugung der TF/IDF Statistik ($O(n)$),
die Erzeugung der Blöcke über $m$ Attribute ($O(nm)$) und die Erzeugung der
Kandidatenpaare $O(nm)$ gliedert. Kritisch bei diesem Algorithmus zu betrachten
ist, dass ein Großteil der Datensatzpaare, aufgrund der Lücke zwischen den
Schwellen, nicht für die Ground Truth ausgewählt werden kann.
@fig:weaklbl_problem illustriert diese Lücke zwischen einer unteren Schwelle bei
0.1 und oberen Schwelle bei 0.7. Für alle Paare $p$ gilt, wenn $lt \leq sim(p)
< ut$, dann folgt $p \notin P \cup N$. Dadurch ist die generierte Repräsentation
eines Datensatzes, durch die Ground Truth, in vielen Fällen nicht sonderlich
repräsentativ, da viele aussagekräftige Paare ausgeschlossen werden. Kejriwal
& Miranker haben in [@KM:Unsupervised:13] die Werte für die Schwellen und der
Fenstergröße empirisch an drei relativ kleine Datensätzen (< 10.000 Einträge)
getestet. Ob die Ergebnisse sich auch auf andere Datesätze, insbesonders
deutlich größerer, anwenden lassen, wird in @sec:free_params evaluiert.

```{.a2s #fig:weaklbl_problem
    caption="Darstellung der Lücke zwischen oberer und unterer Schwelle, des
    Algorithmuses des Label Generator ohne Ground Truth (GT), innerhalb welcher
    Paare nicht für die Ground Truth ausgewählt werden können."}
     lt=0.1                             ut=0.7
       |                                  |
       |    Pairs not available for GT    |
       |<-------------------------------->|
       :                                  :
 0     v                                  v                 1
 <---------------------------------------------------------->
                      TF/IDF Similarity
```

Aufgrund der Verteilung von Matches und Non-Matches, die bis auf wenige
Ausnahmen, immer ein deutliches Ungleichgewicht zugunsten der Non-Matches
aufweist, werden für alle öffentlich verfügbaren Datensätze mit Ground Truth,
lediglich die Matches angegeben. Dementsprechend sind alle Datensatzpaare,
welche nicht in den Matches der Ground Truth enthalten sind, als Non-Matches zu
interpretieren. Das bedeutet, dass bei bekannten Matches, aus der riesigen Menge
von Non-Matches, eine repräsentative Stichprobe gezogen werden muss, um
effizient die Qualität und Effektivität zu prüfen.

```{.texalgo #alg:labels caption="WeakTrainingSet with Ground Truth"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Ground Truth $GT$
  \item Blocking Window Size: $c$
  \end{itemize}
}
\Statex \Statex \Statex
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item A set of positive samples: $P$
  \item A set of negative samples: $N$
  \end{itemize}
}
\Statex\color{gray}
\State Initialize set $P = ()$, set $N = ()$\label{alg:wk:inits}
\State Initialize set of tuple pairs $C = ()$
\State Generate TFIDF statistics of $D$
\For{fields $f \in D$}
    \For{records $r \in D$}
        \State Tokenize $r_f$ into $BKV_f$
        \State Block $r$ on generate tokens for field $f$
    \EndFor
\EndFor
\For{block $B$ generate in previous step}
    \State  Slide a window of size c over tupels in $B$
    \StatexIndent[1] Generate all possible pairs within window and
    \StatexIndent[1] add to $C$
\EndFor\color{black}\label{alg:wk:inite}
\For{pairs $(t_1, t_2) \in GT$}\label{alg:wk:gts}
    \State Add $(t_1, t_2)$ to $P$
    \If{$p \in C$}
        \State Remove $p$ from $C$
    \EndIf
\EndFor\label{alg:wk:gte}
\State Initialize dictionary $M = \{\}$
\For{pairs $(t_1, t_2) \in C$}\label{alg:wk:tfidfs}
    \State Compute TFIDF similarity $sim$ of $(t_1, t_2)$
    \State $M[(t_1, t_2)] = sim$
\EndFor\label{alg:wk:tfidfe}
\State Calculate probability distribution over $M$\label{alg:wk:hist}
\State $max_n = |GT| * 5$
\For{$i = 1$ \textbf{to} $max_n$}\label{alg:wk:ss}
    \State Choose a pair $p$ from $M$ based on probability distribution\label{alg:wk:smpl}
    \State Add $p$ to $N$
\EndFor\color{gray}\label{alg:wk:se}
\State Return $P$ and $N$
```

Um Non-Matches für eine Ground-Truth auszuwählen, müssen Paare gebildet und
deren Ähnlichkeit berechnet werden. Die gesamten Non-Matches zu bilden und deren
Ähnlichkeiten zu berechnen ist nicht effektiv, da diese Funktion bekannterweise
quadratisch (O(n^2^) ist. Für ihr Verfahren zur Bestimmung einer schwachen
Ground Truth haben Kejriwal & Miranker ein Verfahren entwickelt, das Paare
bildet und deren Ähnlichkeit berechnet. Anstatt zweier Schwellen diese in
Matches und Non-Matches zu trennen, ist es bei vorhandener Ground Truth möglich,
die Matches aus der Menge zu entfernen, sodass lediglich Non-Matches übrig
bleiben. Da das Blocking der Paare, anhand von Token durchgeführt wurde, sind
zum einen wenig substantielle Paare, mit einer Ähnlichkeit nahe 0,
ausgeschlossen worden und zum anderen substantielle Paare, mit teils hoher
Ähnlichkeit, gebildet worden. Ein Klassifikator beispielsweise kann relativ
einfach Non-Matches mit geringer Ähnlichkeit von Matches trennen. Damit aber
herausfordernde Paare, die sich etwa nur in einem Attribut unterscheiden, als
Non-Match klassifiziert werden und nicht fälschlicherweise als Match betrachtet
werden, wird eine Menge dieser uneindeutigen Paare benötigt. Aufgrund dessen
bietet die Kandidatenerzeugung aus [@KM:Unsupervised:13] eine gute
Approximation, um schnell und effektiv gute Non-Matches zu erhalten. In
Algorithmus \ref{alg:labels} wird ein Verfahren beschrieben, dass die
Kandidatenerzeugung aus Algorithmus \ref{alg:weaklabels} nutzt, um eine Menge
von Non-Matches zu bestimmen. Gegeben ist die Ground Truth in Form von Matches.
Davon ausgehend wird eine repräsentative Menge von Non-Matches aus dem Datensatz
$D$ selektiert. Die ersten drei Schritte das Generieren der TF/IDF Statistik,
das Blocken durch die Tokens und das Erzeugen der Kandidatenmenge $C$ (Zeilen
\ref{alg:wk:inits}-\ref{alg:wk:inite} in grau), sind daher identisch zum
ursprünglichen Algorithmus. Nachdem die Kandidatenmenge erzeugt wurde, werden
zunächst alle Ground Truth Matches nach $P$ übernommen (Zeilen
\ref{alg:wk:gts}-\ref{alg:wk:gte}). Zusätzlich werden alle Matches aus der
Kandidatenmenge $C$ entfernt, sodass diese ausschließlich Non-Matches
beinhaltet. Anschließend wird ebenfalls die TF/IDF-Ähnlichkeit der Paare in $C$
ermittelt und in $M$ zwischengespeichert (Zeilen
\ref{alg:wk:tfidfs}-\ref{alg:wk:tfidfe}). Anhand dieser wird die
Wahrscheinlichkeitsverteilung der Ähnlichkeiten in $M$, beispielsweise durch ein
Histogramm ermittelt (Zeile \ref{alg:wk:hist}). In @fig:label_sampling ist
beispielhaft eine Verteilung von Non-Matches (`-` Symbol) dargestellt. Die
X-Achse gibt den Ähnlichkeitswert der Paare an. Die Häufung, auf der Y-Achse,
illustriert, wie viele Paare eine entsprechende Ähnlichkeit haben. Eine Häufung
ist vor allen Dingen im unteren Ähnlichkeitsbereich zu erwarten. Durchaus
möglich sind allerdings auch größere Anhäufungen im mittleren Bereich, da durch
das Blocking der Großteil der Paare mit Ähnlichkeit 0 ausgeschlossen worden ist.
In Zeile \ref{alg:wk:smpl} werden nun Non-Matches, anhand der
Wahrscheinlichkeitsverteilung, zufällig aus $M$ gezogen, sodass Paare innerhalb
einer großen Anhäufung (z.B. unterer Bereich) häufiger ausgewählt werden als
Paare in kleinen Anhäufungen (z.B. oberer Bereich). Damit wird erreicht, dass
die Menge, der für die Ground Truth gewählten Non-Matches, möglichst
repräsentativ ist. Die Ziehung wird $max_n$-Mal wiederholt bzw., solange bis
keine Paare mehr übrig sind (Zeilen \ref{alg:wk:ss}-\ref{alg:wk:se}). Zum
Schluss wird analog zum ursprünglichen Algorithmus die Grund Truth bestehend aus
$P$ und $N$ zurückgegeben.

```{.a2s #fig:label_sampling
    caption="Beispielhalfte Verteilung von Non-Matches ('-' Symbol) auf der über
    die Ähnlichkeit (X-Achse) zwischen 0 und 1. Wie viele Non-Matches einen
    bestimmten Ähnlichkeitswert haben, wird durch die Häufung (Y-Achse)
    dargestellt."}
 Cluster-Size

      ^
      |     -
      |    - -
      |   - - -                         -
      |   -  - -                       - -
      |  - -  - -                      - - -
      |  - -  -  -            -      -   -  -
      |  - -  -   - -       -  -     -  - -  -            -
      |  - - -  - - -      -  -     -  -   - -           - - -
      | - - -  - -  - - -  - - -    - -  - - -  - -      - - -  - -                  - -
      +--------------------------------------------------------------------------------------------------->
      0                                            0.5                                                    1

                                              TF/IDF Similarity
```

## Lernen von Blocking Schemata {#sec:ana_bs}

Kejriwal & Miranker haben ein Verfahren entwickelt, dass in Algorithmus
\ref{alg:dnfbs} vorgestellt wurde. Darin wird Blocking Schema erzeugt, indem
Ausdrücke anhand der Fisher-Score bewertet werden. Die berechnete Fisher-Score
drückt für einen Ausdruck $t$, in Abhängigkeit der Groud Truth $P$ und $N$, die
Blockschlüsselabdeckung (engl. blocking key coverage) aus. Laut Ramadan
& Christen [@RC:Unsupervised:15] führt eine hohe Schlüsselabdeckung zu einer
hohen Pair Completeness, sodass viele Matches in einen gemeinsamen Block
gruppiert werden, während die Anzahl an Non-Matches, die zusammen einem Block
zugeordnet werden, gering gehalten wird. Durch dieses Verfahren werden für einen
Entity Resolution Workflow hochqualitative Blöcke erzeugt. Für dynamische
Verfahren, die Anfragen im Subsekundenbereich beantworten und möglichst gleiche
Latenzen haben sollen, ist aufgrund der niedrigen Dichte von Duplikaten (vgl.
Anzahl gesamter Matches vs gesamter Non-Matches), die Fisher-Score als
Bewertungskriterum ungeeignet. Zwar werden für die Duplikate Blöcke generiert,
die es erlauben (möglichst) alle zur Anfrage passenden Entitäten schnell und
präzise zu erhalten, allerdings ist der Großteil aller Anfragen ergebnislos.
Ergebnislos in diesem Zusammenhang bedeutet, dass es zu einer Anfrage keinen
Datensatz gibt, der derselben Entität entspricht. Das Problem ist, dass die
Fisher-Score für diese Datensätze keine Aussage trifft, weshalb die generierten
Blöcke, in welchen sich keine Matches befinden, zum Teil sehr groß werden
können. Aufgrund der Menge dieser Anfragen, wird die Effektivität des ER Systems
dramatisch reduziert.

In [@RC:Unsupervised:15] erweitern Ramadan & Christen den Algorithmus von
Kejriwal & Miranker, um eine Bewertungsfunktion für Entity Resolution, in nahe
Echtzeit. Die Fisher-Score $FS$ wird weiterhin genutzt, um die
Blockschlüsselabdeckung auszudrücken. Da dies jedoch für dynamsiche Verfahren
alleine nicht ausreichend ist, hängt die Bewertung eines Ausdrucks zusätzlich
von der Blockgröße und der Verteilung der Blöcke ab. Das Kontrollieren der
Blöckgröße für alle Blöcke, auch solche die lediglich Non-Matches enthalten,
sorgt dafür, dass alle Anfragen in nahe Echtzeit beantwortet werden können. Zur
Bewertung der Blöckgröße wird die durchschnittliche Anzahl an Datensätzen pro
Block ermittelt $S_{b_{(ave)}} = ave{|b|: b \in B}$, wobei mit $B$ alle von
einem Blocking Verfahren erzeugten Blöcke bezeichnet werden. Zusätzlich wird die
maximale Blockgröße erhoben $S_{b_{(max)}} = max{|b|: b \in B}$. Erzeugt ein
Ausdruck einen Block, der größer eines Schwellwertes ist, wird dieser Ausdruck
verworfen. Über die Verteilung der Blöcke wird geprüft, dass alle Anfragen
möglichst gleiche Latenzen haben. Dafür wird die Varianz in den Blockgrößen von
$B$ bestimmt $V = \frac{\sum_{b=1}^{|B|}(|b| - \mu_b)^2)}{|B|}$. Für einen
Ausdruck $k$ ergibt sich daraus die Bewertungsfunktion: $$SC_k=\alpha
\cdot(1-FS_k) + \beta \cdot S_{b_{(ave)_{k}}} + (1 - \alpha - \beta) \cdot
V_k,$$ wobei $\alpha$ und $\beta$ genutzt werden, um die drei Kriterien zu
gewichten. Je niedriger der Wert, desto besser wird ein Ausdruck gewertet.
Gegenüber der ursprünglichen Bewertungsfuntion hat dieser Ansatz den Nachteil,
dass für jeden Ausdruck von einem Blocking Verfahren, die Blöcke $B$ generiert
werden müssen, um Durschnitt und Varianz zu berechnen. Dadurch dauert die
Bewertung der Ausdrücke deutlich länger. Einen weiterer Nachteil ist, dass die
zwei freien Parameter ($\alpha$ und $\beta$) domänabhängig bzw.
datensatzabhängig angepasst werden müssen. Der Wertebereich für die Fisher-Score
liegt dabei zwischen 0 und 1, wohingegen der Blockdurchschnitt und die
Blockvarianz deutlich darüber liegen können. In der Praxis ist es daher extrem
schwierig geeignete Gewichte zu wählen, sodass die Fisher-Score zur Geltung
kommt und nicht von den anderen beiden Kriterien dominiert wird.

```texalgo
#alg:dnf LearnOptimalBS($S$, $k$, $d$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Set of specific blocking predicates: $S$
  \item Maximum conjunctions per term: $k$
  \item Maximum disjunctions of terms: $d$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme: $BS$
  \end{itemize}
}
\Statex
\State Initialize set of blocking scheme candidates $BS_C = ()$
\State Initialize set of terms $T = ()$
\For{$i = 1$ \textbf{to} $k$}\label{alg:bs:dis}
    \State Generate combination of $S$ with cardinality $i$ and
    \StatexIndent[1] add to $T$
\EndFor\label{alg:bs:die}
\For{term $t \in T$}
    \State $fmeasure, y_{true}, y_{pred} = evaluateTerm(t)$\Comment
    \If{$fmeasure = thres$}
        \State Remove $t$ from $T$\label{alg:bs:del}
    \EndIf
\EndFor
\For{$i = 1$ \textbf{to} $d$}\label{alg:ref:cms}
    \State Generate combination $C$ of $T$ with cardinality $i$
    \State Add $C$ to $BS_C$\label{alg:ref:cme}
\EndFor
\For{Blocking scheme $bs \in BS_C$}
    \State Initialize array $y_{true}$ with length $|P \cup N|$
    \State Initialize array $y_{pred}$ with length $|P \cup N|$
    \For{term $t \in bs$}
        \State $y_{true} \lor t.y_{true}$\label{alg:bs:ort}
        \State $y_{pred} \lor t.y_{pred}$\label{alg:bs:ory}
    \EndFor
    \State Score $s = fmeasure(y_{true}, y_{pred})$\label{alg:bs:fm}
    \If{$s > top_score$}
        \State $BS = bs$
    \EndIf
\EndFor
\State return $BS$

#alg:dnf_eval EvaluateTerm($t$, $D$, $IX$, $P$, $N$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term: $t$
  \item Dataset: $D$
  \item Indexer: $IX$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item F-measure: $f$
  \item Labels: $y_{true}$
  \item Predictions: $y_{pred}$
  \end{itemize}
}
\Statex
\State Build Index $I$ over $D$ using Indexer $IX$ and Term $t$\label{alg:bs:idx}
\State Initialize set of pairs $C = ()$
\State Initialize $TP = 0, FP = 0, FN = 0$
\For{block $b \in I$}\label{alg:bs:feb}
    \State Generate pair combinations $pc$ for records in $b$\label{alg:bs:cmb}
    \StatexIndent[1] and add them to $C$
    \State According to $P$ and $N$ calculate number of true\label{alg:bs:cnt}
    \StatexIndent[1] positives, false positives and false negatives and
    \StatexIndent[1] sum them up with $TP$, $FP$ and $FN$.
\EndFor
\State Calculate F-measure $f$ according to $TP, FN, FP$\label{alg:bs:fmes}
\State Initialize array $y_{true}$ and $y_{pred}$ with length $|P \cup N|$\label{alg:bs:yarr}
\For{$i = 1$ \textbf{to} $|P|$}\label{alg:bs:p}
    \State Pair $p = P[i]$
    \State $y_{true} = True$\label{alg:bs:p1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:p2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:p3}
    \EndIf
\EndFor
\For{$i = |P| + 1$ \textbf{to} $|N|$}\label{alg:bs:n}
    \State Pair $p = N[i]$
    \State $y_{true} = False$\label{alg:bs:n1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:n2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:n3}
    \EndIf
\EndFor
\State return $f, y_{true}, y_{pred}$
```

Das Vorgehen zur Bewertung der Blöcke von Ramadan & Christen in
[@RC:Unsupervised:15], indem diese für jeden Ausdruck erzeugt werden, ist zwar
aufwändig, da die generierten Blöcke jedoch vom genutzten Blocking Verfahren
abhängig sind, ist dieser Schritt unvermeidlich. Verbesserungswürdig ist
allerdings die Bewertungsfunktion. Die Anforderung an eine solche Funktion ist,
Ausdrücke mit einer hohe Pairs Completeness und einer hohen Pairs Quality
ausfindig zu machen und danach einzustufen. Aus @sec:measurements ist bekannt,
dass die Pairs Completness mit dem Recall verwandt ist und die Pairs Quality mit
der Precision. Zudem ist bekannt, dass es einen Kompromiss zwischen Recall und
Precision gibt, der durch das F-measure ausgedrückt werden kann. Ein optimales
F-measure maximiert dementsprechend Recall und Precision. Aufgrund der
Verwandtschaft zwischen Recall und Precision zu Pairs Completeness und Pairs
Quality, kann das F-measure ebenfalls genutzt werden, um diese beiden Attribute
zu optimieren. Damit erfüllt das F-measure die Anforderungen an eine gute
Bewertungsfunktion. Die Evaluierung eines Ausdrucks, mit dem F-measure als
Bewertungsfunktion, ist in Algorithmus \ref{alg:dnf_eval} beschrieben. Zur
Bewertung eines Ausdrucks $t$ benötigt der Algorithmus den Datensatz $D$, sowie
die Matches $P$ und die Non-Matches $N$, der Ground Truth. Zudem wird, das
einzusetzende Blocking Verfahren benötigt. Dieses wird vertreten durch einen
Indexer $IX$. Ein Ausdruck $t$ wird dem Indexer $IX$ als Blocking Schema
übergeben, woraus dieser die zugehörigen Blöcke $I$, anhand der Datensätze aus
$D$ baut (Zeile \ref{alg:bs:idx}). Durch die Betrachtung des konkreten Index,
wird eine DNF erzeuget, die auf den Indexer zugeschnitten ist. Als nächstes
werden alle generierten Blöcke in $I$ betrachtet (Zeile \ref{alg:bs:feb}). Der
Indexer muss dazu eine entsprechende Blockliste bereitstellen. Ein Block, ist,
in diesem Zusammenhang, nicht zwangsweise eine Gruppierung, welche über einen
Blockschlüssel, gebildet wurde, sondern jegliche Anhäufungen von Datensätzen,
die bei einer Anfrage zusammen als Kandidatenmenge ausgewählt werden. Die
Details hierzu werden in @sec:anaindxer erläutert. Für jeden Block werden
zunächst die Paarkombinationen[^3], aller dem Block zugehöriger Datensätze,
ermittelt. Diese werden zu der Menge aller Paarkombinationen aller Blöcke $C$
hinzugefügt (Zeile \ref{alg:bs:cmb}). Des Weiteren wird für einen Block $b$ die
Anzahl der Paare in den Klassifikationskategorien

* true positives, wenn ein Paar $p \in b$ und $p \in P$
* false positives, wenn ein Paar $p \in b$ und $p \in N$
* true negatives, wenn ein Paar $p \notin b$ und $p \in P$

über die Grund Truth ermittelt und in $TP$, $FP$ und $FN$ aufsummiert (Zeile
\ref{alg:bs:cnt}). Anhand der Werte $TP$, $FP$, $FN$ wird das F-measure zur
Bewertung des Ausdrucks $t$ bestimmt (Zeile \ref{alg:bs:fmes}). Bei der späteren
Disjunktion von Ausdrücken kann dieses F-measure allderdings nur zur Vorauswahl
der infrage kommenden Audrücke genutzt werden, da sich die F-measure Werte
verschiedener Ausdrücke aus unterschiedlichen Paarkombinationen berechnen. Damit
die Ausdrücke effizient disjunktiert werden können, wird die Paarkombination auf
die Ground Truth abgebildet. Dazu werden zwei Arrays $y_{true}$ und $y_{pred}$
mit der Länge $|P \cup N|$ erzeugt (Zeile \ref{alg:bs:yarr}). Diese können beim
Zusammenfügen der DNF, ähnlich wie die Featurevektoren in [@KM:Unsupervised:13],
einfach verodert und daraus das F-measure bestimmt werden. Für die Abbildung auf
die Ground Truth müssen alles Matches $P$ (Zeile \ref{alg:bs:p}) und alle
Non-Matches (Zeile \ref{alg:bs:n}) betrachtet werden. $y_{true}$ gibt an,
welcher Klasse ein Paar $p$ angehört: Match, wenn $p \in P$ (Zeile
\ref{alg:bs:p1}) oder Non-Match, wenn $p \in N$ (Zeile \ref{alg:bs:n1}).
$y_{pred}$ gibt an, ob ein Datensatzpaar einen gemeinsamen Block in $I$ hat
$y_{pred}[p] = True$ (Zeilen \ref{alg:bs:p2},\ref{alg:bs:n2}) oder nicht
$y_{pred}[p] = False$ (Zeilen \ref{alg:bs:p3},\ref{alg:bs:n3}). Die Bewertung
des Ausdrucks über das F-measure, $y_{true}$ und $y_{pred}$ werden zum Schluss
an den Aufrufer zurückgegeben.

[^3]: 2-Tupel der Datensätze ohne festgelegte Reihenfolge

Die Änderungen an der Bewertungsfunktion führen dazu, dass das Verfahren zum
Bilden eines Disjunktiven Blocking Schema aus [@KM:Unsupervised:13] nicht mehr
angewendet werden kann, da die Featurevektoren, welche dazu genutzt wurden,
nicht länger erzeugt werden. Der Algorithmus zur Bestimmung des optimalen
Blocking Schemas ist in Algorithmus \ref{alg:dnf} dargestellt. Der erste
Parameter sind die spezifischen Blockingprädikate $S$. Diese werden vom Aufrufer
der Funktion bestimmt. Da die maximale Konjunktion der Blockingprädikate bzw.
die maximale Disjunktion potentiell unendlich groß ist, werden diese über die
Parameter $k$ für die Konjunktionen und $d$ für die Disjunktionen begrenzt. Ein
weiterer Grund die Konjunktionen bzw. Disjunktionen nicht beliebig zu erhöhen
ist, dass das ein Blocking Verfahren deutlich komplexere Blockschlüssel erzeugen
muss. Denn für jedes spezifische Blockingprädikat muss, beim Erzeugen der
Blockschlüssel eines Datensatzes, die Prädikatsfunktion aufgerufen werden.
Demnach nimmt die Effizienz der Blockschlüsselgenerierung ab, je mehr
Konjunktionen bzw. Disjunktionen ein Blocking Schema hat. Am Anfang werden die
konjugierten Ausdrücke erzeugt, indem alle Kombinationen der Menge spezifischer
Blockingprädikte $S$ bis zur Länge $k$ der maximalen Konjunktionen berechnet
werden (Zeilen \ref{alg:bs:dis}-\ref{alg:bs:die}), somit ist $$T
= \{\{\text{2-Tupel von S}\}, \{\text{3-Tupel von S}\}, \dots, \{\text{k-Tupel
von S}\}\}.$$ Anschließend wird jeder Ausdruck $t$ durch den oben erklärten
Algorithmus \ref{alg:dnf_eval} evaluiert. Ist der F-measure Wert $f$ für $t$
kleiner einer Schranke $thres$, indiziert dies einen ungeeigneten Block und der
Ausdruck wird entfernt (Zeile \ref{alg:bs:del}). Aus den noch in $T$ vorhandenen
Ausdrücken, werden durch Disjunktion bis zur Länge $d$, mögliche Kandidaten
eines Blocking Schemas generiert (Zeilen \ref{alg:ref:cms}-\ref{alg:ref:cme}).
Ein Blocking Schema wird ausgewählt, indem die Arrays $y_{pred}$ und $y_{true}$
der Ausdrücke in jedem potentiellen Blocking Schema verodert werden (Zeilen
\ref{alg:bs:ort}-\ref{alg:bs:ory}) und daraus das F-measure berechnet wird
(Zeile \ref{alg:bs:fm}). Das potentielle Blocking Schema mit dem höchsten
F-measure wird abschließend ausgewählt und zurückgegeben.

\TODO{Cleverer Algorithmus durch bessers Sampling. Veroderung bestes Ergebnis
= Recall 1 und Precision bleibt gleich. Recall 1 ist eventuell etwas zu
optimistisch. Im welchem Rahmen kann angenommen werden, dass die Precision
gleich bleibt. Für A1 Precision 0.75 und A2 Precision 0.1 offensichtlich nicht.
Idee für Voraussage, ob DNF besser mitteln der Precision und Recall mit
addieren. => P1 = 1, P2 = 0, P12 = 0.5; R1 = 0.5, R2 = 0.4, R12 = 0.9.
Algorithmus auf Whiteboard}

## Blockschlüsselgenerierung

| ID | Entitäts-ID | Vorname | Zweitname | Nachname |
|----+-------------+---------+-----------+----------|
| 1  | 1           | Peter   | Moritz    | Michel   |
| 2  | 1           | Moritz  | Peter     | Michel   |
| 3  | 2           | Michel  | Moritz    | Peter    |
| 4  | 2           | Michle  | Moritz    | Peter    |

: Beispieldatensätze zur Veranschaulichung der Kommutativität bei der
Blockschüsselerzeugung. Das Attribut ID identifiziert den einzelnen Datensatz
und die Entitäts-ID ordnet Datensätze Entitäten zu. Dementsprechend beschreiben
Datensatz 1 und 2, sowie 3 und 4 die selbe Entität. {#tbl:comm_examles}

Die Erzeugung von Blockschlüsseln aus einem einzelnen Attribut sind vielfältig.
In @sec:staticblocking wurde ein Verfahren vorgestellt, dass Q-gramme bildet.
Ein anderes Verfahren nutzt Attributssuffixe. Auch denkbar sind
Attributsprefixe, welche bei der Sorted Neighborhood Anwendung finden. Zudem
sind phonetische Enkodierung beliebt, um unterschiedliche Schreibweisen
zusammenzuführen. Damit diese Verfahren für ein DNF Blocking Schema nutzbar
sind, werden diese als allgemeine Blockingprädikat abgebildet, beispielsweise
`HasCommonQGram` oder `HasCommonSuffix`. Die Anwendung der erzeugten
Blockschlüssel bei der Disjunktion von spezifischen Blockingprädikaten ist dabei
intuitiv. Für jeden Blockschlüssel $S$, der anhand eines Datensatzes $r$ erzeugt
wurde, wird $r$ einem Block zugeordnet. Die einzige Überlegung hierbei ist, ob
die Blöcke verschiedener spezifischer Blockingprädikate disjunkt sind oder
nicht. Konkret bedeutet dies, Erzeugen zwei Prädikate denselben Schlüssel,
verweisen diese auf denselben Block oder werden unterschiedliche Blöcke
adressiert. Dieses Problem muss vom jeweiligen Blocking Verfahren gelöst werden.
Im Gegensatz dazu ist die Behandlung der Konjunktiven Terme, bei der
Blockschlüsselerzeugung, deutlich komplexer. Hierbei müssen zwei oder mehr
Mengen von Blockschlüsseln miteinander verknüpft werden. Gegeben seien drei
spezifische Blockingprädikate $P_1, \cdots, P_n$, eine Relation $S(P_k, r)$, die
anhand eines Prädikates $P_k$ und eines Datensatzes $r$ eine Menge von
Blockschlüsseln erzeugt, sowie eine Verknüpfung $\circ$, die zwei
Schlüsselmengen miteinander verknüpft. Eine Menge konjunktiver Blockschlüssel,
welche in der DNF als verundeter Ausdruck repräsentiert sind, wird gebildet
durch $B(r) = {S(P_1, r) \circ \cdots \circ S((P_n, r)}$. Betrachtet man die
Konjunktion strikt, so gilt für $\circ$ die Kommutativität. Diese Eigenschaft
ist für die Blockschlüsselbildung interessant, da die Einhaltung oder die
Nichteinhaltung der Kommutativität maßgeblich das Ergebnis eines Blocking
Verfahrens beeinflusst. Hierzu wird der Ausdruck $A
= $ (`ExakteÜbereinstimmung`, `Vorname`) $\land$ (`ExakteÜbereinstimmung`,
`Zweitname`) $\land$ (`ExakteÜbereinstimmung`, `Nachname`) betrachtet. Die
@tbl:comm_examles beinhaltet vier Datensätze, die eindeutig über die ID
identifiziert werden könne. Jeweils zwei der Datensätze beschreiben dieselbe
Entität. Zur Demonstration der Auswirkung der Kommutativität werden drei Paare
betrachtet. Zunächst das Paar (1, 2). Beide Datensätze beschreiben dieselbe
Entität, aber der Vorname wurde mit dem Zweitnamen vertauscht. Beim nächsten
Paar (1, 3) gibt es ebenfalls eine Vertauschung und zwar zwischen Vorname und
Nachname. Allerdings beschreiben die beiden Datensätze unterschiedliche
Entitäten. Das dritte Paar (3, 4) beschreibt wieder dieselbe Entität, jedoch
gibt es in Datensatz 4 einen Schreibfehler im Vornamen. Angenommen $\circ$ ist
kommutativ, dann ist `Peter` $\circ$ `Moritz` $\circ$ `Michel`$=$ `Moritz`
$\circ$ `Peter` $\circ$ `Michel` und folglich wird das Paar (1, 2), trotz
vertauschter Attribute, einem gemeinsamen Block zugeordnet. Das Gleiche gilt
aber auch für das Paar (1, 3), sodass `Peter` $\circ$ `Moritz` $\circ$
`Michel`$=$ `Michel` $\circ$ `Moritz` $\circ$ `Peter` und folglich wird dieses
Paar einem gemeinsamen Block zugeordnet wird. Für den Fall, dass $\circ$ nicht
kommutativ ist, haben weder das Paar (1, 2) noch (1, 3) einen gemeinsamen
Blockschlüssel. Dementsprechend kann durch eine kommutative Verknüpfung der
Blockschlüsselmengen die Pairs Completeness auf Kosten der Pairs Quality erhöht
werden. Ist die Verknüpfung nicht kommutativ, wird umgekehrt die Pairs Quality
auf Kosten der Pairs Completeness verbessert. Die Kommutativität von $\circ$
kann jedoch keinen Einfluss auf Rechtschreibfehler, wie im Falle von Paar (3, 4)
nehmen. Eine Möglichkeit Rechtschreibfehler bei der Erzeugung der Blockschlüssel
herauszufiltern ist, tokenbasierende Prädikate (z.B. Q-Gramme) zu verwenden. Der
Nachteil hierbei ist, dass mit einem solchen Prädikat deutlich mehr
Teilschlüssel, für die einzelnen Attribute, erzeugt werden, was zum einen den
Prozess der Schlüsselgenerierung verlangsamt und zum anderen zu einer größeren
Menge von Blöckschlüsseln führt, sodass die Chance größer ist, dass unähnliche
Datensätze zusammen in einen Block gruppiert werden. Um bei gleicher Komplexität
transpositive Rechtschreibfehler zu filtern, muss $\circ$ kommutativ sein und
zusätzlich auf Zeichenebene angewendet werden. Beispielsweise indem die
Teilschlüssel konkateniert und als Anagramme behandelt werden. Dadurch ist es
wiederum möglich die Pairs Completeness zu erhöhen. Allerdings ebenfalls auf
Kosten der Pairs Quality, da die Kollisionsrate durch Reduktion der möglichen
Blockschlüssel erhöht wird. Im Vergleich zu einem tokenbasierenden Prädikat
bleibt bestenfalls die Anzahl der Blockschlüssel gleich bzw. wird reduziert,
anstatt zu wachsen.

```{.texalgo #alg:bkvs caption="BlockingKeyValues(t, r)"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term from Blocking Schema $t$
  \item Record $r$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Key Values: $BKV$
  \end{itemize}
}
\Statex
\State Initialize list $BKV = []$
\For{specific blocking predicate $p \in t$}\label{alg:bkv:fep}
  \State field $f = p.field$
  \State attribute keys $ak = p.predicate(r.f)$\label{alg:bkv:pre}
  \If{$BKV$ is empty}
    \State $BKV = ak$\label{alg:bkv:f1}
  \Else
    \State $bkv$ = []
    \While{$BKV$ is not empty}\label{alg:bkv:as}
      \State Take key $x$ from BKV
      \For{key $y$ in $ak$}
        \State $xy = concatenate(x ,y)$\label{alg:bkv:cc}
        \State Append $xy$ to $BKV$
      \EndFor
    \EndWhile\label{alg:bkv:ae}
    \State $BKV = bkv$\label{alg:bkv:rpl}
  \EndIf
\EndFor
\State return $BKV$
```

Algorithmus \ref{alg:bkvs} beschreibt ein nicht kommutatives Verfahren
Blockschlüssel durch Konkatenation zu bilden. Jeder Ausdruck $t$ des DNF
Blocking Schema erzeugt für einen Datensatz $r$ eine Menge von Blockschlüsseln.
Die Blockschlüssel der Ausdrücke werden unabhängig voneinander gebildet. Der
Algorithmus erhält daher als Eingabewert nur einen Ausdruck, bestehend aus
mindestens einem spezifischen Blockingprädikat. Diese werden Reihe nach
betrachtet (Zeile \ref{alg:bkv:fep}). Anhand des Prädikats $p$ werden alle
Teilblockschlüssel für das verknüpfte Attribut $p.field$ generiert.
Beispielsweise liefert das Prädikat `GemeinsamerToken` alle durch Leerzeichen
getrennte Token des Attributes $r.field$ (Zeile \ref{alg:bkv:pre}). Ist die BKV
Liste zu diesem Zeitpunkt leer, wird die Schlüsselliste $ak$ als $BKV$ Liste
übernommen. Existieren in $BKV$ allerdings schon Schlüssel wird zunächst eine
temporäre Liste $bkv$ erzeugt. Anschließend werden aus $BKV$ solange Schlüssel
entnommen, bis diese leer ist (Zeile \ref{alg:bkv:as}). Für jeden entnommenen
Schlüssel $x$ werden $|ak|$ neue Schlüssel erzeugt. Dazu wird der neue Schlüssel
$xy$ gebildet, indem ein Schlüssel $y$ aus $ak$ mit $x$ konkateniert wird (Zeile
\ref{alg:bkv:cc}). Alle auf diese Weise neu erzeugten Schlüsselpaare $xy$ werden
zu $bkv$ hinzugefügt. Wenn die $BKV$ Liste leer ist, wird die temporäre Liste
$bkv$ nach $BKV$ übernommen (Zeile \ref{alg:bkv:rpl}). Nachdem alle Prädikate
bearbeitet wurden, wird die Liste der generierten Blockschlüssel $BKV$
zurückgegeben.

## Dynamische Blocking Verfahren {#sec:anaindxer}

### Similarity-Aware Inverted Index

Die Idee des DySimII ist ein Standard Blocking Verfahren, dass es erlaubt, die
Ähnlichkeit von Datensätzen nachzuschlagen, indem Attributsähnlichkeiten
vorausberechnet werden. Im Wesentlichen handelt es sich bei DySimII, um einen
Multi-pass Ansatz, da für einen Datensatz mehrere Blockschlüssel erzeugt werden.
Zu diesem Zweck wird für jedes Attribut eine Enkodierungsfunktion genutzt, die
genau einen Blockschlüssel erzeugt. Anhand dessen werden Attribute zusammen
gruppiert und innerhalb der Gruppierung wird die Ähnlichkeit berechnet. Die
errechnete Ähnlichkeit wird in einer Art Cache, dem Similarity Index,
vorgehalten. Während einer Abfrage wird vermieden, dass ein Großteil der teuren
Ähnlichkeitsberechnungen eingespart wird. Beim Gruppieren von Attributen in
Blöcke, führt der DySimII keine Trennung Attributen durch, sodass Attributswerte
von Vorname und Zweitname bzw. Nachname im einen gemeinsamen Block gruppiert
werden können. Dadurch wird zum einen die Abdeckung möglicher
Attributskombinationen erhöht und zum anderen vermieden, dass Ähnlichkeiten
doppelt berechnet werden. Diese Einsparung macht sich besonders bei ähnlichen
Attributen mit großteils überlappenden Werten, wie Vorname und Zweitname,
bemerkbar. Neben der Ähnlichkeitsvorausberechnung, dienen die Blöcke auch zum
Bilden einer Kandidatenmenge möglicher Duplikate. Für einen Anfragedatensatz $q$
besteht die Kandidatenmenge $C$ einerseits aus Datensätzen, die in irgendeinem
indizierten Attribut, denselben Wert haben und andererseits durch irgendein
Attribut denselben Blockschlüssel haben. Durch die Vermischung der
Attributswerte, sowohl im RI als auch im BI, kann dadurch zwar die Pairs
Completeness erhöht werden, da beispielsweise Attributsvertauschungen erkannt
werden, je ähnlicher die Werte unterschiedlicher Attribute sind, wird jedoch
auch die Pairs Qualtity gesenkt. Diese Verhalten korrespondiert mit der
Kommutativität bei der Bildung von Blockschlüsseln, aus konjunktiven
spezifischen Blockingprädikaten. Im Kontrast zum DySimII, kann die Überlappung,
beim Bauen des DNF Blocking Schema, jedoch mit Bedacht auf ausgewählte Attribute
angewandt werden und wird nicht pauschalisiert. Die Möglichkeit der Überlappung
von Attributswerten führt jedoch zu einer gravierenden Folge bei der Berechnung
der Ähnlichkeit zwischen zwei Datensätzen. Da es in einem Block zu einer
Vermischung von Werten unterschiedlicher Attribute kommen kann, ist aufgrund
dessen nicht möglich die vorausberechnete Ähnlichkeit einem bestimmten Attribut,
zwischen Anfragedatensatz und Kandidatendatensatz, zuzuweisen, um etwa einen
Ähnlichkeitsvektor zu bilden. Stattdessen werden die Ähnlichkeiten addiert. Aus
@sec:classifier ist allerdings bekannt, dass dabei, für einen Klassifikator,
wertvolle Informationen verloren gehen, die entscheidend sind, damit ein Match
von einem Non-Match unterschieden werden kann. Die Entscheidung, des DySimII
Verfahren, nur die Ähnlichkeit aus Attributen, die einen gemeinsamen Block
haben, in die Gesamtähnlichkeit mit einfließen zu lassen, reduziert den
Ähnlichkeitsinformationswert weiter, da alle unähnlichen Attribute mit demselben
Ähnlichkeitswert von 0 bestraft werden.

#### MDySimII

```{.a2s #fig:mdysimII_example
    caption="Ein beispielhafter MDySimII-Index, welcher aus der Tabelle links
    erzeugt worden ist. Die Beispieldatensätze enthalten das Namensattribut
    eines Restaurants und die Art der Küche. RI ist der Record Index, BI ist der
    Block Index, welcher aus dem Blocking Schema (CommonToken, Name) ∧
    (CommonToken, Kitchen) erzeugt wurde. SI ist der Similarity Index."}
 Blocking Scheme: (CommonToken, Name) ∧ (CommonToken, Kitchen)

#-----------+--------------+--------------# #----------------------------------------# #------------------------------------------------#
|[RID      ]| Name         | Kitchen      | | RI (name)                              | | RI (kitchen)                                   |
#-----------+--------------+--------------# | .-------------.       .--------------. | | .---------.      .------.        .----------.  |
|     r1    | tonys pizza  | italian      | | | tonys pizza |       | tonys gelato | | | | italian |      | vegi |        | american |  |
+-----------+--------------+--------------+ | '---+---------'       '---+----------' | | '---+-----'      '---+--'        '---+------'  |
|     r2    | tonys gelato | vegi         | |     |                     |            | |     |                |               |         |
+-----------+--------------+--------------+ |     |  .----+----+----.   |  .----.    | |     |  .----+----.   |  .----+----.  |  .----. |
|     r3    | tonys pizza  | italian vegi | |     +->| r1 | r3 | r4 |   +->| r2 |    | |     +->| r1 | r3 |   +->| r2 | r3 |  +->| r4 | |
+-----------+--------------+--------------+ |        '----+----+----'      '----'    | |        '----+----'      '----+----'     '----' |
|     r4    | tonys pizza  | american     | |                                        | |                                                |
#-----------+--------------+--------------# #----------------------------------------# #------------------------------------------------#
#----------------------------------------------------------------------# #--------------------------------------------------------------#
| BI (name)                                                            | | BI (kitchen)                                                 |
| .--------------. .--------------. .---------------.                  | | .--------------. .--------------. .---------------.          |
| | tonysitalian | | pizzaitalian | | tonysamerican |                  | | | tonysitalian | | pizzaitalian | | tonysamerican |          |
| '---+----------' '---+----------' '---+-----------'                  | | '---+----------' '---+----------' '---+-----------'          |
|     |                |                |                              | |     |                |                |                      |
|     v                v                v                              | |     v                v                v                      |
| .---+---------.  .---+---------.  .---+---------.                    | | .---+-----.      .---+-----.      .---+------.               |
| | tonys pizza |  | tonys pizza |  | tonys pizza |                    | | | italian |      | italian |      | american |               |
| '-------------'  '-------------'  '-------------'                    | | '---------'      '---------'      '----------'               |
|                                                                      | |                                                              |
| .-----------.    .------------.   .-----------.    .---------------. | | .-----------. .------------. .-----------. .---------------. |
| | tonysvegi |    | gelatovegi |   | pizzavegi |    | pizzaamerican | | | | tonysvegi | | gelatovegi | | pizzavegi | | pizzaamerican | |
| '--+--------'    '--+---------'   '--+--------'    '---+-----------' | | '--+--------' '--+---------' '--+--------' '---+-----------' |
|    |                |                |                 |             | |    |             |              |              |             |
|    v                v                v                 v             | |    v             v              v              v             |
| .--+-----------. .--+-----------. .--+----------.  .---+---------.   | | .--+---.      .--+---.       .--+---.      .---+------.      |
| | tonys gelato | | tonys gelato | | tonys pizza |  | tonys pizza |   | | | vegi |      | vegi |       | vegi |      | american |      |
| +--------------+ '--------------' '-------------'  '-------------'   | | '------'      '------'       '------'      '----------'      |
| | tonys pizza  |                                                     | |                                                              |
| '--------------'                                                     | |                                                              |
#----------------------------------------------------------------------# #--------------------------------------------------------------#
#------------------------------------------------# #--------------#
| SI (name)                                      | | SI (kitchen) |
| .--------------.      .-------------.          | |              |
| | tonys gelato |      | tonys pizza |          | | {empty}      |
| '--+-----------'      '--+----------'          | #--------------#
|    |                     |                     |
|    v                     v                     |
| .--+----------+-----. .--+-----------+-----.   |
| | tonys pizza | 0.7 | | tonys gelato | 0.7 |   |
| '-------------+-----' '--------------+-----'   |
#------------------------------------------------#
[RID      ]: {"fill":"#cccccc", "a2s:label": " Record ID"}
```

Eine weitere Einschränkung des DySimII Verfahrens ist die Vorgabe zu einer
Enkodierfunktion pro Attribut, die genau ein Attribut liefert. Aufgrund dessen
kann für DySimII kein DNF Blocking Schema verwendet werden. Im Folgenden wird
ein Verfahren beschrieben, dass zum einen DNF kompatibel ist und zum anderen
einige Nachteile, der Informationsdetailliertheit des Ähnlichkeitswertes,
ausbessert. Dieser Ansatz wird Multi-Dynamic Similarity-Aware Inverted Index
(MDySimII) genannt. Dabei steht das Multi für einen uneingeschränkten Multi-pass
Ansatz. Das bedeutet, dass Blockschlüssel über mehrere Attribute generiert
werden dürfen, dass nicht jedes Attribut zur Generierung genutzt werden muss und
dass beliebig viele Blockschlüssel erzeugt werden können. @fig:mdysimII_example
zeigt beispielhaft die Indexstrukturen des MDySimII, welche aus der Tabelle und
dem Blocking Schema (links oben) erzeugt wurden. Die RIs verknüpfen dabei
jeweils ein Attribut mit den Datensatzidentifieren, die diesem Attribut
entsprechen, etwa `tonys pizza` mit den Datensätzen `r1`, `r3` und `r4`. Die
Blockschüssel für die BIs wurden durch Algorithmus \ref{alg:bkvs} generiert. Für
`r1` wurden, aus den Namenstoken `tonys` und `pizza`, sowie dem Token der
Küchenart `italian`, die Blockschlüssel `tonysitalian` und `pizzaitalian`
gebildet. Die beteiligten Attribute wurden, in ihren eigenen BI, in Blöcke mit
den beiden Blockschlüsseln eingefügt. Der SI, des jeweiligen Attributes, wird
angelegt, wenn in mindestens einem Block mehrere Attribute einfügt wurden. Das
Beispiel zeigt die strikte Trennung von Attributswerten, sodass lediglich Werte
desselben Attributes zusammen gruppiert werden können. Folglich ist der MDySimII
in der Lage einen Vektor mit Ähnlichkeitswerten der einzelnen Attribute für
Datensatze der Kandidatenmenge zurückzugeben.

```texalgo
#alg:mdysim_insert MDySimII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$: $F$
  \item Similarity functions: $S_i,i=1 \cdots |F|$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $RI, BI, SI$
  \end{itemize}
}
\Statex
\For{fields $f \in F$}\label{alg:dII:is}
  \State Initialize $RI_f = \{\}$, $BI_f = \{\}$, $SI_f = \{\}$
\EndFor\label{alg:dII:ie}\color{gray}
\For{records $r \in D$}\color{black}
  \For{fields $f \in F$}\label{alg:dII:ris}
    \State insert $r.id$ into $RI_f[r.f]$
  \EndFor\label{alg:dII:rie}
  \For{terms $t \in BS$}
    \State $bkvs = BlockingKeyValues(t, r)$\label{alg:dII:bkv}
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \If{$a \notin SI_f$}\color{gray}
          \State Append r.f to $BI_f[bkv]$\label{alg:dII:bi}
          \For{attribute $a \in BI_f[bkv]$}
            \State $sim = S_f(r.f, a)$\label{alg:dII:sim}
            \State Append $(r.f, sim)$ to $SI_f[a]$\label{alg:dII:si1}
            \State Append $(a, sim)$ to $SI_f[r.f]$\label{alg:dII:si2}
          \EndFor
        \EndIf\color{black}
      \EndFor
    \EndFor
  \EndFor\label{alg:dII:ins}
\EndFor

#alg:mdysim_query MDySimII - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index
\For{fields $f \in F$}
    \State $ri = RI_f[q.f]$
    \For{$r.id \in ri$}
        \State $M[(r.id, f)] = 1.0$
    \EndFor
    \State $si = SI_f[q.f]$
    \For{$(r.f, sim) \in si$}
        \State $ri = RI_f[r.f]$
        \For{$r.id \in ri$}
            \State $M[(r.id, f)] = sim$
        \EndFor
    \EndFor
\EndFor
\State return $M$
```

Durch die erläuterten Anpassungen für den MDySimII ergeben sich einige
Änderungen im Ablauf. In Algorithmus \ref{alg:mdysim_insert} ist die
*Build-Phase* des MDySimII beschrieben. Zunächst werden die Index
Datenstrukturen Record Identifier Index (RI), welcher alle Attribute speichert
und diese ihren Datensätzen zuordnet, Block Index (BI), welcher Attribute anhand
der Blockschlüssel gruppiert und Similarity Index (SI), welcher
Attributsähnlichkeiten zwischen Attributen im gleichen Block hält, für jedes in
der DNF vorkommende Attribut erzeugt (Zeilen \ref{alg:dII:is}-\ref{alg:dII:ie}).
Als Nächstes werden alle Datensätze in $D$ nacheinander den Indexstrukturen des
MDySimII hinzugefügt. Dazu wird zuerst der Datensatzidentifier unter dem
entsprechenden Attribute, in alle initialisierten RIs, eingefügt (Zeilen
\ref{alg:dII:ris}-\ref{alg:dII:rie}). Anschließend werden die disjunkten
Ausdrücke der DNF einzeln betrachtet. Zu jedem Ausdruck werden für den Datensatz
$r$ die Blockschlüsselwerte $bkvs$ erzeugt (Zeile \ref{alg:dII:bkv}). Für jedes,
durch den aktuellen Ausdruck erfasstes Attribut, werden die Attributswerte von
$r$, unter dem Blockschlüssel $bkv$, in den entsprechenden Block Index eingefügt
(Zeilen \ref{alg:dII:bi}). Nachdem ein Attribut in einen Block eingefügt wurde,
werden analog zum ursprünglichen DySimII Verfahren, die Ähnlichkeiten zwischen
$r.f$ und den bisherigen Attributen des Blocks ermittelt (Zeile
\ref{alg:dII:sim}). Der Algorithmus endet, wenn alle Datensätze $r \in D$
hinzugefügt wurden. Wird später in der *Query-Phase* ein einzelner Datensatz
hinzugefügt, werden lediglich die Schritte in Zeile
\ref{alg:dII:ris}-\ref{alg:dII:ins} durchgeführt.

Wird der MDySimII Index zur Bewertung eines DNF Ausdrucks, für Algorithmus
\ref{alg:dnf_eval} gebaut, werden anstatt der Attributswerte die
Datensatzidentifier in die Blöcke der BIs eingefügt. Zudem wird die Erzeugung
der SIs ausgespart, da diese für die Bewertung nicht benötigt werden. Neben den
BI Blöcken, werden zudem die RI Blöcke übergeben, da diese maßgeblich die
Kandidatenmenge eines Anfragedatensatzes beeinflussen.

Die *Query-Phase* des MDySimII wird in Algorithmus \ref{alg:mdysim_query}
beschrieben. Dieser Algorithmus ist fast identisch zum DySimII Verfahren. Die
Änderungen sind, dass im entsprechenden RI bzw. SI des Attributes nachgeschlagen
wird und dass ein Ähnlichkeitsvektor erstellt wird, anstatt die einzelnen
Ähnlichkeiten aufzusummieren. Aus Effizienzgründen erhebt der MDySimII, ebenso
wie der DySimII, keine Ähnlichkeiten zwischen Attributen, die nicht zusammen
gruppiert wurden. Da das DNF Blocking Schema allerdings nicht mehr garantiert,
dass jedes Attribut berücksichtigt wird, entfällt eine komplette
Attributsabdeckung bei Vorausberechnung der Ähnlichkeiten. Im schlimmsten Fall
besteht das Blocking Schema nur aus einem einstelligen Ausdruck, wodurch das
Blocking lediglich auf einem Attribut durchgeführt. Ist der Ähnlichkeitsvektor
der Kandidaten maximal in den Stellen besetzt, die auch im DNF Blocking Schema
genutzt werden.

Das ursprüngliche Design des Similarity-Aware Inverted Index von Christen
& Gayler [@CG:Scalable:08] hat eine fundamentale Schwachstelle, die auch in den
Varianten DySimII und MDySimII vorhanden ist. Diese Schwachstelle ist der Record
Identifier Index (RI), welcher den Index enorm ausbremst, wenn ein Attribut nur
wenige Auswahlmöglichkeiten bietet, beispielsweise Geschlecht oder Bundesland,
bzw. wenn wenige Attributswerte besonders oft vorkommen, beispielsweise der
Nachname `Müller`, welcher häufig getragen wird. Wenn ein solches Attribut im
DNF Blocking Schema vorkommt, wird zwangsweise über den Record Identifier Index
eine riesige Menge an Kandidaten selektiert, wovon nur ein Bruchteil tatsächlich
relevant ist. Der schlimmste Fall wäre, beispielsweise ein Datensatz mit einem
Attribut Geschlecht (männlich/weiblich), wobei jeder Datenensatz den
Attributswert `weiblich` hat. Dafür erzeugen alle Variationen des
Similarity-Aware Inverted Index eine Kandidatenmenge mit 100% der Datensätze.
Aufgrund der vielen false positives in der Kandidatenmenge, wird Algorithmus zur
Bewertung von Ausdrücken, Ausdrücke mit diesen Attributen, sowohl alleinstehend
als auch in Konjunktion mit anderen, sehr schlecht bewerten. Im ungünstigsten
Fall wird dadurch ein Ausdruck, welcher für den BI sehr gute Blöcke erzeugt,
irrelevant.

#### MDySimIII

```{.a2s #fig:mdysim_example
    caption="Ein beispielhafter MDySimIII-Index, welcher aus der Tabelle links
    erzeugt worden ist. Die Beispieldatensätze enthalten das Namensattribut
    eines Restaurants und die Art der Küche. BI ist der Block Index, welcher aus
    dem Blocking Schema (CommonToken, Name) ∧ (CommonToken, Kitchen) erzeugt
    wurde. SI ist der Similarity Index."}
 Blocking Scheme: (CommonToken, Name) ∧ (CommonToken, Kitchen)

#-----------+--------------+--------------#  #----------------------------------------------# #--------------#
|[RID      ]| Name         | Kitchen      |  | SI (name)                                    | | SI (kitchen) |
#-----------+--------------+--------------#  | .--------------.      .-------------.        | |              |
|     r1    | tonys pizza  | italian      |  | | tonys gelato |      | tonys pizza |        | | {empty}      |
+-----------+--------------+--------------+  | '--+-----------'      '--+----------'        | #--------------#
|     r2    | tonys gelato | vegi         |  |    |                     |                   |
+-----------+--------------+--------------+  |    v                     v                   |
|     r3    | tonys pizza  | italian vegi |  | .--+----------+-----. .--+-----------+-----. |
+-----------+--------------+--------------+  | | tonys pizza | 0.7 | | tonys gelato | 0.7 | |
|     r4    | tonys pizza  | american     |  | '-------------+-----' '--------------+-----' |
#-----------+--------------+--------------#  #----------------------------------------------#
#-------------------------------------------------------------------------------------------# #---------------------------------------------------------------------------#
| BI (name)                                                                                 | | BI (kitchen)                                                              |
| .--------------.         .--------------.         .---------------.                       | | .--------------.     .--------------.     .---------------.               |
| | tonysitalian |         | pizzaitalian |         | tonysamerican |                       | | | tonysitalian |     | pizzaitalian |     | tonysamerican |               |
| '---+----------'         '---+----------'         '---+-----------'                       | | '---+----------'     '---+----------'     '---+-----------'               |
|     |                        |                        |                                   | |     |                    |                    |                           |
|     v            RI          v            RI          v            RI                     | |     v        RI          v        RI          v         RI                |
| .---+---------.  .--+--. .---+---------.  .--+--. .---+---------.  .--.                   | | .---+-----.  .--+--. .---+-----.  .--+--. .---+------.  .--.              |
| | tonys pizza +->+r1|r3| | tonys pizza +->+r1|r3| | tonys pizza +->+r4|                   | | | italian +->+r1|r3| | italian +->+r1|r3| | american +->+r4|              |
| '-------------'  '--+--' '-------------'  '--+--' '-------------'  '--'                   | | '---------'  '--+--' '---------'  '--+--' '----------'  '--'              |
|                                                                                           | |                                                                           |
| .-----------.          .------------.         .-----------.         .---------------.     | | .-----------.     .------------.    .-----------.     .---------------.   |
| | tonysvegi |          | gelatovegi |         | pizzavegi |         | pizzaamerican |     | | | tonysvegi |     | gelatovegi |    | pizzavegi |     | pizzaamerican |   |
| '--+--------'          '--+---------'         '--+--------'         '---+-----------'     | | '--+--------'     '--+---------'    '--+--------'     '---+-----------'   |
|    |                      |                      |                      |                 | |    |                 |                 |                  |               |
|    v              RI      v              RI      v             RI       v            RI   | |    v      RI         v      RI         v      RI          v         RI    |
| .--+-----------.  .--. .--+-----------.  .--. .--+----------.  .--. .---+---------.  .--. | | .--+---.  .--+--. .--+---.  .--+--. .--+---.  .--+--. .---+------.  .--.  |
| | tonys gelato +->|r2| | tonys gelato +->|r2| | tonys pizza +->|r3| | tonys pizza +->+r4| | | | vegi +->+r2|r3| | vegi +->+r2|r3| | vegi +->+r2|r3| | american +->+r4|  |
| +--------------+  #--# '--------------'  '--' '-------------'  '--' '-------------'  '--' | | '------'  '--+--' '------'  '--+--' '------'  '--+--' '----------'  '--'  |
| | tonys pizza  +->|r3|                                                                    | |                                                                           |
| '--------------'  '--'                                                                    | |                                                                           |
#-------------------------------------------------------------------------------------------# #---------------------------------------------------------------------------#
[RID      ]: {"fill":"#cccccc", "a2s:label": " Record ID"}
```

Der MDySimIII ist eine Modifikation des MDySimII Verfahrens, das dahingehend
verändert wurde, dass die Anzahl der Kandidaten, auch bei spezifischen
Blockingprädikaten mit wenigen Optionen bzw. oft vorkommenden Werten, effizient
eingeschränkt werden kann. Dadurch ist es möglich, im Gegensatz zur bisherigen
Familie von Similarity-Aware Inverted Indizes, deutlich mehr spezifische
Blockingprädikate, die zu einem guten Blocking Schema führen, zu betrachten.
Damit dies möglich ist, wurde der globale bzw. attributsglobale Record
Identifier Index (RI), in seiner bisherigen Form, aufgesplittet und in den Block
Index (BI) verschoben. Der Similarity Index bleibt unverändert und verlinkt
weiterhin Attribute eines Blockes mit ihren Ähnlichkeiten. In
@fig:mdysim_example ist ein solcher Index aus den Datensätzen der Tabelle links
oben und dem Blocking Schema `(CommonToken, Name)` $\land$ `(CommonToken,
Kitchen)` erstellt worden. Ein Datensatz besteht aus den beiden Attributen
Restaurantname `Name` und der Küchenart `Kitchen`. Anhand des Blocking Schema
wurden durch Algorithmus \ref{alg:bkvs} fünf Blockschlüssel generiert, nämlich
`tonysitalian`, `tonysvegi`, `pizzaitalian`, `pizzavegi` und `gelatovegi`. Jeder
Block beinhaltet weiterhin die Attribute, welche zum entsprechenden
Blockschlüssel gehören. Die große Veränderung des MDySimIII ist , dass jedes
Attribut eines Blockes einen eigenen Record Identifier Index besitzt.

```texalgo
#alg:mdysimIII_insert MDySimIII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $BI, SI$
  \end{itemize}
}
\Statex\color{gray}
\For{fields $f \in F$}
  \State Initialize $BI_f = \{\}$, $SI_f = \{\}$
\EndFor
\For{records $r \in D$}
  \For{terms $t \in BS$}
    \State $bkvs = BlockingKeyValues(t, r)$
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \State Add $r.f$ to $BI_f[bkv]$\color{black}
        \State $ri = bi[r.f]$
        \State Add $r.id$ to $ri$
        \State $BI_f[bkv] = ri$\color{gray}
        \State Initialize inverted index list $si = ()$
        \For{attribute $a \in bi$}
          \If{$a \notin SI_f$}
            \State $sim = S_f(r.f, a)$
            \State Append $(r.f, sim)$ to $SI_f[a]$
            \State Append $(a, sim)$ to $si$
          \EndIf
        \EndFor
        \State $SI_f[r.f] = si$
      \EndFor
    \EndFor
  \EndFor
\EndFor

#alg:mdysimIII_query MDySimIII - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex\color{gray}
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index\color{black}
\For{terms $t \in BS$}
  \State $bkvs = BlockingKeyValues(t, r)$
  \For{$bkv \in bkvs$}
    \For{fields $f \in t.fields$}
      \State $bi = BI_f[bkv]$
      \For{attribute $r.f \in bi$}
        \If{$q.f = r.f$}
          \For{identifier $r.id \in bi[q.f]$}
            \State $M[(r.id, f)] = 1.0$
          \EndFor
        \Else
          \State $si = SI_f[q.f]$
          \State $sim = si[r.f]$
          \For{identifier $r.id \in bi[r.f]$}
            \State $M[(r.id, f)] = sim$
          \EndFor
        \EndIf
      \EndFor
    \EndFor
  \EndFor
\EndFor\color{gray}
\State return $M$
```

Aufgrund der Änderungen am RI hat sich die Build-Phase an zwei Stellen leicht
verändert. In Algorithmus \ref{alg:mdysimIII_insert} ist das Bauen des Index
gezeigt. In grau sind die Schritte markiert, welche sich gegenüber Algorithmus
\ref{alg:mdysim_insert} nicht verändert haben. Die erste Veränderung ist, dass
das initiale Hinzufügen der Attribute, in ihren attributsspezifischen RI
wegfällt, da dieser so nicht mehr existiert. Die zweite Veränderung (Zeilen
10-12) ist beim Hinzufügen eines Attributes in einen Block. Nachdem das Attribut
wie gewohnt in den Block eingefügt wurde, wird dessen RI geholt (Zeile 10), zu
welchem anschließend der Datensatzidentifier hinzugefügt wird (Zeile 11). Wird
der MDySimIII für den DNF Blocks Lerner gebaut, werden analog zum MDySimII
lediglich die Datensatzidentifier in die Blöcke eingefügt. Die Verknüpfung mit
den Attributen ist dabei irrelevant, da jeder Datensatz, der sich im Block
befindet, ungeachtet seines zugehörigen Attributswertes in der Kandidatenmenge
landet. Zudem wird auch hier die Erzeugung der SIs übersprungen.

Im Gegensatz zur Build-Phase hat sich die Query-Phase grundlegend verändert, um
an die Identifier in den Blöcken zu gelangen, müssen für den Anfragedatensatz,
die Blockschlüssel generiert werden (siehe Algorithmus
\ref{alg:mdysimIII_query}). Diese werden nacheinander aus den Termen $t$ erzeugt
(Zeilen 3-4). Für jeden Blockschlüssel werden anschließend die entsprechenden
Blöcke der, an dem Ausdruck beteiligten Attribute, betrachtet (Zeilen 5-7). Für
jedes Attribut wird zunächst überprüft, ob dies dem eigenen entspricht. Ist dies
der Fall, wird der Ähnlichkeitsvektor der Kandidaten im RI $bi[q.f]$, mit dem
Ähnlichkeitswert 1.0 ergänzt (Zeilen 9-12). Falls die Attribute unterschiedlich
sind, wird die Ähnlichkeit $sim$ im SI nachgeschlagen (Zeile 15) und der
Ähnlichkeitsvektor der Kandidaten im RI $bi[r.f]$, mit dem Ähnlichkeitswert
$sim$ ergänzt. Zum Schluss wird eine zum MDySimII identische Kandidatenmenge $M$
zurückgegeben.

Anstatt, wie vorher alle Datensatzidentifier des gleichen Attributes global zu
gruppieren, werden hier ausschließlich diejenigen, in denselben RI, eingefügt,
die auch denselben Blockschlüssel haben. Beispielsweise werden in
@fig:mdysim_example, im BI des Namen, über den Blockschlüssel `tonysitalian`
`r1` und `r3` zusammen einem RI zugeordnet, nicht jedoch `r4`, welcher keinen
gemeinsamen Blockschlüssel zu `r1` oder `r3` hat. Die Anzahl der Blöcke sind
identisch zum MDySimII Verfahren. Allerdings wird ein Attribut $r.f$ $k$-Mal mit
seinem Datensatzidentifier verknüpft (vgl. MDySimII $|F|$-Mal und DySimII
1-Mal), wobei $$k = \sum_{t \in BS}|BlockingKeyValues(t, r.f)|.$$ Beispielsweise
`r1` viermal, `r2` fünfmal, `r3` neunmal und `r4` auch viermal. Das hat zur
Folge, dass der MDySimIII mehr Speicherplatz benötigt, als seine Vorgänger. Wie
viel größer der Bedarf ist, hängt von der konkreten Verteilung der Daten ab.

## Freie Parameter

Ein kompletter Entity Resolution Workflow, wie in Kapitel 2 betrachtet, führt
eine Reihe von Schritten aus, um Datensätze, die die dieselbe Entität
beschreiben, zu finden. Für ein statisches Entity Resolution System lassen sich
diese Schritte grob in vier Phasen gliedern. Zunächst die Vorverarbeitung, um
offensichtliche Fehler zu korrigieren, gefolgt vom Blocking, welches die
Komplexität der Suche reduziert, dem Matching, mit der Berechnung der
Ähnlichkeiten im Paarvergleich und der Klassifizierung in Matches und
Non-Matches und abschließend der Nacharbeitung, um Gruppen von Duplikaten zu
bilden. Der dynamische Entity Resolution Workflow besteht aus zwei Phasen.
Zunächst der Build-Phase, um einen Index zu erzeugt, damit effizient nach
Duplikaten gesucht werden kann. Die eigentliche Entity Resolution wird in der
Query-Phase durchgeführt. Während der Build-Phase wird eine bestehender
Datenbestand vorverarbeitet und geblockt. Optional können Ähnlichkeiten
vorausberechnet. In der Query-Phase werden Anfragen aus einem Datenstrom
behandelt. Jede Anfrage wird derselben Vorverarbeitung, wie die Datensätze der
Build-Phase, unterzogen und wird ebenfalls geblockt. Das Ergebnis des Blockings
ist eine Kandidatenmenge. Im Matching Schritt wird deren Ähnlichkeiten zur
Anfrage nachgeschlagen bzw. berechnet und die Kandidaten werden in Matches und
Non-Matches klassifiziert. Die Nachbearbeitung entfällt, die klassifizierten
Matches bereits eine Gruppe bilden.

Im Folgenden wird das Problem der freien Parameter betrachtet. Die Definition
laut Wikipedia[^4] ist:

> Ein freier Parameter ist eine Variable eines mathematischen Modells, die vom
> Modell nicht exakt vorhergesagt bzw. eingeschränkt werden kann und daher
> experimentell oder theoretisch geschätzt werden muss.

[^4]: https://en.wikipedia.org/wiki/Free_parameter

Ein System zur dynamischen Entity Resolution besteht aus mindestens vier
Komponenten. Einer Datenquelle, einer Vorverarbeitungspipeline, einem Indexer
und einem Klassifikator. Neben der Wahl geeigneter Verfahren und Algorithmen für
die genannten Komponenten, ist die größte Schwierigkeit, die vielen freien
Parameter auf die Datenquelle anzupassen. Werden beispielsweise die Parameter
des DySimII Blocking Verfahren aus @sec:dysimII betrachtet, so wird pro
Attribute einen Blockschlüssel und eine Ähnlichkeitsfunktion benötigt. Bei einem
Datensatz mit fünf Attributen, sind dies bereits 10 Parameter. Beim Matching
kommen Parameter für die Ähnlichkeitsfunktionen, beispielsweise die Kosten der
Operationen (einfügen, ersetzen, löschen) bei der Levenshtein Distanz, welche
auf ein Attribut optimiert werden. Bei einem Attribut pro Ähnlichkeitsfunktion
und beispielsweise abweichenden Werten für die Kosten der Einfügeoperation,
kommen weitere fünf Parameter hinzu. Beim Matching kann, beispielsweise ein
simpler Schwellenwertklassifikator mit lediglich einer Schwelle genutzt werden.
In dieser Konstellation (ohne Vorverarbeitung) mit Blocking Verfahren,
verschiedenen Ähnlichkeitsfunktionen und einem Klassifikator, kommt das
ER-System bereits auf 16 Parameter. Die freien Parameter manuell zu bestimmen,
ist selbst mit einer cleveren Strategie, die Parameter auszuprobieren, sehr
zeit- und kostenintensiv. Besonders bei großen Datenmengen kann dieses Trial and
Error Verfahren sehr lange dauern, da das Ausprobieren mehrere Stunden, wenn
nicht Tage dauert. Noch schwieriger wird es, wenn keine Ground Truth Daten
vorhanden sind. Dadurch entfällt größtenteils die Möglichkeit die eingestellten
Parameter effektiv und qualitativ zu überprüfen.

Für den Erfolg einer Anfrage müssen zwei Eigenschaften erfüllt sein. Zunächst
benötigt es ein Blocking Verfahren, das, bei gegebenen Anforderungen an die
Latenzen, in der Lage ist die Duplikate, in den existierenden Daten, als
Kandidaten zu selektieren. Anschließend benötigt es einen Klassifikator, der
zuverlässig Matches von Non-Matches aus der Kandidatenmenge filtert.
Dementsprechend sind die wichtigsten freien Parameter, die des Blocking Schema,
die Auswahl der Ähnlichkeitsmaße, die für ein Attribut entscheidende
Unterschiede zwischen Matches und Non-Matches messen und die Parameter eines
Klassifikator, damit dieser die Ähnlichkeiten bestmöglich interpretieren kann.
In @sec:ana_bs wurde bereits ein Verfahren untersucht, um automatisiert ein
Blocking Schema zu erlernen, dass von einem Indexer zum Blocking genutzt werden
kann. Des Weiteren wurden in @sec:ana_lbl zwei Verfahren beschrieben, um eine
Ground Truth zu erzeugen, damit Verfahren und entsprechend die eingestellten
freien Parameter bewertet werden können. Noch zu bestimmen ist ein Verfahren,
dass geeignete Ähnlichkeiten für die Attribute auswählt, sowie ein Verfahren,
dass die Parameter für einen Klassifikator bestimmt.

Aufgrund der oben genannten Gründe ist die manuelle Bestimmung der freien
Parameter oft nicht pratikabel. Dem kann ein Entity Resolution System Abhilfe
verschaffen, dass selbständig und vor der Laufzeit, die freien Parameter
bestimmen kann. Die von einem System bestimmten freien Parameter, werden im
Folgenden als Konfiguration bezeichnet. Ein System zur selbstständigen
Bestimmung seiner Konfiguration kann das Problem der freien Parameter jedoch
nicht vollständig lösen, da die Verfahren zum Lernen ebenfalls parametrierbar
sind und dadurch neue freie Parameter miteinbringen. Beispielsweise die maximale
Disjunktion bzw. Konjunktion des Blocking Schema in Algorithmus \ref{alg:dnf}.
Trotzdem können auf diese Weise die kritischsten Teile der Konfiguration, die
zum einen am meisten Zeit zum Einstellen und zum anderen maßgeblich die Qualtiät
und die Effektivität des Gesamtsystems beeinflussen, ermittelt werden, sodass
für ein solides System, Feinjustierungen ausgenommen, nicht zwangsweise
Domänexperte benötigt wird.
