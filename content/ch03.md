# Analyse

## DySimII

### Problem: DNF-Blocking

Vermeiden von Blöcken mit max 1 var 1 und Kandidatenmenge max 1 var 1, da diese
keinen Mehrwert bringen.

### Problem: Kandidatenmenge

Der DySimII Index hat das Problem, dass er die Kandidatenmenge nicht gut
kontrollieren kann. Daher müssen die Attribute, welche durch den DySimII
indiziert werden sollen, mit bedacht gewählt werden. Attribute wie das
Geschlecht, Nationalität oder Bundesland führen dazu, dass über den Record Index
eine riesige Anzahl an Kandidatenpaaren selektiert werden. So werden
beispielsweise, beim Überprüfen eines Datensatzes mit Deutscher Nationalität
alle Deutschen als Kandidaten gewählt. Dadurch wird die Reduction Ratio des
DySimII Algorithmus dramatisch verschlechtert.

Beispiel: Restaurant, Ferbl-9k, Ferbl-90k

Zwar kann über die Eingrenzung auf die Besten $n$ bzw. durch das Setzen einer
Schranke für die Mindestähnlichkeit ein Großteil der Kandidaten ausgeschlossen
werden, dennoch muss bleibt die Reduction Ratio gleich, da jeden einzelnen die
Ähnlichkeit aus dem Index geholt und abgeglichen werden muss

**Lösung 1:** Die offensichtlichste Lösung ist, Attribute, welche rießige
Kandidatenmengen erzeugen nicht zu indizieren. Allerdings kann es durchaus
vorkommen, dass die Konjunktion im DNF-Schema mit anderen Attributen Block Key
Values erzeugt, die eine hohe Präzision ermöglichen. In diesem Fall landet das
Attribut zwangsweise im Record Index.

Vgl.[@CG:Scalable:08][@RCL.EA:Dynamic:13] State mit Sicherheit nicht gewählt!

**Lösung 2:** Die zweite Lösung setzt voraus, dass die Blockschlüssel so gewählt
wurden, dass Kandidatenmengen nicht zu groß werden können. Das Löschen des
Record Index ist keine Option, da dadurch die Zuordnung zu den eigentlichen
Datzensätzen wegfällt. Um dennoch den Einträge im Record Index zu minimieren
wird dieser in den Blocking Index verschoben. D.h. jedes Attribut eines Blockes
verweist nun auf die Datensätze, welche nicht lediglich das gleiche Attribute
haben, sondern den gleichen Blockschlüssel. Statt pauschal alle Deutschen bei
einer Anfrage als Kandidaten zu selektieren, werden dadruch beispielsweise nur
die Deutschen, die ein der selben Stadt wohnen und den gleichen Nachnamen haben
ausgewählt.

Beispiel/Bild/Pseudo-Code

Dieses Verfahren erhöht zwar die Komplexität etwas von $O(?)$ auf $O(?)$
erhöht aber gleichzeitig die Reduction Ratio dramatisch.

Vgl. MDySimII, MDySimIII

## Problem: Weak Labels

Dabei empfiehlt es sich die Daten vorher zu sortieren, um deterministische
Ergebnisse zu erzielen.

## Ähnlichkeitsmetriken

Aus der Vielfalt der möglichen Ähnlichkeitsmaße gibt es keines das allen
anderen klar überlegen ist. Es ist daher sehr domainabhängig, welcher
Algortihmus gute Ergebnisse liefert. Beim Vergleich von Datensätzen sind diese
Domänen meist durch die unterschiedlichen Attribute getrennt. Daher ist es
notwendig herauszufinden, für welches Attribute welche Ähnlichkeitsmetric
besonders gut funktioniert. Daraus folgt das Problem der Vergleichbarkeit der
Ähnlichkeitsmetriken. Für zwei Strings $a$ und $b$ liefert der
Jaccard-Koeffizient beispielweise Werte zwischen 0 und 1, die
Levenshtein-Distanz hingegen Werte zwischen 0 und $maxlen(a, b)$. Deshalb ist
es notwendig die verschiedenen Ähnlichkeitsmaße zu normalisieren. Dafür wird
das Intervall von 0 bis 1 gewählt, wobei 1 totale Übereinstimmung und 0 keine
Übereinstimmung bedeutet.

### Edit-distance

Die Edit-distance ist eine der beliebtesten und meistgenutzten Metriken, um die
Ähnlichkeit zweier Strings zu bestimmen. Dabei bestimmt die Edit-distance die
benötigten Schritte um einen String in einen anderen zu überführen. Dafür werden
die Tranformationen einfügen, löschen und ersetzen im klassischen Verfahren von
Levenshtein und zusätzlich transponieren in der Erweiterung von
Damerau eingesetzt. Das Ergebnis sind die Anzahl der benötigten
Transformationen. Diese Anzahl alleine ist noch kein genaues Maß zur Bestimmung
der Ähnlichkeit, da zwei Transformationen in einem kürzeren String kritischer
sind als in einem langen. Um die Ähnlichkeit zu normalisieren gibt es zwei
Möglichkeiten. Entweder auf Basis des Transformationspfades oder der
Stringlänge. Es ist zu beachten das beide Methoden nicht der Dreiecksungleichung
stand halten.

Eine Erweiterung der Edit-distance ist die Tranformationen zu gewichten. Dazu
wird jedem Transformationstyp ein positiver reeller Kostenfaktor zugewiesen, mit
welchem die Anzahl seiner Transformationen gewichtet wird. Durch die Gewichtung
ist es allerdings nicht mehr möglich wie bei uniformer Gewichtung zu
Normalisieren. Eine Möglichkeit, welche aus die Dreiecksungleichung erfüllt
stellen Yujian & Bo in [@YB:Normalized:07] vor.

Durchprobieren der Gewichte, welche Ergebnisse liefert das? Ist es überhaupt
Aussagekräfig?

## Berechnung der Metriken für Real-time ER

Können nicht pauschal auf den Index berechnet werden, sondern sind Kennziffern,
die kontinuierlich, mit dem verarbeiten von Anfragen, berechnet und akkumuliert
werden.

* Pair Completeness
* Reduction Ratio
