# Analyse {#chap:analysis}

Ein kompletter Entity Resolution Workflow, wie in Kapitel 2 betrachtet, führt
eine Reihe von Schritten aus. Diese Schritte lassen sich grob in vier Phasen
gliedern. Zunächst die Vorverarbeitung, um offensichtliche Fehler zu
korrigieren, gefolgt vom Blocking, welches die Komplexität der Suche reduziert,
dem Matching, mit der Berechnung der Ähnlichkeiten im Paarvergleich und der
Klassifizierung in Matches und Non-Matches und optional die Nacharbeitung, um
Gruppen von Duplikaten zu erkennen. Ein System oder Framework zur dynamischen
Entity Resolution besteht dementsprechend aus mindestens vier Komponenten. Einer
Datenquelle, einer Vorverarbeitungspipeline, einem Indexer und einem
Klassifikator. Neben der Wahl geeigneter Verfahren und Algorithmen für die
genannten Komponenten, ist die größte Schwierigkeit die vielen Parameter auf die
Datenquelle anzupassen. Werden beispielsweise die Parameter des DySimII Blocking
Verfahren aus @sec:dysimII betrachtet, so wird pro Attribut ein Blockschlüssel
und eine Ähnlichkeitsfunktion benötigt. Bei einem Datensatz mit fünf Attributen,
sind dies bereits 10 Parameter. Beim Matching kommen Parameter für die
Ähnlichkeitsfunktionen, beispielsweise die Kosten der Operationen (einfügen,
ersetzen, löschen) bei der Levenshtein Distanz, welche auf ein Attribut
optimiert werden. Bei einem Attribut pro Ähnlichkeitsfunktion und beispielsweise
abweichenden Werten für die Kosten der Einfügeoperation, kommen weitere fünf
Parameter hinzu. Für das Matching kann ein simpler Schwellenwertklassifikator,
mit lediglich einer Schwelle, genutzt werden. In dieser Konstellation (ohne
Vorverarbeitung) mit Blocking Verfahren, verschiedenen Ähnlichkeitsfunktionen
und einem Klassifikator, kommt das ER-System bereits auf 16 Parameter. Diese
Parameter werden als freie Parameter bezeichnet und sind laut Wikipedia[^4]
folgendermaßen definiert:

> Ein freier Parameter ist eine Variable eines mathematischen Modells, die vom
> Modell nicht exakt vorhergesagt bzw. eingeschränkt werden kann und daher
> experimentell oder theoretisch geschätzt werden muss.

[^4]: https://en.wikipedia.org/wiki/Free_parameter

Die freien Parameter manuell zu bestimmen, ist selbst mit einer cleveren
Strategie, die Parameter auszuprobieren, sehr zeit- und kostenintensiv.
Besonders bei großen Datenmengen kann dieses Trial and Error Verfahren sehr
lange dauern, da das Ausprobieren mehrere Stunden bis Tage in Anspruch nimmt.
Noch schwieriger wird es, wenn keine Ground Truth Daten vorhanden sind. Dadurch
entfällt größtenteils die Möglichkeit die eingestellten Parameter effektiv und
qualitativ zu überprüfen. Aufgrunddessen ist die manuelle Bestimmung der freien
Parameter oft nicht pratikabel. Dem kann ein Entity Resolution System Abhilfe
verschaffen, dass selbständig die freien Parameter bestimmt. Die von einem
System bestimmten freien Parameter, werden im Folgenden als Konfiguration
bezeichnet. Ein System zur selbstständigen Bestimmung seiner Konfiguration kann
das Problem der freien Parameter jedoch nicht vollständig lösen, da die
Verfahren zum Lernen ebenfalls parametrierbar sind und dadurch neue freie
Parameter miteinbringen. Trotzdem können auf diese Weise die kritischsten Teile
der Konfiguration ermittelt werden, die zum einen am meisten Zeit zum Einstellen
und zum anderen maßgeblich die Qualitiät und die Effektivität des Gesamtsystems
beeinflussen, sodass für ein solides System, Feinjustierungen ausgenommen, nicht
zwangsweise ein Domänexperte benötigt wird.

Für den Erfolg einer Anfrage müssen zwei Eigenschaften erfüllt sein. Zunächst
benötigt es ein Blocking Verfahren, dass bei gegebenen Anforderungen an die
Latenzen, in der Lage ist die Duplikate in den existierenden Daten als
Kandidaten zu selektieren. Anschließend benötigt es einen Klassifikator, der
zuverlässig Matches von Non-Matches aus der Kandidatenmenge filtert.
Dementsprechend sind die wichtigsten freien Parameter, die des Blocking Schema,
die Auswahl geeigneter Ähnlichkeitsmaße, die für ein Attribut entscheidende
Unterschiede zwischen Matches und Non-Matches messen und die Parameter eines
Klassifikator, damit dieser die Ähnlichkeiten bestmöglich interpretieren kann.

Damit es möglich ist verschiedene Konfigurationen zu vergleichen und zu
bewerten, benötigt es eine Ground Truth. In @sec:ana_lbl werden zwei Verfahren
beschrieben, die eine Ground Truth zu diesem Zweck syntetisieren. Des Weiteren
wird in @sec:ana_bs ein Verfahren untersucht, um automatisiert ein Blocking
Schema zu erlernen, dass von einem Indexer zum Blocking genutzt werden kann.
Anschließend wird in @sec:bkv_gen betrachtet, wie anhand eines Blocking Schema
für einen Datensatz Blockschlüssel erzeugt werden können. In @sec:anaindxer
werden Blocking Verfahren analysiert und entwickelt, die das erlernte
Blocking Schema nutzen können und die Anforderungen aus @sec:anf erfüllen.
Danach wird ein Verfahren entwickelt, das geeignete Ähnlichkeiten für die
Attribute auswählt und zum Schluss werden die Möglichkeiten zum Trainieren eines
Klassifikators untersucht.

## Generieren von Labels {#sec:ana_lbl}

Mit Hilfe einer Ground Truth können Metriken zu Qualität und Effizienz eines
Verfahrens bzw. einer Kette von Verfahren berechnet werden, wodurch die
Ergebnisse in Relation zu anderen Parametern, Komponenten oder Daten gesetztet
werden können. Für den Fall, dass zu einem Datensatz keine Ground Truth
existiert, ist die Bewertung des Ergebnisses entsprechend schwierig. Der bereits
vorgestellte Algorithmus vom Kejriwal & Miranker [@KM:Unsupervised:13] zur
Erzeugung von schwachen Labels bietet hierfür zumindest die Möglichkeit eine
Tendenz zu bekommen, die aussagt wie Qualität und Effizienz eines Verfahrens zu
bewerten sind.

Der Algorithmus \ref{alg:weaklabels} nutzt dazu Token, in Form von Wörtern, für
das Blocking der Datensätze, sowie zum Ermittlen der Ähnlichkeiten. Im Gegensatz
zu einem Vollvergleich der Zeichenketten, beispielsweise durch eine
Editierdistanz, wird dadurch die Komplexität auf Kosten der Genauigkeit
reduziert. Die Genauigkeit ist gegenüber den Editierdistanzen niedriger, da
lediglich auf Wörterebene miteinander vergleichen wird. Je mehr gemeinsame
Wörter, desto ähnlicher ist ein Datensatzpaar. Hierdurch ist es beispielsweise
nicht möglich Rechtschreibfehler zu erkennen, da der Algorithmus diese als zwei
unterschiedliche Wörter behandelt. Die Gesamtkomplexität des Algorithmus beträgt
$O(n + nm + nm)$, welche sich in die Erzeugung der TF/IDF Statistik ($O(n)$),
die Erzeugung der Blöcke über $m$ Attribute ($O(nm)$) und die Erzeugung der
Kandidatenpaare $O(nm)$ gliedert. Bei diesem Algorithmus ist kritisch zu
betrachten, dass ein Großteil der Datensatzpaare, aufgrund der Lücke zwischen
den Schwellen, nicht für die Ground Truth ausgewählt werden kann.
@fig:weaklbl_problem illustriert diese Lücke zwischen einer unteren Schwelle bei
0.1 und oberen Schwelle bei 0.7. Für alle Paare $p$ gilt, wenn $lt \leq sim(p) <
ut$, dann folgt $p \notin P \cup N$. Dadurch ist in vielen Fällen die generierte
Ground Truth gegenüber dem Datensatz, nicht sonderlich repräsentativ, da viele
aussagekräftige Paare ausgeschlossen werden. Kejriwal & Miranker haben in
[@KM:Unsupervised:13] die Werte für die Schwellen und der Fenstergröße empirisch
an drei relativ kleine Datensätzen (< 10.000 Einträge) getestet. Ob die
Ergebnisse sich auf deutlich größere Datesätze anwenden lassen, wird in
@sec:free_params evaluiert.

```{.a2s #fig:weaklbl_problem
    caption="Darstellung der Lücke zwischen oberer und unterer Schwelle, des
    Algorithmuses des Label Generator ohne Ground Truth (GT), innerhalb welcher
    Paare nicht für die Ground Truth ausgewählt werden können."}
     lt=0.1                             ut=0.7
       |                                  |
       |    Pairs not available for GT    |
       |<-------------------------------->|
       :                                  :
 0     v                                  v                 1
 <---------------------------------------------------------->
                      TF/IDF Similarity
```

Aufgrund der Verteilung von Matches und Non-Matches, die bis auf wenige
Ausnahmen, immer ein deutliches Ungleichgewicht zugunsten der Non-Matches
aufweist, werden für alle öffentlich verfügbaren Datensätze mit Ground Truth,
lediglich die Matches angegeben. Dementsprechend sind alle Datensatzpaare,
welche nicht in den Matches der Ground Truth enthalten sind, als Non-Matches zu
interpretieren. Das bedeutet, dass aus der riesigen Menge von Non-Matches eine
repräsentative Stichprobe gezogen werden muss, anhand welcher Qualität und
Effizienz, effektiv beurteilt werden können.

```{.texalgo #alg:labels caption="WeakTrainingSet with Ground Truth"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset: $D$
  \item Ground Truth $GT$
  \item Blocking Window Size: $c$
  \end{itemize}
}
\Statex \Statex \Statex
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item A set of positive samples: $P$
  \item A set of negative samples: $N$
  \end{itemize}
}
\Statex\color{gray}
\State Initialize set $P = ()$, set $N = ()$\label{alg:wk:inits}
\State Initialize set of tuple pairs $C = ()$
\State Generate TFIDF statistics of $D$
\For{fields $f \in D$}
    \For{records $r \in D$}
        \State Tokenize $r_f$ into $BKV_f$
        \State Block $r$ on generate tokens for field $f$
    \EndFor
\EndFor
\For{block $B$ generate in previous step}
    \State  Slide a window of size c over tupels in $B$
    \StatexIndent[1] Generate all possible pairs within window and
    \StatexIndent[1] add to $C$
\EndFor\color{black}\label{alg:wk:inite}
\For{pairs $(t_1, t_2) \in GT$}\label{alg:wk:gts}
    \State Add $(t_1, t_2)$ to $P$
    \If{$p \in C$}
        \State Remove $p$ from $C$
    \EndIf
\EndFor\label{alg:wk:gte}
\State Initialize dictionary $M = \{\}$
\For{pairs $(t_1, t_2) \in C$}\label{alg:wk:tfidfs}
    \State Compute TFIDF similarity $sim$ of $(t_1, t_2)$
    \State $M[(t_1, t_2)] = sim$
\EndFor\label{alg:wk:tfidfe}
\State Calculate probability distribution over $M$\label{alg:wk:hist}
\State $max_n = |GT| * 5$
\For{$i = 1$ \textbf{to} $max_n$}\label{alg:wk:ss}
    \State Choose a pair $p$ from $M$ based on probability distribution\label{alg:wk:smpl}
    \State Add $p$ to $N$
\EndFor\color{gray}\label{alg:wk:se}
\State Return $P$ and $N$
```

Um Non-Matches für eine Ground-Truth auszuwählen, müssen Paare gebildet und
deren Ähnlichkeit berechnet werden. Die gesamten Non-Matches zu bilden und deren
Ähnlichkeiten zu berechnen ist nicht effektiv, da diese Funktion bekannterweise
quadratisch (O(n^2^) ist. Für ihr Verfahren zur Bestimmung einer schwachen
Ground Truth haben Kejriwal & Miranker ein Verfahren entwickelt, das Paare
bildet und deren Ähnlichkeit berechnet. Anstatt durch zwei Schwellen Matches und
Non-Matches zu trennen, ist es bei vorhandener Ground Truth möglich, die Matches
aus der Gesamtmenge der gebildeten Paare zu entfernen, sodass lediglich die
Non-Matches übrig bleiben. Da das Blocking der Paare anhand von Token
durchgeführt wurde, sind zum einen wenig substantielle Paare mit einer
Ähnlichkeit nahe 0 ausgeschlossen worden und zum anderen substantielle Paare mit
teils hoher Ähnlichkeit erzeugt worden. Ein Klassifikator beispielsweise kann
relativ einfach Non-Matches mit geringer Ähnlichkeit von Matches trennen. Damit
aber herausfordernde Paare, die sich etwa nur in einem Attribut unterscheiden,
als Non-Match klassifiziert werden und nicht fälschlicherweise als Match
betrachtet werden, wird eine Menge dieser uneindeutigen Paare benötigt. Aufgrund
dessen bietet die Kandidatenerzeugung aus [@KM:Unsupervised:13] eine gute
Approximation, um schnell und effektiv gute Non-Matches zu erhalten. In
Algorithmus \ref{alg:labels} wird ein Verfahren beschrieben, das die
Kandidatenerzeugung aus Algorithmus \ref{alg:weaklabels} nutzt, um damit eine
Menge von Non-Matches zu bestimmen. Gegeben ist die Ground Truth in Form von
Matches. Davon ausgehend wird eine repräsentative Menge von Non-Matches aus dem
Datensatz $D$ selektiert. Die ersten drei Schritte: das Generieren der TF/IDF
Statistik, das Blocken durch die Token und das Erzeugen der Kandidatenmenge $C$
(Zeilen \ref{alg:wk:inits}-\ref{alg:wk:inite} in grau), sind daher identisch zum
ursprünglichen Algorithmus. Nachdem die Kandidatenmenge erzeugt wurde, werden
zunächst alle Ground Truth Matches nach $P$ übernommen (Zeilen
\ref{alg:wk:gts}-\ref{alg:wk:gte}). Zusätzlich werden alle Matches aus der
Kandidatenmenge $C$ entfernt, sodass diese ausschließlich Non-Matches
beinhaltet. Anschließend wird ebenfalls die TF/IDF-Ähnlichkeit der Paare in $C$
ermittelt und in $M$ zwischengespeichert (Zeilen
\ref{alg:wk:tfidfs}-\ref{alg:wk:tfidfe}). Anhand dieser wird die
Wahrscheinlichkeitsverteilung der Ähnlichkeiten in $M$ ermittelt, beispielsweise
durch ein Histogramm (Zeile \ref{alg:wk:hist}). In @fig:label_sampling ist
beispielhaft eine Verteilung von Non-Matches (`-` Symbol) dargestellt. Die
X-Achse gibt den Ähnlichkeitswert der Paare an. Die Häufung auf der Y-Achse
illustriert, wie viele Paare eine entsprechende Ähnlichkeit haben. Eine Häufung
ist vor allen Dingen im unteren Ähnlichkeitsbereich zu erwarten. Durchaus
möglich sind allerdings auch größere Anhäufungen im mittleren Bereich, da durch
das Blocking der Großteil der Paare mit Ähnlichkeit 0 ausgeschlossen worden ist.
In Zeile \ref{alg:wk:smpl} werden nun Non-Matches, anhand der
Wahrscheinlichkeitsverteilung, zufällig aus $M$ gezogen, sodass Paare innerhalb
einer großen Anhäufung (z.B. unterer Bereich) häufiger ausgewählt werden als
Paare in kleinen Anhäufungen (z.B. oberer Bereich). Damit wird erreicht, dass
die Menge der für die Ground Truth gewählten Non-Matches möglichst
repräsentativ ist. Die Ziehung wird $max_n$-Mal wiederholt bzw., solange bis
keine Paare mehr übrig sind (Zeilen \ref{alg:wk:ss}-\ref{alg:wk:se}). Zum
Schluss wird analog zum ursprünglichen Algorithmus die Grund Truth bestehend aus
$P$ und $N$ zurückgegeben.

```{.a2s #fig:label_sampling
    caption="Beispielhalfte Verteilung von Non-Matches ('-' Symbol) auf der über
    die Ähnlichkeit (X-Achse) zwischen 0 und 1. Wie viele Non-Matches einen
    bestimmten Ähnlichkeitswert haben, wird durch die Häufung (Y-Achse)
    dargestellt."}
 Cluster-Size

      ^
      |     -
      |    - -
      |   - - -                         -
      |   -  - -                       - -
      |  - -  - -                      - - -
      |  - -  -  -            -      -   -  -
      |  - -  -   - -       -  -     -  - -  -            -
      |  - - -  - - -      -  -     -  -   - -           - - -
      | - - -  - -  - - -  - - -    - -  - - -  - -      - - -  - -                  - -
      +--------------------------------------------------------------------------------------------------->
      0                                            0.5                                                    1

                                              TF/IDF Similarity
```

## Lernen von Blocking Schemata {#sec:ana_bs}

Kejriwal & Miranker haben ein Verfahren entwickelt, dass in Algorithmus
\ref{alg:dnfbs} vorgestellt wurde. Darin wird ein Blocking Schema erzeugt, indem
Ausdrücke anhand der Fisher-Score bewertet werden. Die berechnete Fisher-Score
drückt für einen Ausdruck $t$, in Abhängigkeit der Groud Truth $P$ und $N$, die
Blockschlüsselabdeckung (engl. blocking key coverage) aus. Laut Ramadan &
Christen [@RC:Unsupervised:15] führt eine hohe Schlüsselabdeckung zu einer hohen
Pair Completeness, sodass viele Matches in einen gemeinsamen Block gruppiert
werden, während die Anzahl an Non-Matches, die zusammen einem Block zugeordnet
werden, gering gehalten wird. Durch dieses Verfahren werden für einen Entity
Resolution Workflow hochqualitative Blöcke erzeugt. Für dynamische Verfahren,
die Anfragen im Subsekundenbereich beantworten und möglichst gleiche Latenzen
haben sollen, ist aufgrund der niedrigen Dichte von Duplikaten (vgl. Anzahl
gesamter Matches vs gesamter Non-Matches), die Fisher-Score als
Bewertungskriterum ungeeignet. Zwar werden für die Duplikate Blöcke generiert,
die es erlauben (möglichst) alle zur Anfrage passenden Entitäten schnell und
präzise zu erhalten, allerdings ist der Großteil aller Anfragen ergebnislos.
Ergebnislos in diesem Zusammenhang bedeutet, dass es zu einer Anfrage keinen
Datensatz gibt, der derselben Entität entspricht. Das Problem ist, dass die
Fisher-Score für diese Datensätze keine Aussage trifft, weshalb die generierten
Blöcke, in welchen sich keine Matches befinden, zum Teil sehr groß werden.
Aufgrund der Menge dieser Anfragen, wird die Effektivität des ER Systems
dramatisch reduziert.

In [@RC:Unsupervised:15] erweitern Ramadan & Christen den Algorithmus von
Kejriwal & Miranker, um eine Bewertungsfunktion für Entity Resolution in nahe
Echtzeit. Die Fisher-Score $FS$ wird weiterhin genutzt um die
Blockschlüsselabdeckung auszudrücken. Dies ist jedoch für dynamsiche Blocking
Verfahren alleine nicht ausreichend, weil die Bewertung eines Ausdrucks
zusätzlich von der Blockgröße und der Verteilung der Blöcke abhängt. Das
Kontrollieren der Blöckgröße für alle Blöcke, auch solche die lediglich
Non-Matches enthalten, sorgt dafür, dass alle Anfragen in nahe Echtzeit
beantwortet werden können. Zur Bewertung der Blöckgröße wird die
durchschnittliche Anzahl an Datensätzen pro Block ermittelt $S_{b_{(ave)}} =
ave{|b|: b \in B}$, wobei mit $B$ alle von einem Blocking Verfahren erzeugten
Blöcke bezeichnet werden. Zusätzlich wird die maximale Blockgröße erhoben
$S_{b_{(max)}} = max{|b|: b \in B}$. Erzeugt ein Ausdruck einen Block, der
größer eines Schwellwertes ist, wird dieser Ausdruck verworfen. Über die
Verteilung der Blöcke wird geprüft, dass alle Anfragen möglichst gleiche
Latenzen haben. Dafür wird die Varianz in den Blockgrößen von $B$ bestimmt $V =
\frac{\sum_{b=1}^{|B|}(|b| - \mu_b)^2)}{|B|}$. Für einen Ausdruck $k$ ergibt
sich daraus die Bewertungsfunktion: $$SC_k=\alpha \cdot(1-FS_k) + \beta \cdot
S_{b_{(ave)_{k}}} + (1 - \alpha - \beta) \cdot V_k,$$ wobei $\alpha$ und $\beta$
genutzt werden, um die drei Kriterien zu gewichten. Je niedriger der Wert, desto
besser wird ein Ausdruck gewertet. Gegenüber der ursprünglichen
Bewertungsfuntion hat dieser Ansatz den Nachteil, dass für jeden Ausdruck von
einem Blocking Verfahren die Blöcke $B$ generiert werden müssen, um Durschnitt
und Varianz zu berechnen. Hierdurch dauert die Bewertung der Ausdrücke deutlich
länger. Ein weiterer Nachteil ist, dass die zwei freien Parameter ($\alpha$
und $\beta$) domänabhängig bzw. datensatzabhängig angepasst werden müssen. Der
Wertebereich für die Fisher-Score liegt dabei zwischen 0 und 1, wohingegen der
Blockdurchschnitt und die Blockvarianz deutlich darüber liegen können. In der
Praxis ist es daher äußerst schwierig geeignete Gewichte zu wählen, sodass die
Fisher-Score zur Geltung kommt und nicht von den anderen beiden Kriterien
dominiert wird.

```texalgo
#alg:dnf LearnOptimalBS($S$, $k$, $d$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Set of specific blocking predicates: $S$
  \item Maximum conjunctions per term: $k$
  \item Maximum disjunctions of terms: $d$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme: $BS$
  \end{itemize}
}
\Statex
\State Initialize set of blocking scheme candidates $BS_C = ()$
\State Initialize set of terms $T = ()$
\For{$i = 1$ \textbf{to} $k$}\label{alg:bs:dis}
    \State Generate combination of $S$ with cardinality $i$ and
    \StatexIndent[1] add to $T$
\EndFor\label{alg:bs:die}
\For{term $t \in T$}
    \State $fmeasure, y_{true}, y_{pred} = evaluateTerm(t)$\Comment
    \If{$fmeasure = thres$}
        \State Remove $t$ from $T$\label{alg:bs:del}
    \EndIf
\EndFor
\For{$i = 1$ \textbf{to} $d$}\label{alg:ref:cms}
    \State Generate combination $C$ of $T$ with cardinality $i$
    \State Add $C$ to $BS_C$\label{alg:ref:cme}
\EndFor
\For{Blocking scheme $bs \in BS_C$}
    \State Initialize array $y_{true}$ with length $|P \cup N|$
    \State Initialize array $y_{pred}$ with length $|P \cup N|$
    \For{term $t \in bs$}
        \State $y_{true} \lor t.y_{true}$\label{alg:bs:ort}
        \State $y_{pred} \lor t.y_{pred}$\label{alg:bs:ory}
    \EndFor
    \State Score $s = fmeasure(y_{true}, y_{pred})$\label{alg:bs:fm}
    \If{$s > top_score$}
        \State $BS = bs$
    \EndIf
\EndFor
\State return $BS$

#alg:dnf_eval EvaluateTerm($t$, $D$, $IX$, $P$, $N$)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term: $t$
  \item Dataset: $D$
  \item Indexer: $IX$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item F-Measure: $f$
  \item Labels: $y_{true}$
  \item Predictions: $y_{pred}$
  \end{itemize}
}
\Statex
\State Build Index $I$ over $D$ using Indexer $IX$ and Term $t$\label{alg:bs:idx}
\State Initialize set of pairs $C = ()$
\State Initialize $TP = 0, FP = 0, FN = 0$
\For{block $b \in I$}\label{alg:bs:feb}
    \State Generate pair combinations $pc$ for records in $b$\label{alg:bs:cmb}
    \StatexIndent[1] and add them to $C$
    \State According to $P$ and $N$ calculate number of true\label{alg:bs:cnt}
    \StatexIndent[1] positives, false positives and false negatives and
    \StatexIndent[1] sum them up with $TP$, $FP$ and $FN$.
\EndFor
\State Calculate F-Measure $f$ according to $TP, FN, FP$\label{alg:bs:fmes}
\State Initialize array $y_{true}$ and $y_{pred}$ with length $|P \cup N|$\label{alg:bs:yarr}
\For{$i = 1$ \textbf{to} $|P|$}\label{alg:bs:p}
    \State Pair $p = P[i]$
    \State $y_{true} = True$\label{alg:bs:p1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:p2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:p3}
    \EndIf
\EndFor
\For{$i = |P| + 1$ \textbf{to} $|N|$}\label{alg:bs:n}
    \State Pair $p = N[i]$
    \State $y_{true} = False$\label{alg:bs:n1}
    \If{$p \in C$}
        \State $y_{pred} = True$\label{alg:bs:n2}
    \Else
        \State $y_{pred} = False$\label{alg:bs:n3}
    \EndIf
\EndFor
\State return $f, y_{true}, y_{pred}$
```

Zur Bewertung die Blöcke jedes Ausdrucks erzeugen zu lassen ist zwar aufwändig,
da die generierten Blöcke jedoch vom genutzten Blocking Verfahren abhängig sind,
ist dieser Schritt unvermeidlich. Verbesserungswürdig ist allerdings die
Bewertungsfunktion. Die Anforderung an eine solche Funktion ist, Ausdrücke mit
einer hohen Pairs Completeness und einer hohen Pairs Quality ausfindig zu machen
und danach einzustufen. Aus @sec:measurements ist bekannt, dass die Pairs
Completeness mit dem Recall verwandt ist und die Pairs Quality mit der
Precision. Zudem ist bekannt, dass es einen Kompromiss zwischen Recall und
Precision gibt, der durch das F-Measure ausgedrückt werden kann. Ein optimales
F-Measure maximiert dementsprechend Recall und Precision. Aufgrund der
Verwandtschaft zwischen Recall und Precision zu Pairs Completeness und Pairs
Quality kann das F-Measure ebenfalls genutzt werden, um diese beiden Attribute
zu optimieren. Damit erfüllt das F-Measure die Anforderungen an eine gute
Bewertungsfunktion. Die Evaluierung eines Ausdrucks, mit dem F-Measure als
Bewertungsfunktion, ist in Algorithmus \ref{alg:dnf_eval} beschrieben. Zur
Bewertung eines Ausdrucks $t$ benötigt der Algorithmus den Datensatz $D$, sowie
die Matches $P$ und die Non-Matches $N$, der Ground Truth. Zudem wird, das
einzusetzende Blocking Verfahren benötigt. Dieses wird vertreten durch einen
Indexer $IX$. Ein Ausdruck $t$ wird dem Indexer $IX$ als Blocking Schema
übergeben, woraus dieser die zugehörigen Blöcke $I$, anhand der Datensätze aus
$D$ baut (Zeile \ref{alg:bs:idx}). Durch die Betrachtung des konkreten Index,
wird eine DNF erzeugt die auf den Indexer zugeschnitten ist. Als nächstes werden
alle generierten Blöcke in $I$ betrachtet (Zeile \ref{alg:bs:feb}). Der Indexer
muss dazu eine entsprechende Blockliste bereitstellen. Ein Block ist in diesem
Zusammenhang nicht zwangsweise eine Gruppierung die über einen Blockschlüssel
gebildet wurde, sondern jegliche Anhäufungen von Datensätzen, die bei einer
Anfrage zusammen als Kandidatenmenge ausgewählt werden. Die Details hierzu
werden in @sec:anaindxer erläutert. Für jeden Block werden zunächst die
Paarkombinationen[^3], aller dem Block zugehöriger Datensätze, ermittelt. Diese
werden zu der Menge aller Paarkombinationen aller Blöcke $C$ hinzugefügt (Zeile
\ref{alg:bs:cmb}). Des Weiteren wird für einen Block $b$ die Anzahl der Paare in
den Klassifikationskategorien

* true positives, wenn ein Paar $p \in b$ und $p \in P$
* false positives, wenn ein Paar $p \in b$ und $p \in N$
* true negatives, wenn ein Paar $p \notin b$ und $p \in P$

über die Grund Truth ermittelt und in $TP$, $FP$ und $FN$ aufsummiert (Zeile
\ref{alg:bs:cnt}). Anhand der Werte $TP$, $FP$, $FN$ wird das F-Measure zur
Bewertung des Ausdrucks $t$ bestimmt (Zeile \ref{alg:bs:fmes}). Bei der späteren
Disjunktion von Ausdrücken kann dieses F-Measure allderdings nur zur Vorauswahl
der infrage kommenden Audrücke genutzt werden, da sich die F-Measure Werte
verschiedener Ausdrücke aus unterschiedlichen Paarkombinationen berechnen. Damit
die Ausdrücke effizient disjunktiert werden können, wird die Paarkombination auf
die Ground Truth abgebildet. Dazu werden zwei Arrays $y_{true}$ und $y_{pred}$
mit der Länge $|P \cup N|$ erzeugt (Zeile \ref{alg:bs:yarr}). Diese können beim
Zusammenfügen der DNF, ähnlich wie die Featurevektoren in [@KM:Unsupervised:13],
einfach verodert und daraus das F-Measure bestimmt werden. Für die Abbildung auf
die Ground Truth müssen alles Matches $P$ (Zeile \ref{alg:bs:p}) und alle
Non-Matches (Zeile \ref{alg:bs:n}) betrachtet werden. $y_{true}$ gibt an,
welcher Klasse ein Paar $p$ angehört: Match, wenn $p \in P$ (Zeile
\ref{alg:bs:p1}) oder Non-Match, wenn $p \in N$ (Zeile \ref{alg:bs:n1}).
$y_{pred}$ gibt an, ob ein Datensatzpaar einen gemeinsamen Block in $I$ hat
$y_{pred}[p] = True$ (Zeilen \ref{alg:bs:p2},\ref{alg:bs:n2}) oder nicht
$y_{pred}[p] = False$ (Zeilen \ref{alg:bs:p3},\ref{alg:bs:n3}). Die Bewertung
des Ausdrucks über das F-Measure, $y_{true}$ und $y_{pred}$ werden zum Schluss
an den Aufrufer zurückgegeben.

[^3]: 2-Tupel der Datensätze ohne festgelegte Reihenfolge

Die Änderungen an der Bewertungsfunktion führen dazu, dass das Verfahren zum
Bilden eines Disjunktiven Blocking Schema aus [@KM:Unsupervised:13] nicht mehr
angewendet werden kann, da die Featurevektoren (welche dazu genutzt wurden)
nicht länger erzeugt werden. Der Algorithmus zur Bestimmung des optimalen
Blocking Schemas ist in Algorithmus \ref{alg:dnf} dargestellt. Die ersten
Parameter sind die spezifischen Blockingprädikate $S$. Diese werden vom Aufrufer
der Funktion bestimmt. Da die maximale Konjunktion der Blockingprädikate bzw.
die maximale Disjunktion potentiell unendlich groß ist, werden diese über die
Parameter $k$ für die Konjunktionen und $d$ für die Disjunktionen begrenzt. Ein
weiterer Grund die Konjunktionen bzw. Disjunktionen nicht beliebig zu erhöhen
ist, dass das ein Blocking Verfahren deutlich komplexere Blockschlüssel erzeugen
muss. Denn für jedes spezifische Blockingprädikat muss, beim Erzeugen der
Blockschlüssel eines Datensatzes, die Prädikatsfunktion aufgerufen werden.
Demnach nimmt die Effizienz der Blockschlüsselgenerierung ab, je mehr
Konjunktionen bzw. Disjunktionen ein Blocking Schema hat. Am Anfang werden die
konjugierten Ausdrücke erzeugt, indem alle Kombinationen der Menge spezifischer
Blockingprädikte $S$ bis zur Länge $k$ der maximalen Konjunktionen berechnet
werden (Zeilen \ref{alg:bs:dis}-\ref{alg:bs:die}), somit ist $$T =
\{\{\text{2-Tupel von S}\}, \{\text{3-Tupel von S}\}, \dots, \{\text{k-Tupel von
S}\}\}.$$ Anschließend wird jeder Ausdruck $t$ durch den oben erklärten
Algorithmus \ref{alg:dnf_eval} evaluiert. Ist der F-Measure Wert $f$ für $t$
kleiner einer Schranke $thres$, indiziert dies einen ungeeigneten Block und der
Ausdruck wird entfernt (Zeile \ref{alg:bs:del}). Aus den in $T$ noch vorhandenen
Ausdrücken, werden durch Disjunktion bis zur Länge $d$ mögliche Kandidaten eines
Blocking Schemas generiert (Zeilen \ref{alg:ref:cms}-\ref{alg:ref:cme}). Ein
Blocking Schema wird ausgewählt, indem die Arrays $y_{pred}$ und $y_{true}$ der
Ausdrücke in jedem potentiellen Blocking Schema verodert werden (Zeilen
\ref{alg:bs:ort}-\ref{alg:bs:ory}) und daraus das F-Measure berechnet wird
(Zeile \ref{alg:bs:fm}). Das potentielle Blocking Schema mit dem höchsten
F-Measure wird abschließend ausgewählt und zurückgegeben.

<!-- \TODO{Cleverer Algorithmus durch bessers Sampling. Veroderung bestes Ergebnis -->
<!-- = Recall 1 und Precision bleibt gleich. Recall 1 ist eventuell etwas zu -->
<!-- optimistisch. Im welchem Rahmen kann angenommen werden, dass die Precision -->
<!-- gleich bleibt. Für A1 Precision 0.75 und A2 Precision 0.1 offensichtlich nicht. -->
<!-- Idee für Voraussage, ob DNF besser mitteln der Precision und Recall mit -->
<!-- addieren. => P1 = 1, P2 = 0, P12 = 0.5; R1 = 0.5, R2 = 0.4, R12 = 0.9. -->
<!-- Algorithmus auf Whiteboard} -->

## Blockschlüsselgenerierung {#sec:bkv_gen}

| ID | Entitäts-ID | Vorname | Zweitname | Nachname |
|----+-------------+---------+-----------+----------|
| 1  | 1           | Peter   | Moritz    | Michel   |
| 2  | 1           | Moritz  | Peter     | Michel   |
| 3  | 2           | Michel  | Moritz    | Peter    |
| 4  | 2           | Michle  | Moritz    | Peter    |

: Beispieldatensätze zur Veranschaulichung der Kommutativität bei der
Blockschüsselerzeugung. Das Attribut ID identifiziert den einzelnen Datensatz
und die Entitäts-ID ordnet Datensätze Entitäten zu. Dementsprechend beschreiben
Datensatz 1 und 2, sowie 3 und 4 die selbe Entität. {#tbl:comm_examles}

Die Erzeugung von Blockschlüsseln aus einem einzelnen Attribut sind vielfältig.
In @sec:staticblocking wurde ein Verfahren vorgestellt, dass Q-gramme bildet.
Ein anderes Verfahren nutzt Attributssuffixe. Auch denkbar sind
Attributsprefixe, welche bei der Sorted Neighborhood Anwendung finden. Zudem
sind phonetische Enkodierungen beliebt, um unterschiedliche Schreibweisen
zusammenzuführen. Damit diese Verfahren für ein DNF Blocking Schema nutzbar
sind, werden diese als allgemeine Blockingprädikate abgebildet, beispielsweise
`HasCommonQGram` oder `HasCommonSuffix`. Die Anwendung der erzeugten
Blockschlüssel bei der Disjunktion von spezifischen Blockingprädikaten ist dabei
intuitiv. Für jeden Blockschlüssel $s$, der anhand eines Datensatzes $r$ erzeugt
wurde, wird $r$ einem Block zugeordnet. Die einzige Überlegung hierbei ist, ob
die Blöcke verschiedener spezifischer Blockingprädikate disjunkt sind oder
nicht. Konkret bedeutet dies, erzeugen zwei Prädikate auf unterschiedlichen
Attributen denselben Schlüssel, verweisen diese auf denselben Block oder werden
unterschiedliche Blöcke adressiert. Dieses Problem muss vom jeweiligen Blocking
Verfahren gelöst werden, die Konsequenzen daraus werden in @sec:anaindxer
erläutert. Im Gegensatz dazu ist die Behandlung der Konjunktiven Ausdrücke bei
der Blockschlüsselerzeugung deutlich komplexer. Hierbei müssen zwei oder mehr
Mengen von Blockschlüsseln miteinander verknüpft werden. Sei $\Sigma$ das
Alphabet über einem Datensatz $D$, $\Sigma^*$ die Menge aller Wörter des
Alphabets und $r.f$ ist ein Attribut eines Datensatzes $r$, dann ist $r.f
\subseteq \Sigma^*$. Gegeben seien $n$ spezifische Blockingprädikate $(p_1,
f_1), \cdots, (p_n, f_n)$ und eine Relation $S((p_k, f_k), r) \subseteq r.f_k$,
die anhand eines Prädikates $(p_k, f_k)$ und eines Datensatzes $r$ eine Menge
von Blockschlüsseln erzeugt. Die Menge von Blockschlüsseln, die aus einem
Konjunktiven Ausdruck gebildet werden, besteht entsprechend aus der Konjunktion
der Teilmengen $S$. Betrachtet man die Verundung der spezifischen
Blockingprädikate strikt, so müssen die Mengen kommutativ verknüpft werden.
Diese Eigenschaft ist für die Blockschlüsselbildung interessant, da deren
Einhaltung oder Nichteinhaltung maßgeblich das Ergebnis eines Blocking
Verfahrens beeinflusst. Wenn die Kommutativität nicht eingehalten wird, können
die Blockschlüssel relativ einfach über das Kartesisches Produkt der Mengen
erzeugt werden $$BKV(r) = {S_1 \times S_2 \times \cdots \times S_n}.$$ Die
kommutative Verknüpfung ist etwas komplizierter, hierzu definieren wird zunächst
die Vereinigungsmenge $\mathbb{S} = S_1 \cup S_2 \cup \cdots \cup S_n$ und
bilden die Blockschlüssel über das Kartesische Produkt dieser Menge $$BKV(r) =
\underbrace{\mathbb{S} \times \cdots \times \mathbb{S}}_{\text{n-Mal}}.$$ Als
Beispiel wird der Ausdruck $A = $ (`ExakteÜbereinstimmung`, `Vorname`) $\land$
(`ExakteÜbereinstimmung`, `Zweitname`) $\land$ (`ExakteÜbereinstimmung`,
`Nachname`) betrachtet. Die @tbl:comm_examles beinhaltet vier Datensätze, die
eindeutig über die ID identifiziert werden können. Jeweils zwei der Datensätze
beschreiben dieselbe Entität. Zur Demonstration der Auswirkung der
Kommutativität werden drei Paare betrachtet. Zunächst das Paar (1, 2). Beide
Datensätze beschreiben dieselbe Entität, aber der Vorname wurde mit dem
Zweitnamen vertauscht. Beim nächsten Paar (1, 3) gibt es ebenfalls eine
Vertauschung, hier zwischen Vorname und Nachname. Allerdings beschreiben die
beiden Datensätze unterschiedliche Entitäten. Das dritte Paar (3, 4) beschreibt
wieder dieselbe Entität, jedoch gibt es in Datensatz 4 einen Schreibfehler im
Vornamen. Angenommen $\land$ ist kommutativ, dann ist $BKV(1) \cup BKV(2) \neq
\emptyset$, wodurch das Paar (1, 2), trotz vertauschter Attribute, einem
gemeinsamen Block zugeordnet wird. Das Gleiche gilt aber auch für das Paar (1,
3), sodass $BKV(1) \cup BKV(3) \neq \emptyset$ und folglich wird dieses Paar
ebenfalls einem gemeinsamen Block zugeordnet. Für den Fall, dass $\land$ nicht
kommutativ ist, haben weder das Paar (1, 2) noch (1, 3) einen gemeinsamen
Blockschlüssel. Der Nachteil der kommutativen Blockschlüsselbildung anhand der
obigen Formel ist, dass deutlich mehr Blockschlüssel erzeugt werden.
Dementsprechend wird dadurch die Pairs Completeness auf Kosten der Pairs Quality
erhöht werden. Ist die Erzeugung nicht kommutativ wird umgekehrt, die Pairs
Quality auf Kosten der Pairs Completeness verbessert. Damit die Menge der Blöcke
durch eine kommutative Schlüsselerzeugung nicht explodiert, kann diese simuliert
werden. Dazu werden zunächst die Schlüssel nicht kommutativ erzeugt und
anschließend werden die Tupel des Kartesischen Produktes sortiert. Die
Kommutativität von $\land$ kann jedoch keinen Einfluss auf Rechtschreibfehler,
wie im Falle von Paar (3, 4) nehmen. Eine Möglichkeit Rechtschreibfehler bei der
Erzeugung der Blockschlüssel herauszufiltern ist, tokenbasierende Prädikate
(z.B. Q-Gramme) zu verwenden. Der Nachteil hierbei ist, dass für die einzelnen
Attribute mit einem solchen Prädikat deutlich mehr Teilschlüssel erzeugt werden.
Als Folge dessen wird zum einen den Prozess der Schlüsselgenerierung verlangsamt
und zum anderen die Chance erhöht, dass aufgrund einer größeren Menge von
Blöckschlüsseln unähnliche Datensätze zusammen in einen Block gruppiert werden.
Um bei gleicher Komplexität transpositive Rechtschreibfehler zu filtern, muss
$\land$ kommutativ sein und zusätzlich auf Zeichenebene angewendet werden,
beispielsweise indem die Teilschlüssel konkateniert und als Anagramme behandelt
werden. Dadurch ist es wiederum möglich die Pairs Completeness zu erhöhen.
Allerdings ebenfalls auf Kosten der Pairs Quality, da die Kollisionsrate durch
Reduktion der möglichen Blockschlüssel erhöht wird. Im Vergleich zu einem
tokenbasierenden Prädikat bleibt bestenfalls die Anzahl der Blockschlüssel
gleich bzw. wird reduziert, anstatt zu wachsen.

```{.texalgo #alg:bkvs caption="BlockingKeyValues(t, r)"}
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Term from Blocking Schema $t$
  \item Record $r$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Key Values: $BKV$
  \end{itemize}
}
\Statex
\State Initialize list $BKV = []$
\For{specific blocking predicate $p \in t$}\label{alg:bkv:fep}
  \State field $f = p.field$
  \State attribute keys $ak = p.predicate(r.f)$\label{alg:bkv:pre}
  \If{$BKV$ is empty}
    \State $BKV = ak$\label{alg:bkv:f1}
  \Else
    \State $bkv$ = []
    \While{$BKV$ is not empty}\label{alg:bkv:as}
      \State Take key $x$ from BKV
      \For{key $y$ in $ak$}
        \State $xy = concatenate(x ,y)$\label{alg:bkv:cc}
        \State Append $xy$ to $BKV$
      \EndFor
    \EndWhile\label{alg:bkv:ae}
    \State $BKV = bkv$\label{alg:bkv:rpl}
  \EndIf
\EndFor
\State return $BKV$
```

Algorithmus \ref{alg:bkvs} beschreibt ein nicht kommutatives Verfahren
Blockschlüssel durch Konkatenation zu bilden. Jeder Ausdruck $t$ des DNF
Blocking Schema erzeugt für einen Datensatz $r$ eine Menge von Blockschlüsseln.
Die Blockschlüssel der Ausdrücke werden unabhängig voneinander gebildet. Der
Algorithmus erhält daher als Eingabewert nur einen Ausdruck, bestehend aus
mindestens einem spezifischen Blockingprädikat. Diese werden der Reihe nach
betrachtet (Zeile \ref{alg:bkv:fep}). Anhand des Prädikats $p$ werden alle
Teilblockschlüssel für das verknüpfte Attribut $p.field$ generiert.
Beispielsweise liefert das Prädikat `GemeinsamerToken` alle durch Leerzeichen
getrennte Token des Attributes $r.field$ (Zeile \ref{alg:bkv:pre}). Ist die BKV
Liste zu diesem Zeitpunkt leer, wird die Schlüsselliste $ak$ als $BKV$ Liste
übernommen. Existieren in $BKV$ allerdings schon Schlüssel, wird zunächst eine
temporäre Liste $bkv$ erzeugt. Anschließend werden aus $BKV$ solange Schlüssel
entnommen, bis diese leer ist (Zeile \ref{alg:bkv:as}). Für jeden entnommenen
Schlüssel $x$ werden $|ak|$ neue Schlüssel erzeugt. Dazu wird der neue Schlüssel
$xy$ gebildet, indem ein Schlüssel $y$ aus $ak$ mit $x$ konkateniert wird (Zeile
\ref{alg:bkv:cc}). Alle auf diese Weise neu erzeugten Schlüsselpaare $xy$ werden
zu $bkv$ hinzugefügt. Wenn die $BKV$ Liste leer ist, wird die temporäre Liste
$bkv$ nach $BKV$ übernommen (Zeile \ref{alg:bkv:rpl}). Nachdem alle Prädikate
bearbeitet wurden, wird die Liste der generierten Blockschlüssel $BKV$
zurückgegeben.

## Dynamische Blocking Verfahren {#sec:anaindxer}

### Similarity-Aware Inverted Index

Die Idee des DySimII (vgl. @sec:dysimII) ist ein Standard Blocking Verfahren,
dass es erlaubt, die Ähnlichkeit von Datensätzen nachzuschlagen, indem
Attributsähnlichkeiten vorausberechnet werden. Im Wesentlichen handelt es sich
bei DySimII um einen Multi-pass Ansatz, da für einen Datensatz mehrere
Blockschlüssel erzeugt werden. Zu diesem Zweck wird für jedes Attribut eine
Enkodierungsfunktion genutzt, die genau einen Blockschlüssel erzeugt. Anhand
dessen werden Attribute zusammen gruppiert und innerhalb der Gruppierung die
Ähnlichkeit berechnet. Die errechnete Ähnlichkeit wird in einer Art Cache, dem
Similarity Index, vorgehalten. Während einer Abfrage wird vermieden, dass ein
Großteil der teuren Ähnlichkeitsberechnungen eingespart wird.

Beim Gruppieren von Attributen in Blöcke, führt der DySimII keine Trennung von
Attributen durch, sodass Attributswerte von Vorname und Zweitname bzw. Nachname
im einen gemeinsamen Block gruppiert werden können. Dadurch wird zum einen die
Abdeckung möglicher Attributskombinationen erhöht und zum anderen vermieden,
dass Ähnlichkeiten doppelt berechnet werden. Diese Einsparung macht sich
besonders bei ähnlichen Attributen mit großteils überlappenden Werten, wie
Vorname und Zweitname, bemerkbar. Neben der Ähnlichkeitsvorausberechnung dienen
die Blöcke auch zum Bilden einer Kandidatenmenge möglicher Duplikate. Für einen
Anfragedatensatz $q$ besteht die Kandidatenmenge $C$ einerseits aus Datensätzen,
die über den RI selektiert werden, weil diese in irgendeinem indizierten
Attribut denselben Wert haben und andererseits aus Datensätzen, die über einen
gemeinsamen Blockschlüssel in irgendeinem Attribut, im BI zusammen gruppiert
wurden. Durch die Vermischung der Attributswerte, sowohl im RI als auch im BI,
kann dadurch zwar die Pairs Completeness erhöht werden, da beispielsweise
Attributsvertauschungen erkannt werden, je ähnlicher die Werte unterschiedlicher
Attribute sind, desto mehr wird folglich die Pairs Quality gesenkt. Diese
Verhalten korrespondiert mit der Kommutativität bei der Bildung von
Blockschlüsseln, aus konjunktiven spezifischen Blockingprädikaten. Im Kontrast
zum DySimII kann die Überlappung, beim Bauen des DNF Blocking Schema, jedoch mit
Bedacht auf ausgewählte Attribute angewandt werden und wird nicht
pauschalisiert. Die Möglichkeit der Überlappung von Attributswerten führt zu
einer gravierenden Folge bei der Berechnung der Ähnlichkeit zwischen zwei
Datensätzen. Da es in einem Block zu einer Vermischung von Werten
unterschiedlicher Attribute kommen kann, ist es aufgrunddessen nicht möglich die
vorausberechnete Ähnlichkeit einem bestimmten Attribut zuzuweisen. Diese müssten
mit den tatsächlichen Attributen eines Datensatzes verglichen werden, um das
korrekte Attribut zu finden. Der orginale Datensatz zum Nachschlagen steht dem
DySimII jedoch nicht zur Verfügung, weshalb stattdessen die Ähnlichkeiten
aufsummiert werden. Aus @sec:classifier ist allerdings bekannt, dass dabei für
einen Klassifikator wertvolle Informationen verloren gehen, die entscheidend
sind damit ein Match von einem Non-Match unterschieden werden kann. Die
Entscheidung des DySimII Verfahrens, nur die Ähnlichkeiten aus Attributen, die
einen gemeinsamen Block haben, in die Gesamtähnlichkeit mit einfließen zu
lassen, reduziert den Ähnlichkeitsinformationswert weiter, da alle unähnlichen
Attribute mit demselben Ähnlichkeitswert von 0 bestraft werden.

#### MDySimII

```{.a2s #fig:mdysimII_example
    caption="Ein beispielhafter MDySimII-Index, welcher aus der Tabelle links
    erzeugt worden ist. Die Beispieldatensätze enthalten das Namensattribut
    eines Restaurants und die Art der Küche. RI ist der Record Index, BI ist der
    Block Index, welcher aus dem Blocking Schema (CommonToken, Name) AND
    (CommonToken, Kitchen) erzeugt wurde. SI ist der Similarity Index."}
 Blocking Scheme: (CommonToken, Name) ∧ (CommonToken, Kitchen)

#-----------+--------------+--------------# #----------------------------------------# #------------------------------------------------#
|[RID      ]| Name         | Kitchen      | | RI (name)                              | | RI (kitchen)                                   |
#-----------+--------------+--------------# | .-------------.       .--------------. | | .---------.      .------.        .----------.  |
|     r1    | tonys pizza  | italian      | | | tonys pizza |       | tonys gelato | | | | italian |      | vegi |        | american |  |
+-----------+--------------+--------------+ | '---+---------'       '---+----------' | | '---+-----'      '---+--'        '---+------'  |
|     r2    | tonys gelato | vegi         | |     |                     |            | |     |                |               |         |
+-----------+--------------+--------------+ |     |  .----+----+----.   |  .----.    | |     |  .----+----.   |  .----+----.  |  .----. |
|     r3    | tonys pizza  | italian vegi | |     +->| r1 | r3 | r4 |   +->| r2 |    | |     +->| r1 | r3 |   +->| r2 | r3 |  +->| r4 | |
+-----------+--------------+--------------+ |        '----+----+----'      '----'    | |        '----+----'      '----+----'     '----' |
|     r4    | tonys pizza  | american     | |                                        | |                                                |
#-----------+--------------+--------------# #----------------------------------------# #------------------------------------------------#
#----------------------------------------------------------------------# #--------------------------------------------------------------#
| BI (name)                                                            | | BI (kitchen)                                                 |
| .--------------. .--------------. .---------------.                  | | .--------------. .--------------. .---------------.          |
| | tonysitalian | | pizzaitalian | | tonysamerican |                  | | | tonysitalian | | pizzaitalian | | tonysamerican |          |
| '---+----------' '---+----------' '---+-----------'                  | | '---+----------' '---+----------' '---+-----------'          |
|     |                |                |                              | |     |                |                |                      |
|     v                v                v                              | |     v                v                v                      |
| .---+---------.  .---+---------.  .---+---------.                    | | .---+-----.      .---+-----.      .---+------.               |
| | tonys pizza |  | tonys pizza |  | tonys pizza |                    | | | italian |      | italian |      | american |               |
| '-------------'  '-------------'  '-------------'                    | | '---------'      '---------'      '----------'               |
|                                                                      | |                                                              |
| .-----------.    .------------.   .-----------.    .---------------. | | .-----------. .------------. .-----------. .---------------. |
| | tonysvegi |    | gelatovegi |   | pizzavegi |    | pizzaamerican | | | | tonysvegi | | gelatovegi | | pizzavegi | | pizzaamerican | |
| '--+--------'    '--+---------'   '--+--------'    '---+-----------' | | '--+--------' '--+---------' '--+--------' '---+-----------' |
|    |                |                |                 |             | |    |             |              |              |             |
|    v                v                v                 v             | |    v             v              v              v             |
| .--+-----------. .--+-----------. .--+----------.  .---+---------.   | | .--+---.      .--+---.       .--+---.      .---+------.      |
| | tonys gelato | | tonys gelato | | tonys pizza |  | tonys pizza |   | | | vegi |      | vegi |       | vegi |      | american |      |
| +--------------+ '--------------' '-------------'  '-------------'   | | '------'      '------'       '------'      '----------'      |
| | tonys pizza  |                                                     | |                                                              |
| '--------------'                                                     | |                                                              |
#----------------------------------------------------------------------# #--------------------------------------------------------------#
#------------------------------------------------# #--------------#
| SI (name)                                      | | SI (kitchen) |
| .--------------.      .-------------.          | |              |
| | tonys gelato |      | tonys pizza |          | | {empty}      |
| '--+-----------'      '--+----------'          | #--------------#
|    |                     |                     |
|    v                     v                     |
| .--+----------+-----. .--+-----------+-----.   |
| | tonys pizza | 0.7 | | tonys gelato | 0.7 |   |
| '-------------+-----' '--------------+-----'   |
#------------------------------------------------#
[RID      ]: {"fill":"#cccccc", "a2s:label": " Record ID"}
```

Eine weitere Einschränkung des DySimII Verfahrens ist die Vorgabe zu einer
Enkodierfunktion pro Attribut, die genau ein Attribut liefert. Aufgrund dessen
kann für DySimII kein DNF Blocking Schema verwendet werden. Im Folgenden wird
ein Verfahren beschrieben, dass zum einen DNF kompatibel ist und zum anderen
einige Nachteile, der Informationsdetailliertheit des Ähnlichkeitswertes,
ausbessert. Dieser Ansatz wird Multi-Dynamic Similarity-Aware Inverted Index
(MDySimII) genannt. Dabei steht das Multi für einen uneingeschränkten Multi-pass
Ansatz. Das bedeutet, dass Blockschlüssel über mehrere Attribute generiert
werden dürfen, dass nicht jedes Attribut zur Generierung genutzt werden muss und
dass beliebig viele Blockschlüssel erzeugt werden können. @fig:mdysimII_example
zeigt beispielhaft die Indexstrukturen des MDySimII, welche aus der Tabelle und
dem Blocking Schema (links oben) erzeugt wurden. Die RIs verknüpfen dabei
jeweils ein Attribut mit den Datensatzidentifieren, die diesem Attribut
entsprechen, etwa `tonys pizza` mit den Datensätzen `r1`, `r3` und `r4`. Die
Blockschüssel für die BIs wurden durch Algorithmus \ref{alg:bkvs} generiert. Für
`r1` wurden, aus den Namenstoken `tonys` und `pizza`, sowie dem Token der
Küchenart `italian`, die Blockschlüssel `tonysitalian` und `pizzaitalian`
gebildet. Die beteiligten Attribute wurden, in ihren eigenen BI, in Blöcke mit
den beiden Blockschlüsseln eingefügt. Der SI des jeweiligen Attributes wird
angelegt, wenn in mindestens einem Block mehrere Attribute einfügt wurden. Das
Beispiel zeigt die strikte Trennung von Attributswerten, sodass lediglich Werte
desselben Attributes zusammen gruppiert werden können. Folglich ist der MDySimII
in der Lage einen Vektor mit Ähnlichkeitswerten der einzelnen Attribute für
Datensatze der Kandidatenmenge zurückzugeben.

```texalgo
#alg:mdysim_insert MDySimII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$: $F$
  \item Similarity functions: $S_i,i=1 \cdots |F|$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $RI, BI, SI$
  \end{itemize}
}
\Statex
\For{fields $f \in F$}\label{alg:dII:is}
  \State Initialize $RI_f = \{\}$, $BI_f = \{\}$, $SI_f = \{\}$
\EndFor\label{alg:dII:ie}\color{gray}
\For{records $r \in D$}\color{black}
  \For{fields $f \in F$}\label{alg:dII:ris}
    \State insert $r.id$ into $RI_f[r.f]$
  \EndFor\label{alg:dII:rie}
  \For{terms $t \in BS$}
    \State $bkvs = BlockingKeyValues(t, r)$\label{alg:dII:bkv}
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \If{$a \notin SI_f$}\color{gray}
          \State Append r.f to $BI_f[bkv]$\label{alg:dII:bi}
          \For{attribute $a \in BI_f[bkv]$}
            \State $sim = S_f(r.f, a)$\label{alg:dII:sim}
            \State Append $(r.f, sim)$ to $SI_f[a]$\label{alg:dII:si1}
            \State Append $(a, sim)$ to $SI_f[r.f]$\label{alg:dII:si2}
          \EndFor
        \EndIf\color{black}
      \EndFor
    \EndFor
  \EndFor\label{alg:dII:ins}
\EndFor

#alg:mdysim_query MDySimII - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index
\For{fields $f \in F$}
    \State $ri = RI_f[q.f]$
    \For{$r.id \in ri$}
        \State $M[(r.id, f)] = 1.0$
    \EndFor
    \State $si = SI_f[q.f]$
    \For{$(r.f, sim) \in si$}
        \State $ri = RI_f[r.f]$
        \For{$r.id \in ri$}
            \State $M[(r.id, f)] = sim$
        \EndFor
    \EndFor
\EndFor
\State return $M$
```

Durch die erläuterten Anpassungen für den MDySimII ergeben sich einige
Änderungen im Ablauf. In Algorithmus \ref{alg:mdysim_insert} ist die
*Build-Phase* des MDySimII beschrieben. Zunächst werden die Index
Datenstrukturen Record Identifier Index (RI), welcher alle Attribute speichert
und diese ihren Datensätzen zuordnet, Block Index (BI), welcher Attribute anhand
der Blockschlüssel gruppiert und Similarity Index (SI), welcher
Attributsähnlichkeiten zwischen Attributen im gleichen Block hält, für jedes in
der DNF vorkommende Attribut erzeugt (Zeilen \ref{alg:dII:is}-\ref{alg:dII:ie}).
Als Nächstes werden alle Datensätze in $D$ nacheinander den Indexstrukturen des
MDySimII hinzugefügt. Dazu wird zuerst der Datensatzidentifier unter dem
entsprechenden Attribut, in alle initialisierten RIs, eingefügt (Zeilen
\ref{alg:dII:ris}-\ref{alg:dII:rie}). Anschließend werden die disjunkten
Ausdrücke der DNF einzeln betrachtet. Zu jedem Ausdruck werden für den Datensatz
$r$ die Blockschlüsselwerte $bkvs$ erzeugt (Zeile \ref{alg:dII:bkv}). Für jedes
durch den aktuellen Ausdruck erfasste Attribut werden die Attributswerte von
$r$, unter dem Blockschlüssel $bkv$, in den entsprechenden Block Index eingefügt
(Zeilen \ref{alg:dII:bi}). Nachdem ein Attribut in einen Block eingefügt wurde,
werden analog zum ursprünglichen DySimII Verfahren, die Ähnlichkeiten zwischen
$r.f$ und den bisherigen Attributen des Blocks ermittelt (Zeile
\ref{alg:dII:sim}). Der Algorithmus endet, wenn alle Datensätze $r \in D$
hinzugefügt wurden. Wird später in der *Query-Phase* ein einzelner Datensatz
hinzugefügt, werden lediglich die Schritte in Zeile
\ref{alg:dII:ris}-\ref{alg:dII:ins} durchgeführt.

Wird der MDySimII Index zur Bewertung eines DNF Ausdrucks für Algorithmus
\ref{alg:dnf_eval} gebaut, werden anstatt der Attributswerte die
Datensatzidentifier in die Blöcke der BIs eingefügt. Zudem wird die Erzeugung
der SIs ausgespart, da diese für die Bewertung nicht benötigt werden. Neben den
BI Blöcken, werden zudem die RI Blöcke übergeben, da diese maßgeblich die
Kandidatenmenge eines Anfragedatensatzes beeinflussen.

Die *Query-Phase* des MDySimII wird in Algorithmus \ref{alg:mdysim_query}
beschrieben. Dieser Algorithmus ist fast identisch zum DySimII Verfahren. Die
Änderungen sind, dass im entsprechenden RI bzw. SI des Attributes nachgeschlagen
und dass ein Ähnlichkeitsvektor erstellt wird, anstatt die einzelnen
Ähnlichkeiten aufzusummieren. Aus Effizienzgründen erhebt der MDySimII, ebenso
wie der DySimII, keine Ähnlichkeiten zwischen Attributen, die nicht zusammen
gruppiert wurden. Durch das DNF Blocking Schema ist allerdings nicht mehr
garantiert, dass jedes Attribut berücksichtigt wird, da der Ähnlichkeitsvektor
der Kandidaten maximal in den Stellen besetzt ist, die auch im DNF Blocking
Schema genutzt werden. Hierdurch entfällt eine komplette Attributsabdeckung, bei
der Vorausberechnung der Ähnlichkeiten. Im schlimmsten Fall besteht das Blocking
Schema nur aus einem einstelligen Ausdruck, wodurch das Blocking und die
Vorausberechnung lediglich auf einem Attribut durchgeführt wird.

Das ursprüngliche Design des Similarity-Aware Inverted Index von Christen &
Gayler [@CG:Scalable:08] hat eine fundamentale Schwachstelle, die auch in den
Varianten DySimII und MDySimII vorhanden ist. Diese Schwachstelle ist der Record
Identifier Index (RI), welcher den Index enorm ausbremst, wenn ein Attribut nur
wenige Auswahlmöglichkeiten bietet, beispielsweise Geschlecht oder Bundesland,
bzw. wenn wenige Attributswerte besonders häufig vorkommen, beispielsweise der
in Deutschland weit verbreitete Nachname `Müller`. Wenn ein solches Attribut im
DNF Blocking Schema vorkommt, wird zwangsweise über den Record Identifier Index
eine riesige Menge an Kandidaten selektiert, wovon nur ein Bruchteil tatsächlich
relevant ist. Der schlimmste Fall tritt ein, wenn ein Datensatz ein Attribut
hat, dass für alle Datensätze stets dasselbe ist. Beispielsweise das Geschlecht
(männlich/weiblich), für einen Datensatz der nur weibliche Personen enthält.
Dafür erzeugen alle Variationen des Similarity-Aware Inverted Index eine
Kandidatenmenge mit 100% der Datensätze. Aufgrund der offensichtlich vielen
False Positives in der Kandidatenmenge, wird der Algorithmus zur Bewertung von
Ausdrücken, diejenigen mit diesen Attributen, sowohl alleinstehend als auch in
Konjunktion mit anderen, sehr schlecht bewerten. Im ungünstigsten Fall wird
dadurch ein Ausdruck verworfen, welcher für den BI qualitativ hochwertige und
effiziente Blöcke erzeugt.

#### MDySimIII

```{.a2s #fig:mdysim_example
    caption="Ein beispielhafter MDySimIII-Index, welcher aus der Tabelle links
    erzeugt worden ist. Die Beispieldatensätze enthalten das Namensattribut
    eines Restaurants und die Art der Küche. BI ist der Block Index, welcher aus
    dem Blocking Schema (CommonToken, Name) AND (CommonToken, Kitchen) erzeugt
    wurde. SI ist der Similarity Index."}
 Blocking Scheme: (CommonToken, Name) ∧ (CommonToken, Kitchen)

#-----------+--------------+--------------#  #----------------------------------------------# #--------------#
|[RID      ]| Name         | Kitchen      |  | SI (name)                                    | | SI (kitchen) |
#-----------+--------------+--------------#  | .--------------.      .-------------.        | |              |
|     r1    | tonys pizza  | italian      |  | | tonys gelato |      | tonys pizza |        | | {empty}      |
+-----------+--------------+--------------+  | '--+-----------'      '--+----------'        | #--------------#
|     r2    | tonys gelato | vegi         |  |    |                     |                   |
+-----------+--------------+--------------+  |    v                     v                   |
|     r3    | tonys pizza  | italian vegi |  | .--+----------+-----. .--+-----------+-----. |
+-----------+--------------+--------------+  | | tonys pizza | 0.7 | | tonys gelato | 0.7 | |
|     r4    | tonys pizza  | american     |  | '-------------+-----' '--------------+-----' |
#-----------+--------------+--------------#  #----------------------------------------------#
#-------------------------------------------------------------------------------------------# #---------------------------------------------------------------------------#
| BI (name)                                                                                 | | BI (kitchen)                                                              |
| .--------------.         .--------------.         .---------------.                       | | .--------------.     .--------------.     .---------------.               |
| | tonysitalian |         | pizzaitalian |         | tonysamerican |                       | | | tonysitalian |     | pizzaitalian |     | tonysamerican |               |
| '---+----------'         '---+----------'         '---+-----------'                       | | '---+----------'     '---+----------'     '---+-----------'               |
|     |                        |                        |                                   | |     |                    |                    |                           |
|     v            RI          v            RI          v            RI                     | |     v        RI          v        RI          v         RI                |
| .---+---------.  .--+--. .---+---------.  .--+--. .---+---------.  .--.                   | | .---+-----.  .--+--. .---+-----.  .--+--. .---+------.  .--.              |
| | tonys pizza +->+r1|r3| | tonys pizza +->+r1|r3| | tonys pizza +->+r4|                   | | | italian +->+r1|r3| | italian +->+r1|r3| | american +->+r4|              |
| '-------------'  '--+--' '-------------'  '--+--' '-------------'  '--'                   | | '---------'  '--+--' '---------'  '--+--' '----------'  '--'              |
|                                                                                           | |                                                                           |
| .-----------.          .------------.         .-----------.         .---------------.     | | .-----------.     .------------.    .-----------.     .---------------.   |
| | tonysvegi |          | gelatovegi |         | pizzavegi |         | pizzaamerican |     | | | tonysvegi |     | gelatovegi |    | pizzavegi |     | pizzaamerican |   |
| '--+--------'          '--+---------'         '--+--------'         '---+-----------'     | | '--+--------'     '--+---------'    '--+--------'     '---+-----------'   |
|    |                      |                      |                      |                 | |    |                 |                 |                  |               |
|    v              RI      v              RI      v             RI       v            RI   | |    v      RI         v      RI         v      RI          v         RI    |
| .--+-----------.  .--. .--+-----------.  .--. .--+----------.  .--. .---+---------.  .--. | | .--+---.  .--+--. .--+---.  .--+--. .--+---.  .--+--. .---+------.  .--.  |
| | tonys gelato +->|r2| | tonys gelato +->|r2| | tonys pizza +->|r3| | tonys pizza +->+r4| | | | vegi +->+r2|r3| | vegi +->+r2|r3| | vegi +->+r2|r3| | american +->+r4|  |
| +--------------+  #--# '--------------'  '--' '-------------'  '--' '-------------'  '--' | | '------'  '--+--' '------'  '--+--' '------'  '--+--' '----------'  '--'  |
| | tonys pizza  +->|r3|                                                                    | |                                                                           |
| '--------------'  '--'                                                                    | |                                                                           |
#-------------------------------------------------------------------------------------------# #---------------------------------------------------------------------------#
[RID      ]: {"fill":"#cccccc", "a2s:label": " Record ID"}
```

Der MDySimIII ist eine Modifikation des MDySimII Verfahrens, der dahingehend
verändert wurde, dass die Anzahl der Kandidaten, auch bei spezifischen
Blockingprädikaten mit wenigen Optionen bzw. häufig vorkommenden Werten,
effizient eingeschränkt werden kann. Dadurch ist es möglich, im Gegensatz zur
bisherigen Familie von Similarity-Aware Inverted Indizes, deutlich mehr
spezifische Blockingprädikate, die zu einem guten Blocking Schema führen, zu
betrachten. Damit dies möglich ist, wurde der globale bzw. attributsglobale
Record Identifier Index (RI), in seiner bisherigen Form, aufgesplittet und in
den Block Index (BI) verschoben. Der Similarity Index bleibt unverändert und
verlinkt weiterhin Attribute eines Blockes mit ihren Ähnlichkeiten. In
@fig:mdysim_example ist ein solcher Index aus den Datensätzen der Tabelle links
oben und dem Blocking Schema `(CommonToken, Name)` $\land$ `(CommonToken,
Kitchen)` erstellt worden. Ein Datensatz besteht aus den beiden Attributen
Restaurantname `Name` und der Küchenart `Kitchen`. Anhand des Blocking Schema
wurden durch Algorithmus \ref{alg:bkvs} fünf Blockschlüssel generiert, nämlich
`tonysitalian`, `tonysvegi`, `pizzaitalian`, `pizzavegi` und `gelatovegi`. Jeder
Block beinhaltet weiterhin die Attribute, welche zum entsprechenden
Blockschlüssel gehören. Die große Veränderung des MDySimIII ist , dass jedes
Attribut eines Blockes einen eigenen Record Identifier Index besitzt.

```texalgo
#alg:mdysimIII_insert MDySimIII - Build
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Data set: $D$
  \item DNF Blocking Scheme: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Index data structures: $BI, SI$
  \end{itemize}
}
\Statex\color{gray}
\For{fields $f \in F$}
  \State Initialize $BI_f = \{\}$, $SI_f = \{\}$
\EndFor
\For{records $r \in D$}
  \For{terms $t \in BS$}
    \State $bkvs = BlockingKeyValues(t, r)$
    \For{$bkv \in bkvs$}
      \For{fields $f \in t.fields$}
        \State Add $r.f$ to $BI_f[bkv]$\color{black}
        \State $ri = bi[r.f]$
        \State Add $r.id$ to $ri$
        \State $BI_f[bkv] = ri$\color{gray}
        \State Initialize inverted index list $si = ()$
        \For{attribute $a \in bi$}
          \If{$a \notin SI_f$}
            \State $sim = S_f(r.f, a)$
            \State Append $(r.f, sim)$ to $SI_f[a]$
            \State Append $(a, sim)$ to $si$
          \EndIf
        \EndFor
        \State $SI_f[r.f] = si$
      \EndFor
    \EndFor
  \EndFor
\EndFor

#alg:mdysimIII_query MDySimIII - Query
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Query record: $q$
  \item DNF Blocking Schema: $BS$
  \item Fields used in $BS$ as: $F$
  \item Similarity funcitons: $S_i,i=1 \cdots n$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Matches: $M$
  \end{itemize}
}
\Statex\color{gray}
\State Initialize dictionary $M = \{\}$
\State Insert $q$ into Index\color{black}
\For{terms $t \in BS$}
  \State $bkvs = BlockingKeyValues(t, r)$
  \For{$bkv \in bkvs$}
    \For{fields $f \in t.fields$}
      \State $bi = BI_f[bkv]$
      \For{attribute $r.f \in bi$}
        \If{$q.f = r.f$}
          \For{identifier $r.id \in bi[q.f]$}
            \State $M[(r.id, f)] = 1.0$
          \EndFor
        \Else
          \State $si = SI_f[q.f]$
          \State $sim = si[r.f]$
          \For{identifier $r.id \in bi[r.f]$}
            \State $M[(r.id, f)] = sim$
          \EndFor
        \EndIf
      \EndFor
    \EndFor
  \EndFor
\EndFor\color{gray}
\State return $M$
```

Aufgrund der Änderungen am RI hat sich die Build-Phase an zwei Stellen leicht
verändert. In Algorithmus \ref{alg:mdysimIII_insert} ist das Bauen des Index
gezeigt. In grau sind die Schritte markiert, welche sich gegenüber Algorithmus
\ref{alg:mdysim_insert} nicht verändert haben. Die erste Veränderung ist, dass
das initiale Hinzufügen der Attribute, in ihren attributsspezifischen RI
wegfällt, da dieser so nicht mehr existiert. Die zweite Veränderung (Zeilen
10-12) ist beim Hinzufügen eines Attributes in einen Block. Nachdem das Attribut
wie gewohnt in den Block eingefügt wurde, wird dessen RI geholt (Zeile 10), zu
welchem anschließend der Datensatzidentifier hinzugefügt wird (Zeile 11). Wird
der MDySimIII für den DNF Blocks Lerner gebaut, werden analog zum MDySimII
lediglich die Datensatzidentifier in die Blöcke eingefügt. Die Verknüpfung mit
den Attributen ist dabei irrelevant, da jeder Datensatz, der sich im Block
befindet, ungeachtet seines zugehörigen Attributswertes in der Kandidatenmenge
landet. Zudem wird auch hier die Erzeugung der SIs übersprungen.

Im Gegensatz zur Build-Phase hat sich die Query-Phase grundlegend verändert. Um
an die Identifikatoren in den Blöcken zu gelangen, müssen für den Anfragedatensatz,
die Blockschlüssel generiert werden (siehe Algorithmus
\ref{alg:mdysimIII_query}). Diese werden nacheinander aus den Termen $t$ erzeugt
(Zeilen 3-4). Für jeden Blockschlüssel werden anschließend die entsprechenden
Blöcke, der an dem Ausdruck beteiligten Attribute, betrachtet (Zeilen 5-7). Für
jedes Attribut wird zunächst überprüft, ob dies dem eigenen entspricht. Ist dies
der Fall, wird der Ähnlichkeitsvektor der Kandidaten im RI $bi[q.f]$, mit dem
Ähnlichkeitswert 1.0 ergänzt (Zeilen 9-12). Falls die Attribute unterschiedlich
sind, wird die Ähnlichkeit $sim$ im SI nachgeschlagen (Zeile 15) und der
Ähnlichkeitsvektor der Kandidaten im RI $bi[r.f]$, mit dem Ähnlichkeitswert
$sim$ ergänzt. Zum Schluss wird eine zum MDySimII identische Kandidatenmenge $M$
zurückgegeben.

Anstatt wie vorher, alle Datensatzidentifier des gleichen Attributes global zu
gruppieren, werden hier ausschließlich diejenigen in denselben RI eingefügt, die
auch denselben Blockschlüssel haben. Beispielsweise werden in
@fig:mdysim_example, im BI des Namen, über den Blockschlüssel `tonysitalian`
`r1` und `r3` zusammen einem RI zugeordnet, nicht jedoch `r4`, welcher keinen
gemeinsamen Blockschlüssel zu `r1` oder `r3` hat. Die Anzahl der Blöcke sind
identisch zum MDySimII Verfahren. Allerdings wird ein Attribut $r.f$ $k$-Mal mit
seinem Datensatzidentifier verknüpft (vgl. MDySimII $|F|$-Mal und DySimII
1-Mal), wobei $$k = \sum_{t \in BS}|BlockingKeyValues(t, r.f)|.$$ Beispielsweise
`r1` viermal, `r2` fünfmal, `r3` neunmal und `r4` ebenfalls viermal. Das hat zur
Folge, dass der MDySimIII mehr Speicherplatz als seine Vorgänger benötigt. Wie
viel größer der Bedarf ist hängt von der konkreten Verteilung der Daten ab.

## Lernen von Ähnlichkeitsmaßen {#sec:anasim}

```texalgo
#alg:simlearn PredictSimilarities(D, BS, S, P, N)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Dataset $D$
  \item Blocking Scheme $BS$
  \item Fields used in $BS$: $F$
  \item Similarity functions: $S_i,i=1 \cdots |F|$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item : $SIMS_i, i=1 \cdots |F|$
  \end{itemize}
}
\Statex
\State Initialize list $SIMS = []$
\For{field $f \in F$}\label{alg:sim:1}
  \State Initialize bestScore = 0
  \For{similarity function $sim \in S$}\label{alg:sim:2}
    \State score = $FieldScore(BS, sim, field, P, N)$\label{alg:sim:3}
    \If{$score > bestScore$}\label{alg:sim:4}
      \State $bestScore = score$
      \State $bestSim = sim$
    \EndIf\label{alg:sim:4}
  \EndFor
  \State $SIMS[field] = bestSim$
\EndFor
\State return $SIMS$

#alg:simscore FieldScore(BS, sim, field, P, N)
\Require
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item Blocking Scheme $BS$
  \item Similarity function: $sim$
  \item Attribute field: $field$
  \item Set of positive pairs: $P$
  \item Set of negative pairs: $N$
  \end{itemize}
}
\Ensure
\Statex{
  \begin{itemize}[noitemsep, topsep=0pt, leftmargin=*, label={-}]
  \item field score: $score$
  \end{itemize}
}
\Statex
\State Initialize $y_{true} = [], y_scores = []$\label{alg:sis:0}
\For{pair $(p_1.id, p_2.id) \in P \cup N$}\label{alg:sis:1}
  \State $p_1.f = D[p_1.id][field]$
  \State $p_2.f = D[p_2.id][field]$
  \For{term $t \in BS$}\label{alg:sis:2}
    \If{$field \in term$}\label{alg:sis:3}
      \State $p_{1_{bkv}} = BlockingKeyValues(t, p_1.f)$\label{alg:sis:31}
      \State $p_{2_{bkv}} = BlockingKeyValues(t, p_2.f)$\label{alg:sis:32}
      \If{$p_{1_{bkv}} \cup p_{2_{bkv}} \neq \emptyset$}\label{alg:sis:4}
        \If{$p \in P$}\label{alg:sis:5}
          \State Append 1 to $y_{true}$
        \Else
          \State Append 0 to $y_{true}$
        \EndIf
        \State Append $sim(p_1.f, p_2.f)$ to $y_{score}$\label{alg:sis:6}
      \EndIf
    \EndIf
  \EndFor
\EndFor
\State return $average\_precision\_score(y_{true}, y_{score})$
```

Aus der Vielfalt der möglichen Ähnlichkeitsmaße, gibt es keines das allen
anderen klar überlegen ist. Es ist daher sehr domainabhängig welches Maß gute
Ergebnisse liefert. Beim Vergleich von Datensätzen sind diese Domänen meist
durch die unterschiedlichen Attribute getrennt. Daher ist es notwendig
herauszufinden, für welches Attribute welche Ähnlichkeitsmetrik besonders gut
funktioniert. Bevor ein Maß für die Qualität verschiedener Ähnlichkeitsmaße
gefunden werden kann, muss das Problem der Vergleichbarkeit gelöst werden. Für
zwei Strings $a$ und $b$ liefert der Jaccard-Koeffizient beispielsweise Werte
zwischen 0 und 1, die Levenshtein-Distanz hingegen Werte zwischen 0 und
$maxlen(a, b)$. Deshalb ist es notwendig die verschiedenen Ähnlichkeitsmaße zu
normieren. Dafür wird das Intervall von 0 bis 1 gewählt, wobei 1 totale
Übereinstimmung und 0 keine Übereinstimmung bedeutet. Dieses Intervall ist
sinnvoll, da viele Klassifikatoren für diese Werte ihre Ausführungszeit
optimiert können. Ähnlichkeitsmaße, die einen höheren Wert berechnen (je
unähnlicher zwei Strings sind) werden als Abstandsfunktionen bezeichnet. Eine
wichtige Eigenschaft von Abstandsfunktionen ist es, dass diese die
Dreiecksungleichung in metrischen Räumen erfüllt. Der metrische Raum ist
definiert als $(X, d)$, wobei $X$ hier die Menge an Wörtern einer Sprache ist
und $d$ eine beliebige Abstandsfunktion. Die Dreiecksungleichung sagt aus, dass
der Abstand von $x$ nach $z$ nicht größer sein darf, als die Summe beliebiger
Zwischenschritte über andere Wörter, $x$ nach $y$ und $y$ nach $z$, daraus folgt
$d(x, z) \leq d(x, y)+ d(y, z)$, für alle $x, y, z \in X$. Bei der
Normalisierung muss berücksichtigt werden, dass die Dreiecksgleichung nicht
verletzt wird. Ansonsten ist nicht garantiert, dass sich alle normalisierten
Werte im Intervall 0 bis 1 befinden. Um korrekt zu normalisieren muss für eine
Abstandsfunktion, der größtmögliche Wert bestimmt werden. Dafür wird meist der
länger der beiden Strings genutzt. Bei den Editierdistanzen, beispielsweise der
Levenshtein-Distanz, ist dies die Anzahl der Zeichen im längeren String
$maxlen(a, b)$. In diesem Fall wird der String in jeder Postition modifiziert,
um $a$ in $b$ zu wandeln. Beispielsweise ist $Levenshtein(Liste, Baum) = 5 =
maxlen(Liste, Baum)$, weil $(L \rightarrow B, i \rightarrow a, s \rightarrow u,
t \rightarrow m, del~e)$. Daraus folgt die Normalisierungsfunktion für
Editierdistanzen: $$sim_{norm}(sim, a, b) = 1 - \frac{sim(a,
b)}{max\_sim\_val(a, b)}$$ Diese Normalisierungfunktion, anhand des größt
möglichen Wertes der Ähnlichkeitsfunktion zwischen $a$ und $b$, hat den
Nachteil, dass die errechnetet Ähnlichkeiten z.T. zu optimistisch sind. Eine
verbreitete Alternative, Abstandsfunktionen zu normalisieren, ist den
größtmöglichen Wert anhand des kürzen Strings zu berechnen. Dadurch wird zwar
überschwinglicher Optimismus, beim Normalisieren korrigiert, jedoch verstößt
diese Art der Normalisierung gegen die Dreiecksungleichung.

Durch die Normalisierung ist der Similarity Lerner in der Lage für jedes
Attribut das beste Ähnlichkeitsmaß auszuwählen. Algorithmen \ref{alg:simlearn}
und \ref{alg:simscore} beschreibt das Vorgehen. In @sec:similarity wurde
gezeigt, dass die verschiedenen Ähnlichkeitsfunktionen oft mindestens einen
Parameter haben. Da diese Parameter teilweise die Dreiecksungleichung
aufheben, beispielsweise die Gewichte bzw. Kosten bei den Editierdistanzen,
prüft der Similarity Lerner nur die Ähnlichkeitsfunktionen mit den
Standardparametern. Von der Engine bekommt er dazu eine Auswahl an
Ähnlichkeitsfunktionen $S$, das Blocking Schema $BS$, sowie die Ground Truth $P$
und $N$. Der Algorithmus prüft nur die Attribute, welche im Blocking Schema
enthalten sind (Zeile \ref{alg:sim:1}). Auf das gewählte Attribut werden die
Ähnlichkeitsfunktionen in $S$ (Zeile \ref{alg:sim:3}) durch den Algorithmus
\ref{alg:simscore} angewandt und bewertet (Zeile \ref{alg:sim:3}). Die
Ähnlichkeitsfunktion, die für ein Attribut die beste Bewertung liefert, wird für
den Indexer als Ähnlichkeitsfunktion ausgewählt. Die Liste der ausgewählten
Attribute wird abschließend an die Engine zurückgegeben. Für die Bewertung der
Ähnlichkeiten eines Attributes wird die Average Precision Score genutzt. Diese
wird berechnet anhand der Fläche unter der Precision-Recall Kurve. Die Average
Precision Funktion benötigt zur Berechnung für jedes Paar der Ground Truth die
Klasse, Match oder Non-Match, sowie den errechneten und normalisierten
Ähnlichkeitswert. Diese Werte werden in den beiden Array $y_{true}$, für die
Klassen und $y_{score}$, für die Ähnlichkeitswerte, gesammelt (Zeile
\ref{alg:sis:0}). Für jedes Paar $(p_1.id, p_2.id)$ der Ground Truth, wird
zunächst das entsprechende Attribute aus dem Datensatz $D$ geholt (Zeile
\ref{alg:sis:1}). Anhand der Attribute werden für jeden Ausdruck im Blocking
Schema $BS$, die Blockschlüssel $p_{1_{bkv}}$ und $p_{2_{bkv}}$ erzeugt (Zeilen
\ref{alg:sis:31}, \ref{alg:sis:32}). Falls die Schnittmenge der beiden
Blockschlüsselmengen mindestens einen gemeinsamen Blockschlüssel beinhaltet
(Zeile \ref{alg:sis:4}), wird die Ähnlichkeit zwischen den beiden Attributes des
Paares berechnet \ref{alg:sis:6}. Der Ähnlichkeitswert wird in die Liste
$y_{scores}$ aufgenommen. Ist die Schnittmenge leer, so haben die Attribute des
Paares keinen gemeinsamen Block und werden folglich vom Indexer nicht
miteinander verglichen, weshalb der Vergleich auch hier übersprungen wird.
Zusätzlich wird der Liste $y_{true}$ eine 1 angefügt, wenn das Paar ein Match
ist bzw. eine 0, wenn das Paar ein Non-Match ist. Abschließend wird die Average
Precision Score berechnet und zurückgegeben.

## Lernen der Hyperparameter der Klassifikatoren

Für jeden Klassifikator sollen, anhand eines geeigneten Qualitiätsmaßes, die
besten Parameter bestimmt werden. Die einfachste Möglichkeit dafür ist eine Grid
Search. Diese erzeugt ein Parameternetz aller möglichen Parameterkombinationen
und sucht dieses vollständig ab. Dadurch ist zwar sichergestellt, dass die
besten Parameter gefunden werden, je mehr Parameter es gibt, desto länger dauert
diese Suche allerdings. Eine einfache Möglichkeit den Suchraum zu reduzieren
ist, per Zufall nur eine bestimmte Anzahl an Parameterkombinationen zu
bestimmten und nur diese zu vergleichen. Neben diesen beiden dynamischen
Methoden zum Lernen der Hyperparamter, welche modellübergreifend funktionieren,
gibt es in Scikit-learn auch spezialisierte Parametersuchen, beispielsweise für
Logistic Regression, welche effizienter die Parameter für ihr
Klassifikationsmodell finden. Der Fusion-Lerner sucht für jeden Klassifikator
seperat die besten Parameter. Dabei wird das Ergebnis der besten Konfiguration
mit den besten Konfigurationen anderer Klassifikatoren verglichen. Zum Schluss
wird der Klassifikator ausgewählt, dessen beste Konfiguration, bei gegeben
Qualitätsmaß das beste Ergebnis liefert.

Egal welches Suchverfahren genutzt wird, wichtig ist, dass beim Vergleichen,
Validieren und Auswählen der Parameter bzw. der Modelle darauf geachtet wird,
dass das Modell nicht überanpasst und dadurch ausschließlich auf den
Trainingsdaten gute Ergebnisse erzielt werden. Um dies zu unterbinden, werden
Kreuzvalidierungsverfahren genutzt, welche ebenfalls in Scikit-learn
implementiert sind. Dabei unterscheidet man zwischen vollständiger
Kreuzvalidierung und nicht-vollständiger Kreuzvalidierung. Ein Beispiel für die
vollständige Kreuzvalidierung ist Leave-one-out. Dabei werden aus den
Trainingsdaten mit $n$ Objekten $n$ Untermengen gebildet, bei welchen jeweils
ein Element fehlt. Eine dieser Menge wird als Validierungsmenge ausgewählt,
anhand welcher ein trainiertes Modell überprüft wird. Die anderen werden als
Trainingsdaten genutzt. Das Verfahren wird $n$-Mal wiederholt, bis jede Menge
als Validierungsmenge genutzt wurde. Das Ergebnis ist das Mittel aller
Durchläufe. Da dies z.T. sehr lange dauert gibt es unvollständige Verfahren,
wie das K-Fold. Dieses bildet zufällig $k$ gleichmächtige Untermengen der
Trainingsdaten. Eine dieser Mengen wird, analog zum Leave-one-out, zum
Validieren ausgewählt. Die anderen $k-1$ Mengen werden als Trainingsdaten
genutzt. Das ganze wird $k$-Mal wiederholt, bis jede Menge einmal als
Validierungsmenge genutzt wurde. Das Ergebnis der $k$ Durchläufe wird ebenfalls
gemittelt und als ein Wert zurückgegeben. Ist $k=n$ entspricht K-Fold dem
Leave-one-out Verfahren. Eine beliebte Erweiterung des K-Fold ist der Stratified
K-Fold. Dieser unterscheidet sich lediglich in der Generierung der Untermengen.
Dabei werden Objekte ebenfalls zufällig aus der Trainingsmenge selektiert,
jedoch wird darauf geachtet, dass das Verhältnis der Klassenzugehörigkeit der
Objekte bestehen bleibt. Befinden sich in der Ausgangstrainingsmenge,
beispielsweise 30% Matches und 70% Non-Matches, dann haben alle $k$ Untermengen
ebenfalls dieses 30% zu 70% Verhältnis. Damit wird sichergestellt, dass jede
Untermenge eine gute Repräsentation des Ganzen ist und folglich die Ergebnisse
mehr Aussagekraft haben.

Hauptverantwortlich für die Qualität eines Modells ist die Ground Truth. Je mehr
herausfordernde Matches und Non-Matches diese enthält, desto genauer kann ein
Modell trainiert werden. Herausfordernd sind Paare, wo nicht offensichtlich ist,
ob diese zu den Matches oder Non-Matches gehören. Allerdings kann für bestimmte
Klassifikatoren die Größe der Ground Truth dazu führen, dass die Effizienz stark
beeinträchtigt wird. Dies ist der Fall bei einer SVM, weil durch eine größere
Ground Truth entsprechend mehr Stützvektoren erzeugt werden, die bei jeder
Klassifikation verglichen werden müssen. Eine Lösung für dieses Problem ist das
sog. Subsampling. Hierbei wird ein Kompromiss zwischen Qualtität und Effizienz
getroffen, indem die Anzahl der Paare der Ground Truth reduziert wird. Soll beim
Subsampling die Repräsentativität der Ground Truth erhalten bleiben, können
Paare aus den Matches bzw. Non-Matches nach ihrer Ähnlichkeitsverteilung, wie in
@sec:ana_lbl, ausgewählt werden.
