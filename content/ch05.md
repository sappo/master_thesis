# Evaluierung der Implementierung

In diesem Kapitel wird die Programmierumgebung, die Implementierung der Engine
und die Implementierung der Komponenten evaluiert. Eine große Herausforderung
bei der Umsetzung der Algorithmen war es, diese für die begrenzten Ressourcen,
insbesondere Arbeitsspeicher und Rechenzeit, zu optimieren.

## Programmierumgebung

Als Programmiersprache für die Implementierung wurde Python und C eingesetzt.
Wobei C lediglich zur Implementierung der Ähnlichkeitsberechnung eingesetzt
wurde, alle anderen Teile wurden mit Python umgesetzt. Python hat den Vorteil,
dass es sehr einfach und schnell möglich ist, einen Prototypen eines
Algorithmuses zu entwickeln und zu testen. Zudem gibt es eine Vielzahl von
Qualitativ hochwertigen Paketen, die komkompfortable Standardfunktionalitäten
bereitstellen, beispielsweise das Einlesen und das Schreiben von großen
CSV-Dateien oder das Plotten von Graphen. Des Weiteren wird Python im Maschine
Learning Bereich oft genutzt, was dazu führt, dass es eine Vielzahl von
effizienten, ausgereiften und umfangreichen Frameworks gibt, um verschiedenste
Lernaufgaben zu behandeln. Vor allem der Fusion-Lerner und der Klassifikator
profitieren hiervon.

Der große Nachteil von Python ist das Global Interpreter Lock (GIL). Dieses
verhindert, dass Python-Code in mehreren Threads gleichzeitig ausgeführt werden
kann. Die Multithreading Bibliothek von Python ist daher lediglich geeignet, um
Programme mit hoher E/A-Last zu beschleunigen, da Schreib- bzw. Lesezugriffe das
GIL freigeben. Der Grund warum in Python ein GIL einzusetzen wird ist, dass
dadurch die Single-Thread Ausführung optimiert wird. Multithreading, im Sinne
von Gleichzeitigausführung, d.h. ein Prozess mit mehreren Threads, die auf
verschiedenen Prozessorkernen, zur selben Zeit ausgeführt werden, wird dadurch
allerdings komplett unterbunden. Um denoch Python zu paralelisieren gibt es zwei
beliebte Möglichkeiten. Die erste Möglichkeit ist, statt Multithreading,
Multiprocessing einzusetzen. Das hat allderdings den Nachteil, dass Daten
zwischen Prozessen ausgetauscht werden müssen. Das lohnt sich offensichtlich nur
für rechenintensive Aufgaben, wo der Overhead des Datenaustausches keine Rolle
spielt. Die zweite Möglichkeit ist das Multithreading in einer anderen
Programmiersprache umzusetzen, beispielsweise in C. Dies ist möglich, da das GIL
lediglich die Mehrfachausführung von Python-Code verhindert. Allerdings erweist
sich dies oft als relativ schwierig, da selbst einfache Datenklassen,
beispielsweise `set` oder `dict`, keine Entsprechung in C haben und daher
manuell, in beide Richtungen Python zu C und C zu Python, z.T. aufwendig
konvertiert werden müssen.

Aufgrund der genannten Nachteile von Python wurde die Engine und sämtliche
Komponenten lediglich in einem Thread ausgeführt. Ideen dies zu optimieren
konnten nicht im Zeitrahmen der Thesis umgesetzt werden. Dabei kann vor allen in
der Fit-Phase durch Multi-Threading und Parallel Programming Laufzeit eingespart
werden. Die längste Laufzeit haben dabei der DNF Blocks Lerner und der
Fusion-Lerner.

## Label Generator

Für den Label Generator wurden beide Ausprägungen (mit und ohne Ground Truth)
umgesetzt. Zunächst wird die Variante ohne Ground Truth beschrieben und
anschließend die Variante mit Ground Truth, welche eine Modifikation der ersten
Ausprägung ist.

Der Label Generator wurde gegenüber dem Algorithmus \ref{alg:weaklabels} von
Kejriwal & Mirankern [@KM:Unsupervised:13] und dessen Anpassung mit Ground Truth
Matches in Algorithmus \ref{alg:labels} in zwei Punkten modifiziert. Zunächst
werden die Datensätze in den Blöcken alphabetisch sortieren. Damit ist es
möglich deterministische Ergebnisse zu bekommen und daraufbasierend geeignete
Testfälle zu schreiben. Des Weiteren werden wie beim klassischen Sorted
Neighborhood Verfahren, dadurch ähnliche Datensätze näher zusammengebracht, was
die Wahrscheinlichkeit erhöht aussagekräftige Paare zu selektieren. Die zweite
Anpassung ist sowohl eine Laufzeit-, als auch ein Arbeitsspeicheroptimierung.
Ähnlich zum Record Identifier Index der Similarity-Aware Inverted-Index
Verfahren, kann es durch das Blocking auf Basis der Token ebenfalls dazu kommen,
dass rießige Blöcke erzeugt werden. Selbst wenn ähnliche Attribute durch
Sortierung näher zueinander sortiert wurden, ist in diesen Blöcken ein größes
Fenster nötig, um aussagekräftig Paare zu finden. Dies wiederum führt zu einer
Explosion der Kandidatenmenge und damit des Arbeitsspeichers und der Laufzeit.
Zur Optimierung wird ein Blockfilter eingeführt, sodass lediglich Kandidaten in
Blöcken generiert werden, deren Anzahl an Datensätzen kleiner einer Schwelle $z$
sind.

## Blocking Schema Lerner {#sec:eval_dnflearner}

Die Algorithmen des DNF Blocks Lerners haben bei der Implementierung das
Problem, das nur eine bestimmte Menge an Arbeitsspeicher zur Verfügung steht.
Der kritische Teil des Algorithmus ist die Erzeugung der Paarkombinationen, für
jeden Block. Angenommen die beiden Datensatzidentifier eines Paares $(p1.id,
p2.id)$ sind Integerwerte und der Datensatz hat nicht mehr als 2^30 Einträge,
dann benötigt ein Integerwert 28 Bytes. Um möglichst effizient auf die Paare
zuzugreifen, ist die Menge von Paarkombinationen als `set` implementiert. Damit
ein `set` $s$ ein Zugriffkomplexität von $O(1)$ ermöglichen kann, wird für jedes
Element in der Menge ein Hashwert berechnet. Auf einem 64-bit System beträgt die
Größe dieses Hashwertes $h$ 8 Bytes. Somit benötigt ein Eintrag $(h_j, p1_j.id,
p2_j.id) \in s$ 64 Bytes. Angenommen es werden für einen Block mit 1.000
Einträgen Paarkombinationen erzeugt. Bei Attributen mit wenigen möglichen Werten
können Blöcke entstehen, die sehr viele Datensätze enthalten. Beispielsweise hat
ein Block mit 10.000 Einträgen 49.995.000 Paare und benötigt 2.9 GB an
Arbeitsspeicher. Somit kann bereits ein rießiger Block den zur Verfügung
stehenden Arbeitsspeicher sprengen und führt damit zum Abbruch des Programmes.
Aus diesem Grund wurde der Algorithmus dahingehend erweitert, dass die Erzeugung
der Paare bei Ausdrücken, die zu viele Paare erzeugen würden, unterbunden wird
und dies Ausdrücke mit der niedrigsten Wert der Bewertungsskala bewertet werden.

Zur genaueren Analyse des Problems, wird die Verteilung der Blöcke, anhand ihrer
Größe (Anzahl von Datensätzen), betrachtet. Um die Verteilungen in Gute,
benötiget weniger Arbeitsspeicher als zur Verfügung steht und Schlechte,
benötiget mehr Arbeitsspeicher als zur Verfügung steht, zu kategorisieren, wurde
eine Schwelle $t$ eingeführt. Anhand dieser Schwelle wird ein Block $B$ bei $|B|
< t$ als guter Block und bei $|B| > t$ als schlechter Block bewertet. Daraus
kann für jede Verteilung berechnet werden, wie viel Prozent gute bzw. schlechte
Blöcke es gibt. Dadurch ist es möglich bei Audrücken mit einer höheren
schlechten Blockrate von $b$, beispielsweise $b=0.1$, die Erzeugung der
Blockpaare zu verhindern und die weitere Verarbeitung abzubrechen. Da aber
bereits ein einziger schlechter Block, mit genügend Einträgen, den
Arbeitsspeicher überfüllen kann, wird mit der Schwelle $b$ lediglich eine
Vorauswahl, besonders schlechter Ausdrücke, getroffen. Für den Fall, dass es nur
wenige schlechte Blöcke gibt, bestehen deren Blockschlüssel meistens aus
Stopwörtern, beispielsweise bei Strassennamen `Strasse`, `Weg`, oder `Platz`.
Dieses Problem kann folglich durch eine bessere Vorverarbeitung der Daten gelöst
werden. Da es das Ziel ein selbstkonfigurierendes System ist, muss die Engine,
die auf diese Weise gefundenen Stopwörter nutzen und den Lernvorgang mit der
erweiterten Vorverarbeitung der Daten wiederholen. Dieser Prozess sorgt
allerdings dafür, dass das Lernen der Konfiguration deutlich länger dauert. Eine
einfacherere Möglichkeit ist, für jeden Ausdruck eine Liste mit verbotenen
Blockschlüsseln anzulegen und die Blockschlüssel schlechter Blöcke dort
hinzuzufügen. In der Build- und Query-Phase dürfen diese Blöckschlüssel vom
Indexer demnach nicht genutzt werden.

Trotz dieser Optimierungen hat der DNF Blocks Generator immer noch hohe
Arbeitsspeicheranforderungen, welche verhindern das Multithreading oder
Multiprocessing auf einem Rechner zur Laufzeitoptimierung eingesetzt werden
können. Denkbar ist aber die Verteilung auf ein Cluster von Rechnern,
beispielsweise per Hadoop, wodurch deutlich mehr Prädikate in kürzerer Zeit
überprüft werden können.
