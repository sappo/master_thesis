# Evaluation \label{chap:evaluation}

Diese Kapitel präsentiert die Ergebnisse der Evaluation. Zunächst wird dazu der
experimentelle Aufbau erläutert. Im Anschluss werden die Komponenten des Systems
bestimmt, für welche Alternativen zur Verfügung stehen. Danach findet die
Festlegung der noch offenen freien Parameter statt. Für alle zu wählenden freien
Parameter wird eine Abwägung zwischen Qualität und Effizienz durchgeführt. Als
Nächstes wird der Einfluss der Ähnlichkeitsvektoren und Ähnlichkeitsfunktionen
auf die Performanz des Systems geprüft. Anschließend wird das
selbstkonfigurierende System gegen eine manuelle Baseline evaluiert und zum
Schluss wird betrachtet welche Ergenisse mit einer vollständig synthetisierte
Ground Truth Paare erzielt werden können.

## Experimenteller Aufbau

Der experimentelle Aufbau beschreibt alle Komponenten und Schritte die benötigt
werden, um die Evaluation durchzuführen und auszuwerten. Zunächst wird
beschrieben, wie die Metriken berechnet wurden. Anschließend werden die
Datensätze und deren Aufbereitung für die Evaluierung beschrieben. Danach wird
beschrieben in welchen Schritten welche Metriken berechnet wurden und
abschließend wird die genutzt Hardware bekannt gemacht.

### Berechnung der Metriken für dynamische Entity Resolution

Während im statischen Entity Resolution die Metriken (vgl. @sec:measurements) am
Ende des Verfahrens einmalig berechnet werden können, ist dies im dynamischen
Falle nicht möglich, da ein Datenstrom kein definiertes Ende hat. Das bedeutet,
die Metriken müssen inkrementell mit jeder Anfrage $q$ erhoben werden. Für
Anfragen ohne tatsächliche Matches wird lediglich das Reduction Ratio erhoben,
da alle anderen Metriken keine Aussagekraft haben, beispielsweise wäre der
Recall undefiniert und die Precision immer 0. Zur Berechnung der
Effizienzmaße Pairs Completeness, Pairs Quality und Reduction Ratio
werden die tatsächlichen Matches $n_M$, die tatsächlichen Non-Matches $n_N$, die
Matches in der Kandidatenmenge $s_M$ und Non-Matches in der Kandidatenmenge
$s_N$ benötigt. Die Kandidatenmenge wird mit $C$ bezeichnet, die tatsächlichen
Matches mit $P$ und die Menge der Datensätze des Indexers mit $IX$. $s_M$ ist
die Anzahl der Matches zur Anfrage $q$ in $C$, $s_N$ ist die Anzahl der
Non-Matches zu $q$ in $C$, $n_M$ ist die Gesamtanzahl der Matches zu $q$ in den
Matches $P$ und $n_N$ ist die Gesamtanzahl an Non-Matches zu $q$ in $IX$. Für
$n_N$ muss der Anfragedatensatz von der Gesamtmenge abgezogen werden, da dieser
zu Begin jeder Anfrage vom Indexer in den Datenbestand aufgenommen wird, bzw.
wenn er dort schon vorhanden ist keine Rolle für die Enity Resolution spielt, da
er herausgefiltert wird. Mit jeder Anfrage werden $s_M, s_N$ und $n_M$ mit den
vorherigen Werten aufsummiert, sodass die Effizienzmaße Bezug auf alle bisher
gestellten Anfragen nehmen. Die Anzahl $n_N$ nimmt Bezug auf eine wachsende
Menge, sodass beim Aufsummieren die frühen Anfragen abgewertet werden. $n_M$
wird zur Berechnung des Reduction Ratio benötigt, sodass dieses für jede Anfrage
berechnet, gespeichert und auf Abruf gemittelt wird.

Die Qualitätsmaße Recall, Precision, F-measure und Average Precision werden über
die True Positives (TP), False Positives (FP) und False Negatives (FN) bestimmt.
Deren Berechnung ist identisch zu den Werten der Effizienzmaße mit der
Abweichung, dass diese auf der Ergebnismenge $R$ gemessen werden. Die True
Negatives werden nicht berechnet, da diese in den Metriken nicht benötigt
werden. Auch hier werden die Werte für jede Anfrage summiert, sodass die aus der
Summe berechneten Metriken alle bisherigen Anfragen berücksichtigen.

### Ähnlichkeitsmaße {#sec:eval_sim}

Der Similarity Lerner wählt aus einer Menge von Ähnlichkeitsfunktionen die
besten aus. Für die Evaluierung wurden dem Similarity Lerner die folgenden 7
Metriken übergeben, wobei die letzen vier aus @sec:similarity bekannt sind:

* Bag-Distanz [@BCP:String:02]
* Compression-Distanz [@CV:Clustering:05]
* Hamming-Distanz [@Ham:Error:50]
* Jaro-Distanz
* Jaro-Winkler-Distanz
* Jaccard-Koeffizent
* Levenshtein-Distanz

Dazu wird die Implementierung der Algorithmen aus der *Harry* Bibliothek
[@RW:Harry:16] von Rieck & Wressnegger genutzt. Dabei handelt es sich um eine
Open-Source C-Bibliothek. Bei der Verwendung dieser Bibliothek ist die
Problematik aufgetreten, dass die Variablen ausschließlich auf dem Stack
gehalten werden. Da eine Ähnlichkeitsfunktion vor Benutzung konfiguriert werden
muss, ist es in einem Prozess folglich nur möglich ein Ähnlichkeitsmaß
gleichzeitig zu verwenden und häufiges Wechseln der Funktion ist dadurch mit
hohem Mehraufwand verbunden. Deshalb wurden im Rahmen dieser Arbeit die
Berechnung und Normalisierung der Ähnlichkeiten umgeschrieben, sodass alle
Variablen auf den Heap allokiert werden. Als Folge dessen ist es nach einmaliger
Konfiguration möglich, beliebig viele Ähnlichkeitsfunktionen gleichzeitig zu
nutzen. Zudem wurde eine Pythonschnittstelle entwickelt, damit diese Funktionen
durch den Similarity Lerner aufgerufen werden können. Die modifizierte
Bibliothek ist unter dem Namen *libsimilarity*[^simlib] auf Github, als
Open-Source Software, verfügbar. Für den Similarity Lerner wurden alle genutzten
Ähnlichkeitsmaße derart konfiguriert, dass beim Normalisieren der Distanzen, auf
den Wertebereich zwischen 0 und 1, die Dreieckungleichung eingehalten wird.

[^simlib]: [https://github.com/sappo/libsimilarity](https://github.com/sappo/libsimilarity)

### Datensätze {#sec:datasets}

Im Folgenden werden die Datensätze beschrieben, die in der Evaluation genutzt
werden. @tbl:datasets_overview bietet einen Überblick über alle Datensätze.
Dabei ist der NCVoter Datensatz mit Abstand der größte, weshalb diesem besondere
Aufmerksamkeit in der Evalution geschenkt wird. Die Duplikatspaare beinhalten
sowohl Matches zwischen zwei Datensätzen, als auch Matches zwischen Cliquen
(mehr als 2 Datensätze), wobei die Cliquen vollständig als Paare aufgelöst sind.
Beispielsweise gibt es für eine Clique (1,2,3) die Paare (1,2), (1,3) und (2,3).
Die Duplikatspaare wurden im Falle von des Cora und des Restaurant Datensatzes
von Hand identifiziert und für die anderen Datensätze semi-automatisch über
verschiedene Verfahren bestimmt.

```tex
\begin{table}
\centering
\begin{tabular}{llll}\toprule
Datensatz             &  Einträge & Duplikatspaare & Attribute \\ \midrule
Abt-Buy               &     2.171 &          1.096 &         4 \\
Amazon-GoogleProducts &     4.587 &          1.299 &         4 \\
Cora                  &     1.879 &         64.577 &         5 \\
DBLP-ACM              &     4.908 &          2.223 &         4 \\
DBLP-Scholar          &    66.877 &          5.346 &         4 \\
NCVoter               & 8.261.839 &        155.470 &        17 \\
Restaurant            &       864 &            112 &         4 \\ \bottomrule
\end{tabular}
\caption{Überblick der verwendeten Datensätze}
\label{tbl:datasets_overview}
\end{table}
```

#### CORA[^cora]

Der CORA Datensatz beinhaltet 1879 bibliographische Einträge über
wissenschaftliche Veröffentlichungen aus dem Maschine Learning Bereich. Die
Einträge bestehen aus Autor, Jahr, Titel, Technologie, Institution, Buchtitel,
Bearbeiter, Verlag, Seiten, Adresse, Monat, Notiz, Journal, Ausgabe Typ und
Datum. Insgesamt beinhaltet dieser Datensatz 64.577 Duplikate. Dieser Datensatz
ist besonders schwierig zu Deduplizieren, da teilweise nur Initialen der Autoren
vorhanden sind bzw. Attribute zusammengefügt oder getauscht wurden.

#### Abt-Buy & Amazon-GoogleProducts[^fever]

Diese beiden Datensätze beinhalten Produkte aus dem Onlinehandel verschiedener
Plattformen mit Name, Beschreibung, Hersteller und Preis. Der Abt-Buy Datensatz
beinhaltet 2171 Einträge mit 1096 Duplikaten. Im Amazon-GoogleProducts Datensatz
sind es 4587 Einträge mit 1299 Duplikaten.

#### DBLP-ACM & DBLP-Scholar[^fever]

Diese beiden Datensätze beinhalten bibliografische Einträge mit Titel,
Autor(en), Konferenz, und Jahr. Der DBLP-ACM Datensatz beinhaltet 4908 Einträge
und 2223 Duplikate. Im DBLP-Scholar Datensatz sind sind 66877 Einträge mit 5346
Duplikaten. Dabei ist zu beachten, dass der DBLP-ACM Datensatz einfach zu
klassifizieren ist, da ein Großteil der Daten durch eine Instanz gepflegt
werden.

#### Restaurant[^res]

Der Restaurant Datensatz ist ein kleiner mit lediglich 864 Einträgen, welche aus
Restaurantname, Adresse, Telefonnummer und der Küchenart bestehen. Es gibt
insgesamt 112 Restaurantduplikate, welche doppelt vorkommen.

[^cora]: https://hpi.de/naumann/projects/repeatability/datasets/cora-dataset.html
[^fever]: http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution
[^res]: http://hpi.de/naumann/projects/data-quality-and-cleansing/dude-duplicate-detection.html

#### NCVoter

Der NC Voter Registration (NCVoter) Datensatz beinhaltet ca. 8 mio Datensätze aus
dem Wählerverzeichnis des Bundestates North Carolina in den USA. Eine genaue
Analyse des Datensatzes wurde von Christen [@Chr:Preparation:13] durchgeführt.
Der Datensatz beinhaltet ca. 145.000 Duplikate zwischen zwei Einträgen, sowie
3.500 zwischen drei und mehr Einträgen. Die Zuordnung der Duplikate wurde dabei
über die Wählerregistriernummer getätigt. Weitere Attribute sind Namenspräfix,
Vorname, Zweiter, Vorname, Nachname, Namenssuffix, Alter, Geschlecht,
Rassenziffer, Ethnizitätsziffer, Strasse + Hausnummer, Stadt, Bundesland,
Postleitzahl, Telefonnummer, Geburtsort und Registrierdatum.

#### Febrl

Die Febrl-Datensätze wurden synthetisch durch den Febrlgenerator [@Chr:Febrl:08]
erzeugt. Die Attributsdaten dafür liefert ein australisches Telefonbuch. Die
generierten Einträge haben folgende Attribute: Kultur, Geschlecht, Alter,
Geburtsdatum, Titel, Vorname, Nachname, Bundesland, Vorort, Postleitzahl,
Hausnummer, Straße und Telefonnummer.

* Febrl-4k-1k: 5.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-9k-1k: 10.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-90k-10k: 100.000 Einträge mit 10.000 Duplikaten zwischen zwei Datensätzen

### Aufbereitung der Datensätze

![Aufteilung der Datensätze in Validierungsmenge, Trainingsmenge und Testmenge.
Tupel in den Mengen sind durch Punkte markiert und Duplikate durch eine Line
zwischen zwei Tupeln. Die farbigen Linen zeigen, wie die jeweilige Untermenge
gebildet wird. ](./images/testsets.svg){#fig:testsets width=80%}

Für die Durchführung der Evaluierung wurde der NCVoter Datensatz aus
@sec:datasets in jeweils vier disjunkte Teile gesplittet. Diese Aufteilung ist
in @fig:testsets dargestellt. Die Hälfte der Datensätze eines Datensatzes
befindet sich in der Base und die andere Hälfte ist zu gleichen Teilen in
Validierung, Training und Testing aufgeteilt. Datensätze in den Mengen sind
durch schwarze Punkte markiert. Matches sind durch Linien verbunden. In der
Build-Phase wird der initiale Index stets aus den Datensätzen der Base gebaut.
Der Anfragestrom, in der Query-Phase, wird durch Datensätze aus Validierung,
Training oder Testing zusammengestellt. Durch die Verteilung der Matches ist
sichergestellt, dass dadurch für jedes Match eine Query durchgeführt wird, in
welcher das jeweilige andere in der Base gefunden werden kann. In der Fit-Phase
werden die Duplikate zusammen benötigt, weshalb jeweils Validierung, Training
und Testing mit der Base zusammengefasst werden, wie durch die rote, grüne bzw.
blaue Umrandung dargestellt ist.

![Vorgehen zur Aufteilung eines Datensatzes in vier Teilmengen. Datensätze
werden in drei Kategorien zugeordnet: Non-Matches (rot), Matches (grün),
Matchcliquen (blaugrün). Diese werden seperat in Teil 2, 3 und 4 aufgeteilt.
](./images/testsetssplitting.svg){#fig:testsetssplit}

Das Vorgehen der Aufteilung eines Datensatzes ist in @fig:testsetssplit
dargestellt. Dazu werden die Datensätze in drei Kategorien eingeteilt (Teil 1):
Non-Matches (rot), Matches zwischen zwei Datensätzen (grün) und Cliquen von
Matches (blaugrün). Zuerst werden die Cliquen auf die Mengen verteilt (Teil 2).
Dafür wird jeweils ein Datensatz bestimmt, welcher der Base zugewiesen wird,
hier `1` und `16`. Anschließend werden die restlichen Datensätze der Cliquen per
Round-Robin auf Validation, Training und Testing verteilt. Für die erste Clique
bedeutet das `1` in die Base, `2` in Validation und `3` in Training. Bei der
zweiten Clique kommt `16` in die Base, `17` in Testing, da das Round-Robin der
vorherigen Clique vorgesetzt wird, `18` in Validation und `19` in Training.
Danach werden die übrigen Paare ebenfalls über Round-Robin aufgeteilt (Teil 3).
Jeweils ein Datensatz der Paare wird der Base zugewiesen (`7`, `9`, `11`) und
der andere wird auf Validation, Training und Testing verteilt, wobei der
Round-Robin Mechanismus unabhängig von dem der Cliquen ist. Zum Schluss werden
die Non-Matches aufgeteilt (Teil 4). Dazu werden jeweils drei Datensätze der
Base zugewiesen und anschließend drei per Round-Robin auf Validation, Training
und Testing verteilt. Dieser Round-Robin Mechanismus ist ebenfalls unabhängig
von den beiden anderen. Durch die drei unabhängigen Round-Robin Aufteilungen,
kann es dazu kommen, dass der Testing Datensatz bis zu drei Datensätze weniger
hat als Validation oder Testing. Bei tausenden bzw. Millionen von Datensätzen
ist dies jedoch nicht ausschlaggebend.

Die übrigen Datensätze wurden nach demselben Verfahren in lediglich zwei
disjunkte Mengen geteilt, sodass weiterhin 50 % der Datensätze in der Base sind
und die anderen 50 % in der Anfragemenge.

### Durchführung

Die Durchfürung der Evaluation erfolgt pro Durchlauf in drei Schritten. Im
ersten Schritt konfiguriert sich das System selbst und die Konfiguration wird
abgespeichert. Anschließend wird die Build- und Query-Phase zum ersten Mal
durchgeführt, um die Metriken für Qualtität und Effizienz zu bestimmen. Im
letzten Schritt wird die Build- und Query-Phase ein zweites Mal durchgeführt, um
Laufzeiten zu messen, die durch die Erhebung der Metriken in Schritt 2
verfälscht wurden. Insbesondere wird die Zeit zum Einfügen in den Index und zum
Anfragen der Duplikate für jeden Datensatz gemessen. Dabei wird für die
Abschnitte \ref{sec:sel_comp}-\ref{sec:gtvsnogt} lediglich der NCVoterDatensatz
benutzt. Des Weiteren werden für die Abschnitte
\ref{sec:sel_comp}-\ref{sec:esim_metrik} die Validierungsdaten des NCVoter
Datensatzes genutzt. Die übrigen Datensätze werden in @sec:ds_eval evaluiert.

Die Evaluation wurde auf 19 komponentengleichen Linuxrechnern, mit einer
Intel(R) Core(TM) i7-6700 CPU mit 3.40GHz und 8 Prozessorkernen, sowie 32 GB
Arbeitsspeicher, durchgeführt. Diese Rechner sind Bestandteil eines Rechnerpools
an der Hochschule RheinMain, der für die Nutzung von Studierenden bereit steht.
Während der Tests wurde die Hardware nicht von anderen Studierenden genutzt.

## Auswahl der Komponenten {#sec:sel_comp}

Die Komponenten des selbstkonfigurierenden Systems wurden in der Analyse in
Kapitel 3 und im Design in Kapitel 4 vorgestellt. Dabei ist der Label Generator
(@sec:ana_lbl), der Blocking Schema Lerner (@sec:ana_bs), und der Similarity
Lerner (@sec:anasim) fix. Alternativen gibt es für jeweils für Parser,
Präprozessor, Fusion-Lerner, Klassifikator und Indexer und werden hier
untersucht.

Die Datensätze aus @sec:datasets liegen alle im CSV-Format vor, weshalb auch die
gesplitteten Datensätze ebenfalls ins CSV-Format geschrieben wurden, um alle
Datensätze einheitlich von einem CSV-Parser zu verarbeiten. Im Rahmen dieser
Arbeit konnte der CSV-Parser nicht dahingehend anpasst werden, zuverlässig die
Attributstypen zu bestimmen. Aus diesem Grund werden alle Attribute als Strings
behandelt, was zudem anderem die Wahl geeigneter Blockingprädikate für den
Blocking Schema Lerner vereinfacht.

Weiterhin sind alle Datensätze aus @sec:datasets in englischer Sprache, weshalb
der Präprozessor bekannt englische Stopwörter herausfiltert und anschließend
alle Attribute in kleinschreibweise konvertiert. Weitere Attributs- bzw.
Datensatzspezifische Anpassungen werden nicht durchgeführt.

Um die Hyperparameter der Klassifikatoren zu erlernen muss der
Fusion-Lerner insbesondere Wissen, wie deren API Schnittstelle ist, damit er die
Modelle trainieren und auswerten kann. Aufgrunddessen und weil die
Implementierung von verschiedenen Lernverfahren und Klassifikatoren nicht
Schwerpunkt der Thesis ist, wurde für die Umsetzung die Python Maschine Learning
Bibliothek Scikit-learn [@PVG.EA:Scikitlearn:11] eingesetzt. Diese bietet ein
breites Spektrum an Funktionen:

* Klassifikation, bestimmen zu welcher Klasse ein Objekt gehört.
* Regression, einen fortlaufenden Wert eines Objektes vorhersagen.
* Clustering, automatisches gruppieren von Objekten.
* Dimensionsreduktion, reduzieren der Anzahl zu betrachtender zufälliger
  Variablen.
* Modellauswahl, vergleichen, validieren und auswählen von Parametern und
  Modellen.
* Vorverarbeitung, Eingabetransformation und Normalisierung.
* Evaluation, berechnen der Effizienz und Qualität von Modellen.

Für den Fusion-Lerner sind dabei das Module zur Modellauswahl, zur Evaluation
und zur Klassifikation interessant. Dieser ist zudem die einzige Komponente, die
ihre Aufgabe parallelisieren kann, da dies in Scikit-learn transparent
implementiert ist. Zur Auswahl stehen eine Grid Search und eine zufällige Suche
mit begrenzter Tiefe. Tests mit dem größten Datensatz (NCVoter) haben ergeben,
dass die Zeit, die eine Grid Search benötigt (> 1 Stunde), für die Evaluation
vertretbar ist. Deshalb wird die Klasse `GridSearchCV` verwendet. Für die
Kreuzvalidierung wird der Stratified K-Fold (implementiert in `StratifiedKFold`)
verwendet, da dieser das Verhältnis der Ground Truth beibehält.

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_index_bt.pdf}
        \caption{MDySimII vs MDySimIII - Bauzeit}
        \label{fig:IIvsIIIbt}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_memusg.pdf}
        \caption{MDySimII vs MDySimIII - Speicherverbrauch}
        \label{fig:IIvsIIImem}
    \end{minipage}
\end{figure}
```

Die als Klassifikator nutzbaren Komponenten müssen zur Grid Search kompatibel
sein. Das Scikit-learn Klassifikationsmodul beinhaltet dazu SVMs, DecisionTrees,
neuronale Netze und mehr. Diese Implementierungen können ohne Anpassungen mit
der Scikit-learn `GridSearchCV` verwendet werden. Die vorgestellten
Klassifikatoren in @sec:autolearn verwenden hauptsächlich Decision Trees und
Support Vector Machines. Deshalb werden diese beiden für die Evaluation
eingesetzt. Die entsprechenden Scikit-learn Klassen sind
`DecisionTreeClassifier`, `SVC` für SVM mit RBF-Kernel und `LinearSVC` für SVM
mit Linearkernel.

```tex
\begin{figure}
    \centering
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_prc.pdf}
        \caption{MDySimII vs MDySimIII - Precision-Recall Kurve}
        \label{fig:IIvsIIIprc}
    \end{minipage} \\
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_tc_query.pdf}
        \caption{MDySimII vs MDySimIII - Anfragezeiten}
        \label{fig:IIvsIIIqry}
    \end{minipage}
\end{figure}
```

Der Indexer kann durch den MDySimII oder den MDySimIII aus @sec:anaindxer
besetzt werden. Beide Indexer wurden auf dem ferbl-9k-1k-1 Datensatz mit Ground
Truth gegeneinander getestet, weil das MDySimII Verfahren auf dem
NCVoter-Datensatz nicht in angemessener Zeit durchgeführt werden konnte. Für den
MDySimII wurde ein Blocking Schema mit einem zweistelligen Ausdruck gelernt und
für den MDySimIII wurde ein Blocking Schema mit einem zweistelligen und einem
einstellingen Ausdruck. In @fig:IIvsIIIbt sind die Bauzeiten der beiden Indexer
verglichen. Aufgrund der komplexeren Struktur und des längeren Blocking Schemas
schneidet der MDySimIII hier erwartungsgemäß leicht schlechter ab.
@fig:IIvsIIImem zeigt den Speicherbedarf beider Indexer. Überraschend ist, dass
der MDySimII fast das doppelte an Speicher benötigt, obwohl dieser die Vorteile
in der Struktur und dem einfacheren Blocking Schema hat. Ein Hinweis dafür kann
in der Precision-Recall Kurve in @fig:IIvsIIIprc abgelesen werden. Zwar erreicht
der MDySimII einen Recall von über 80 %, doch die Precision ist nahezu 0 %. Im
Gegensatz dazu errreicht der MDySimIII knapp 60 % Recall, dafür ist die
Precision nahe 100 %. Daraus folgt, dass der MDySimII im Durchschnitt deutlich
größere Blöck erzeugt als der MDySimIII. Dementsprechend wächst auch der
Similarity Index, da dieser die Ähnlichkeiten aller Attribute eines Blockes
untereinander verknüpft. Die schlechte Precision des MDySimII macht sich direkt
in den Anfragezeiten in @fig:IIvsIIIqry bemerkbar. Die durchschnittliche
Anfragezeit für den MDySimII liegt bei 10^-2^ und ist damit um 10^-2^ dramatisch
schlechter als die des MDySimIII mit 10^-4^. Der Datensatz ist mit 10.000
Einträgen relativ klein, dennoch macht sich bereits hier ein deutlicher
Unterschied bemerkbar, sowohl in der Qualität als auch der Effizienz. Aufgrund
der schlechten Precision skaliert der MDySimII nicht und erfüllt daher bei
großen Datensätzen nicht die Anforderungen an die niedrigen Latenzen. Deswegen
wird in der weiteren Evaluation der MDySimIII als Indexer genutzt.

## Auswahl der Freien Parameter {#sec:free_params}

Auf der Validierungsmenge wurden robuste, freie Parameter für die Evaluierung
gewählt. Robust bedeutet, dass diese nicht optimal für jeden Datensatz sind,
sondern gute Ergebnisse für alle Datensätze liefern und gleichzeitig verhindern,
dass die Entity Resolution katastrophal versagt. Um die freien Parameter zu
bestimmen, wurden diese anhand des NCVoter Datensatzes ausprobiert und
ausgewertet. Für diesen Datensatz wurden dazu die folgenden acht Attribute
genutzt: Geburtsdatum, Vorname, Nachname, Bundesstaat, Ort, PLZ, Straße und
Telefonnummer. Zunächst werden die Parameter des Blocking Schema Lerners
bestimmt, da das Blocking Schema zur Bewertung des Label Generators und des
Fusion-Lerners benötigt wird.

### Blocking Schema Lerner

Für den Blocking Schema Lerner aus @sec:ana_bs müssen folgende freie Parameter
bestimmt werden:

* Blockschlüsselgenerator
* Blockingprädikate
* Maximale Konjunktion und Disjunktion
* Blockfilter (Größe/Ratio)

#### Blockschlüsselgenerator

Von den drei unterschiedlichen Möglichkeiten zur Erzeugung zusammengesetzer
Blockschlüssel aus @sec:bkv_gen wurde im Rahmen dieser Thesis nur der
vorgestellte Algorithmus \ref{alg:bkvs} implementiert. Weshalb auch dieser für
die Evaluation genutzt wird.

#### Geeignete Prädikate

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_memusg.pdf}
        \caption{Maximaler Arbeitsspeicherverbrauch der unterschiedlichen
                 Prädikatspaare in der Query-Phase.}
        \label{fig:pred_mem}
    \end{minipage}\hfill
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_index_ips.pdf}
        \caption{Einfügeoperation pro Sekunde für die unterschiedlichen
                 Prädikatspaare in der Build-Phase.}
        \label{fig:pred_ips}
    \end{minipage}\\
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_query_ips.pdf}
        \caption{Anfragen pro Sekunde für die unterschiedlichen Prädikatspaare
                 in der Query-Phase.} \label{fig:pred_qps}
    \end{minipage}
\end{figure}
```

Die Blockingprädikate sind Hauptbestandteil eines Blocking Schema und haben
deswegen den größten Einfluss auf die Qualität und Effizienz des Gesamtsystems.
Um geeignete Prädikate auszuwählen, wurden folgende 6 Prädikate betrachtet.

* Identität eines Attributes (ID)
* Token eines Attributes, welche durch Leerzeichen getrennt sind (Tok)
* Prefixe eines Attributes der Längen 2-4 (Pre)
* Suffixe eines Attributes der Längen 2-4 (Suf)
* Bigram eines Attributes sind n-Gramme der Länge 2 (Bi)
* Trigram eines Attributes sind n-Gramme der Länge 3 (Tri)

Aus Performanzgründen wurden immer nur zwei Prädikate zusammen getestet. Jeder
Durchlauf wurde folglich mit 16 spezifischen Blockingprädikaten durchgeführt,
eines pro Attribut pro Prädikat. Da die Blockschlüsselerzeugung über ID
offensichtlich am effizientesten ist, wurden alle Kombinationen der anderen
Prädikate zusammen mit ID getestet. Zusätzlich wurden (Prefix, Sufix) und
(Bigram, Trigram) getestet. Da einige dieser Kombinationen einen deutlichen
Einfluss auf die Effizienz haben, wurde aus Zeitgründen der Febrl-4k-1k
Datensatz genutzt.

In @fig:pred_mem ist der Arbeitsspeicherverbrauch des gebauten Indexes
dargestellt. Alle generierten Blocking Schemata bestehen aus zwei zweistelligen
Ausdrücken. Das speicherhungriste Blocking Schema wurde durch (ID, Trigram)
erzeugt und besteht ausschließlich aus spezifischen Blockingprädikaten der
Trigramme, alle anderen Blocking Schemata nutzen beide Prädikate. Das
speichersparenste Schema wird durch (ID, Token) erzeugt. In den
[@fig:pred_ips;@fig:pred_qps] sind die Einfügeoperationen und Anfragen pro
Sekunde dargestellt. Dabei zeigt sich, dass (ID, Token) mit deutlichem Abstand
in beiden Abbildungen dominiert. Dies ist zum einen auf die geringerer Anzahl
der Blockschlüssel zurückzuführen, die pro Attribute erzeugt werden und zum
anderen auf die deutlich bessere Pairs Quality (PQ), welche in @tbl:pred_qual zu
sehen ist. Hierbei erreicht (ID, Token) einen Wert von 0.96, das nächstbeste
Paar (Bi-Tri) kommt lediglich auf 0.33. Die hohe Pairs Quality kommt allerdings
auf Kosten der Pairs Completeness (PC), welche von allen Kombinationen mit 0.83
die schlechteste ist. Andere Paare erreichen hier bis zu 0.99, beispielsweise
für (Prefix, Suffix). Die Prädikate ID und Token sind zwar qualitativ leicht
unterlegen, zeigen jedoch bei der Effizienz eine deutliche Überlegenheit. Alle
anderen Prädikate überzeugen zwar qualitätiv, skalieren jedoch nicht und können
die Anforderungen an die niedrigen Latenzen dadurch nicht erfüllen. Für die
weitere Evaluation werden deshalb nur noch die Prädikate ID und Token benutzt.

```tex
\begin{table}
\centering
\begin{tabular}{lrrrrrrr}\toprule
   & Bi-Tri & ID-Bi & ID-Pre & ID-Suf & ID-Tok & ID-Tri & Pre-Suf \\ \midrule
PC &   0.97 &  0.90 &   0.99 &   0.97 &   0.83 &   0.95 &    0.99 \\
PQ &   0.33 &  0.09 &   0.33 &   0.09 &   0.96 &   0.28 &    0.18 \\ \bottomrule
\end{tabular}
\caption{Pairs Completeness und Pairs Quality der Prädikatkombinationen}
\label{tbl:pred_qual}
\end{table}
```

#### Maximale Konjunktion/Disjunktion

![Bauzeiten des Indexers bei unterschiedlicher maximaler Länge der Konjunktion
der Ausdrücke des Blocking Schema.
](./images/dnf/MDySimIII_index_bt.pdf){#fig:dnf_bt width=50%}

Die maximale Konjunktion $max_k$ von spezifischen Blockingprädikaten von
Ausdrücken und die maximale Disjunktion $max_d$ von Ausdrücken können
entscheidend sein, um ein gutes Blocking Schema zu bilden. Ein konjunktiver
Ausdruck wird bewertet, indem die Blöcke durch Indexer gebaut werden, was
relativ zeitintensiv ist. Auf der anderen Seite werden für die Disjunktion der
Ausdrücke nur boolsche Vektoren verodert und miteinander verglichen, was
deutlich effizienter ist. Im Allgemeinen gilt, je weniger Konjunktionen erlaubt
sind, desto höher ist die Wahrscheinlichkeit, dass ein Ausdruck Non-Matches
nicht korrekt ausschließen kann und je weniger Disjunktionen, desto höher ist
die Wahrscheinlichkeit, dass nicht alle Matches erfasst werden. In
[@KM:Unsupervised:13] geben Kejriwal & Miranker an, in der Evaluation ihres DNF
Blocking Schema Lerners keine Verbesserung für $max_k > 2$ gemessen zu haben.
Im Gegensatz dazu wird $max_d$ von ihnen nicht beschränkt, zudem wird über die
maximal gemessenen Disjunktion keinerlei Aussage gemacht. Für die Evaluation des
Algorithmus aus @sec:ana_bs wurde $max_d$ für alle Durchläufe auf 5 gesetzt, da
das Verfahren hierfür effizient genug ist und es keinen anderen Grund gibt diese
zu restriktieren. Für $max_k$ wurden die Werte 1, 2 und 3 getestet. Bei den
ausgwählten 8 Attributen des NCVoter Datensatzes mit den zwei spezifischen
Blockingprädikaten beträgt die Anzahl der gebildeten Ausdrücke für $max_k = 1$
gleich 16, für $max_k = 2$ gleich 136, das sind 8.5 Mal mehr Ausdrücke und für
$max_k = 3$ gleich 696 Ausdrücke, was nochmals 5 Mal soviele sind. Dabei ist zu
beachten, dass $max_k$ jeweils die Ausdrücke von $max_k - 1$ beinhaltet. Die
Anzahl der zu prüfenden Ausdrücke, hat deutlichen Einfluss auf die gesamte
Lernzeit. Bei $max_k = 1$ dauert das Lernen nur 22 Minuten, bei $max_k = 2$
dauert es 5.5 Mal solange mit 2 Stunden und bei $max_k = 3$ dauert es nochmal
12.5 Mal solange, mit einem vollen Tag und einer Stunde. Während von $max_k = 2$
auf $max_k = 3$ die Anzahl der Ausdrücke um das fünffache wächst, ist dies bei
der Lernzeit über das 12fache. Dies lässt sich mit den Bauzeiten des Indexers
erklären. In @fig:dnf_bt sind die Bauzeiten des jeweils besten Blocking Schema
im jeweiligen Durchlauf dargestellt. Während der Bauzeit wurden ca. 4 Mio.
Datensätze eingefügt. Dabei besteht das beste Blocking Schema in allen drei
Fällen stets aus Ausdrücken der Länge $max_k$. Die Blocking Schemata für $max_k
= 1$ und $max_k = 2$ haben jeweils zwei Ausdrücke und das für $max_k = 3$
besteht aus drei Ausdrücken. Die Bauzeiten verraten, dass je länger die
Ausdrücke werden, desto länger benötigt der Indexer zum Erzeugen der
Blockschlüssel, wodurch sich die Bauzeit verlängert. Da die Blockschlüssel auch
für jede Anfrage erzeugt werden, verlängern sich diese ebenfalls. Während bei
$max_k = 1$ noch 50k Anfragen/s beantwortet werden, sind es bei $max_k = 2$ nur
noch 22k Anfragen/s und bei $max_k = 3$ lediglich noch 13k Anfragen/s. Neben der
Effizienz muss allerdings auch die Qualität überzeugen, was bei $max_k = 1$
nicht der Fall ist. Die Pairs Completeness beträgt lediglich 0.16 und die Pairs
Quality 0.1. Für $max_k = 2$ liegt die Pairs Completeness bei guten 0.95, auch
die Pairs Quality ist mit 0.13 leicht besser. Für $max_k = 3$ kann diese sich
noch auf 0.98 verbessern, die Pairs Quality bleibt mit 0.13 jedoch gleich.
Sowohl $max_k = 2$ als auch $max_k = 3$ bieten einen guten Recall, da $max_k =
2$ jedoch deutlich effizienter ist, wird dieser Wert für die Evaluation genutzt.
Die maximale Disjunktion lag bei drei in Verbindung mit $max_k = 3$, weshalb für
$max_d$ der Wert 3 für die Evaluation ausreichend erscheint.

#### Blockfilter

![Precision-Recall Kurve des zweiten guten Blocking Schema bei sinkendem $t$.
](./images/fp_dnf/MDySimIII_ncvoter_block_filters_prc.pdf){#fig:tvsg_prc
width=50%}

In @sec:eval_dnflearner wurden zwei Filter für den Blocking Schema
Lerner eingeführt, die dafür sorgen, dass offensichtlich schlechte Ausdrücke und
schlechte Blockschlüssel nicht im Detail betrachtet werden, wenn diese zur
Überlastung der Arbeitsspeicherkapazitäten führen können. Der erste Filter ist
die Schwelle $t$, ab welcher ein Block mit mehr Einträgen als schlechter Block
gilt. Der zweite Filter ist die minimale gute Blockrate, ist die Anzahl der
Einträge in guten Blöcken prozentual kleiner als $g$ wird der komplette Ausdruck
verworfen. Erfüllt ein Ausdruck die gute Blockrate, aber erzeugt Blockschlüssel
größer $t$, dann werden diese verboten und nur die Blöcke kleiner $t$ benutzt.
Bei den meisten schlechten Blockschlüsseln handelt es sich um Stoppwörter, die
in der Vorverarbeitung nicht korrekt aussortiert wurden. In der Evaluation wurde
$t$ für die Werte 25, 50, 100, 200, 500 und 1000 jeweils auf $g$ 0.75, 0.8,
0.85, 0.9, 0.95 und 1 angewandt. @tbl:tvsg_pr zeigt Recall und Presion für alle
Kombinationen von $g$ und $t$. Für $t$ gleich 500 und 1000 konnten keine
Ergebnisse ausgewertet werden, da diese zu Abbrüchen aufgrund zu hoher
Speicheranforderungen geführt haben. Weitere Abbrüche gab es für $t$ gleich 200
in Verbindung mit $g$ gleich 0.75 und 0.85. Zudem ist eine minimale gute
Blockrate von 1.0 offensichtlich ungeeignet, weil zu viele gute Ausdrücke
dadurch ausgeschlossen werden. Die übrigen Paarungen beschränken sich auf zwei
Blocking Schemata. Eines mit schlechter Pairs Completeness und mittelmäßiger
Pairs Quality und eines mit guter Pairs Completeness und schlechter Pairs
Quality. Über einen Klassifikator kann eine schlechte Pairs Quality zu einer
guten Precision verbessert werden, der Recall hingegen ist jedoch durch die
Pairs Completeness beschränkt (Recall $\leq$ Pairs Completeness). Deswegen ist
das zweite Blocking Schema mit höherer Pairs Completeness zu bevorzugen. In
@fig:tvsg_prc ist die Precision-Recall Kurve (hier: Precision=Pairs Quality,
Recall=Pairs Completeness) für das zweite Blocking Schema mit steigender
maximaler Blockgröße abgebildet. Je größer $t$, desto größer ist entsprechend
auch der Recall, da weniger Blockschlüssel verboten werden. Allerdings fällt
gleichzeitig die Precision. Eine robuste maximale Blockgröße, die für alle
gewählten $g$ funktioniert wird mit  100 ausgewählt. Für die minimale gute
Blockrate wird 0.85 gewählt, da dort jeweils nach unten und oben noch Puffer
ist, in welchem das präferierte Blocking Schema ebenfalls ausgewählt wurde. Für
deutlich größere Datensätze als den NCVoter wird die Blockgröße von 100
vermutlich zu Verschlechterungen der Effizienz führen. Da allerdings in der
Evaluation kein größerer Datensatz genutzt wird, ist die gewählte Blockgröße
vertretbar.

```tex
\begin{table}
\centering
\begin{tabular}{lrrrrrr}\toprule
t \verb|\| g & 0.75 &   0.80 & 0.85      & 0.90      & 0.95      & 1.0       \\ \midrule
25   & 0.87/0.27 & 0.16/0.47 & 0.16/0.47 & 0.16/0.47 & 0.16/0.47 & 0.06/0.57 \\
50   & 0.92/0.18 & 0.92/0.18 & 0.92/0.18 & 0.16/0.44 & 0.16/0.44 & 0.06/0.57 \\
100  & 0.95/0.13 & 0.95/0.13 & 0.95/0.13 & 0.95/0.13 & 0.16/0.41 & 0.14/0.25 \\
200  &           &           & 0.98/0.09 & 0.98/0.09 & 0.98/0.09 & 0.14/0.25 \\
500  &           &           &           &           &           &           \\
1000 &           &           &           &           &           &           \\ \bottomrule
\end{tabular}
\caption{Tabelle mit Recall=Pairs Completeness und Precision=Pairs Quality für
unterschiedliche maximale Blockgrößen und minimale gute Blockrate.}
\label{tbl:tvsg_pr}
\end{table}
```

### Label Generator

Die freien Parameter des Labelgenerators sind die Fenstergröße, die untere und
obere Schwelle, sowie die maximalen Matches und Non-Matches. Dabei werden die
Schwellen nur benötigt, falls keine Ground Truth existiert und der Label
Generator diese selbstständig erzeugt. Die Auswertung der eigenständig
generierten Ground Truth erfolgt durch die bekannten Matches und wird in
Relation zu einem Kontrollexperitment, das mit Ground Truth Matches
durchgeführt wurde, gesetzt.

#### Fenstergröße {#sec:lblwin}

```tex
\begin{table}
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=2) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 8k     & 0.95 & 0.11 \\
0.2      & 551k & 655k   & 0.95 & 0.11 \\
0.3      & 551k & 1,377k & 0.95 & 0.11 \\
0.4      & 433k & 1,377k & 0.15 & 0.10 \\
0.5      & 433k & 1,377k & 0.16 & 0.10 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=5) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 32k    & 0.15 & 0.01 \\
0.2      & 551k & 1,377k & 0.15 & 0.01 \\
0.3      & 551k & 1,377k & 0.15 & 0.01 \\
0.4      & 551k & 1,377k & 0.16 & 0.11 \\
0.5      & 551k & 1,377k & 0.16 & 0.11 \\ \bottomrule
\end{tabular}
\end{minipage}\\\vskip 2.5em
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=10) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 66k    & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.16 & 0.13 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=20) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 119k   & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.16 & 0.12 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\caption{Auswertung der Konfiguration mit selbständig synthetisierter Ground
Truth für verschiedene Fenstergrößen. Die im Kontrollexperiment ermittelte Pair
Completeness beträgt im Vergleich 0.95 und Pairs Quality 0.13.}
\label{tbl:windows}
\end{table}
```

Zur Bestimmung einer geeigneten Fenstergröße wurde der Label Generator ohne
Matches betrachtet (vgl. @sec:ana_lbl). Diese Variante reagiert im Vergleich
zur Variante mit Matches deutlich empfindlicher auf die unterschiedlichen
Parameter, wodurch der Effekt der unterschiedlichen Fenstergrößen $w$ einfacher
ausgewertet werden kann. Dazu wurde die untere Schwelle $lt$ und die obere
Schwelle $ut$ in 0.1 Schritten bis 0.5 erhöht und jeweils mit den Fenstergrößen
2, 5, 10 und 20 ausprobiert. Die maximalen Matches wurden mit 10 % der
Gesamtmenge und die maximalen Non-Matches mit 25 % der Gesamtmenge bestimmt. Bei
dem genutzten NCVoter Datensatz sind somit die maximalen Matches bei 551k und
die maximalen Non-Matches bei 1.337k. In der Tabelle \ref{tbl:windows} sind die
Ergebnisse für die unterschiedlichen Fenstergrößen dargestellt. Die obere
Schwelle $ut$ hatte dabei keine entscheidende Auswirkung, sodass lediglich die
untere Schwelle $lt$ betrachtet wird. Für jede Schwelle wurden Matches (P),
Non-Matches (N), Pairs Completeness (PC) und Pairs Quality (PQ) analysiert. Die
Pairs Quality liefert keinen entscheidenden Hinweis auf ein geeignetes Fenster,
da diese sich lediglich zwischen 1 % und 11 % hin und her bewegt. Dies bedeutet
zwar einen Performanzunterschied, welcher jedoch nicht ausschlaggebend
signifikant ist. Beim Blick auf die Pairs Completeness zeigt sich, dass diese
sich zwischen 13 % und 16 % für alle Fenstergrößen bewegt, mit Außnahme von
$w=2$. Dort ist die Pairs Completeness für $lt \leq 0.3$ mit 95 % deutlich
besser. Das Kontrollexperiment mit $w=2$ erreicht ebenfalls eine Pairs
Completeness von 95 % und die Pairs Quality ist mit 13 % lediglich 2 % besser.
Für $w=2$ werden am wenigsten Paare gebildet, allerdings sind die Paare die
gebildet werden, die mit der höchsten Ähnlichkeit zueinander, da nur
Blocknachbarn betrachtet werden. Aufgrund dessen werden viele deutlich
verschiedene Paare ausgeschlossen, wie sich bei $lt=0.1$ bemerkbar macht, da
hier lediglich 8k Non-Matches generiert wurden. Bei $lt=0.2$ gibt es mit 655k
dann allerdings schon eine große Auswahl an Non-Matches und mit $lt=0.3$ wurden
bereits mehr Paare generiert als das Maximum. Aufgrund des guten Abschneidens
gegenüber dem Kontrollexperiment und keiner wirklichen Konkurrenz durch andere
Fenstergrößen wird diese für die weitere Evaluation mit 2 bestimmt.

#### Untere und obere Schwelle

![Ähnlichkeitsverteilung der TF/IDF Ähnlichkeiten der Ground Truth des NCVoter
Datensatzes, welche anhand der tatsächlichen Matches erzeugt wurde. Die
Datensätze Matches bzw. Non-Matches wurden in 5 % Schritten nach Ähnlichkeit
zusammengefasst.](./images/ncvoter_matches_histo.pdf){#fig:match_histo}

Die untere Schwelle $lt$ legt fest, bis zu welchem Ähnlichkeitswert Paare als
Non-Matches betrachtet werden und die obere Schwelle $ut$ legt fest, ab welchem
Ähnlichkeitswert Paare als Matches betrachtet werden, wobei stets gilt $lt \leq
ut$. In einem Experiment wurden $lt$ und $ut$ in 0.1 Schritten betrachtet und so
alle Konfigurationen bis 1.0 auf dem NCVoter-Datensatz getestet. Für das
Fenster wurde der bereits bestimmte Wert von 2 gesetzt. Zur Auswertung wurden
Pairs Completeness, Pairs Quality und die Ground Truth analysiert. Anhand der
Pairs Completeness und Pairs Quality kann betrachtet werden, wie gut ein
Blocking Verfahren auf der generierten Ground Truth funktioniert. Durch die
gefilterte Ground Truth hingegen kann herausgefunden werden, wie viele Ground
Truth Paare für den Fusion-Lerner zur Verfügung stehen. Die [@tbl:recall;
@tbl:fp; @tbl:fn] betrachten nacheinander die Pairs Completeness, die Matches
und die Non-Matches. Die Pairs Quality ist uninteressant, da deren Werte relativ
konstant bei 0.1 liegen, mit einer Varianz von 0.03. In @tbl:recall ist gut zu
sehen, dass die Pairs Quality zwischen einer $ut$ von 0.1 und 0.4 immer eine
gute Pairs Quality von 95 % erzeugt. Der Blick auf das erlernte Blocking Schema
zeigt, dass dieses immer dasselbe und gleich zu dem aus @sec:lblwin ist. Dies
trifft auch noch teilweise für $ut=0.5$ zu, allerdings nur für $lt \leq 0.3$.
Für alle $ut > 0.5$ variieren die Blocking Schemata, wobei unabhängig von $lt$
keines über 17 % Pairs Completeness kommt. In @fig:match_histo ist die
Ähnlichkeitsverteilung der Ground Truth, des Kontrollexperiments dargestellt,
die aus den tatsächlichen Matches erzeugt wurde. Der Großteil der Matches hat
eine Ähnlichkeit zwischen 0.2 und 0.5, wohingegen der Großteil der Non-Matches
sich überlappend zwischen 0.1 und 0.3 befindet. Für $ut > 0.5$ fällt der Recall
dramatisch ab, weil ein Großteil der Matches nicht mehr erfasst wird und für $ut
= 0.5$ sind die Recallwerte für $lt \leq 0.3$ noch gut. Wird $lt$ allerdings
weiter erhöht fällt der Recall, da sich nun zu viele tatsächliche Matches in den
Non-Matches befinden.

```tex
\begin{table}
\centering
\begin{tabular}{rrrrrrrrrrr}\toprule
PC  & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & 0.8  & 0.9  & 1.0  \\ \midrule
0.1 & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.2 &      & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.3 &      &      & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.16 & 0.13 & 0.13 \\
0.4 &      &      &      & 0.95 & 0.15 & 0.16 & 0.16 & 0.16 & 0.14 & 0.14 \\
0.5 &      &      &      &      & 0.16 & 0.16 & 0.13 & 0.14 & 0.14 & 0.06 \\
0.6 &      &      &      &      &      & 0.13 & 0.14 & 0.14 & 0.16 & 0.06 \\
0.7 &      &      &      &      &      &      & 0.14 & 0.10 & 0.16 & 0.06 \\
0.8 &      &      &      &      &      &      &      & 0.10 & 0.16 & 0.06 \\
0.9 &      &      &      &      &      &      &      &      & 0.14 & 0.06 \\
1.0 &      &      &      &      &      &      &      &      &      & 0.06 \\ \bottomrule
\end{tabular}
\caption{Pairs Completeness bei verschiedenen Schwellen}\label{tbl:recall}
\begin{tabular}{rrrrrrr}\toprule
Matches & 0.1       & 0.2       & 0.3       & 0.4       & 0.5   \\ \midrule
0.1 & 551k/300k & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.2 &           & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.3 &           &           & 551k/300k & 551k/300k & 443k/288k \\
0.4 &           &           &           & 551k/300k & 443k/287k \\
0.5 &           &           &           &           & 443k/273k \\ \bottomrule
\end{tabular}
\caption{Anzahl der Matches und gefilterten Matches bei verschiedenen Schwellen}\label{tbl:fp}
\begin{tabular}{rrrrrrr}\toprule
Non-Matches & 0.1     & 0.2        & 0.3         & 0.4         & 0.5        \\ \midrule
0.1         & 8k/0    & 8k/0       & 8k/0        & 8k/0        & 8k/0       \\
0.2         &         & 655k/66    & 655k/66     & 655k/66     & 655k/66    \\
0.3         &         &            & 1377k/2430  & 1377k/2430  & 1377k/2430 \\
0.4         &         &            &             & 1377k/13916 & 1377k/11k  \\
0.5         &         &            &             &             & 1377k/8k   \\ \bottomrule
\end{tabular}
\caption{Anzahl der Non-Matches und gefilterten Non-Matches bei verschiedenen
         Schwellen}\label{tbl:fn}
\end{table}
```

Deshalb werden in den [@tbl:fp; @tbl:fn] lediglich $ut$-Werte kleiner 0.6
betrachtet. Die maximalen Matches, die der Label Generator erzeugen darf, liegen
bei 10 % der Gesamtmenge und betragen 551k. Bei den Non-Matches ist das Limit 25
% und damit 1,3 mio. Für die künstliche Anreicherung der gefilterten Non-Matches
stehen jedoch alle erzeugten Non-Matches zur Verfügung, dementsprechend je höher
$lt$ desto mehr Non-Matches und umgekehrt für $ut$. In @tbl:fp ist zu sehen,
dass für $ut \leq 0.4$ die Ausgangsmenge der Matches auf das Maximum beschränkt
wurde. Da jeweils die Matches mit der höchsten Ähnlichkeit genutzt werden, sind
diese Mengen identisch, weshalb auch die gefilterten Mengen mit jeweils 300k
Datensätzen aufgrund desselben Blocking Schema identisch sind. Für $ut=0.5$ ist
die Ausgangsmenge kleiner als das Maximum. Die gefilterte Menge beträgt in
diesem Fall 288k, was mehr als genügend Matches sind um einen Klassifikator zu
trainieren. Zu beachten ist, dass diese Menge 6-Mal soviele Matches beinhaltet,
wie die Menge tatsächlichen Matches.

In @tbl:fn wird die Anzahl der Non-Matches dargestellt. Für $lt=0.1$ sind
insgesamt nur 8k Paare erzeugt worden, weil das TF/IDF Blocking die meisten der
sehr unähnlichen Paare ausschließt. Nach dem Filtern durch das Blocking Schema
sind keine Non-Matches mehr vorhanden, da das Blocking Schema verhindert, dass
diese offensichtlichen Non-Matches zusammen gruppiert werden. Für $lt=0.2$ gibt
es ein ähnliches Bild. Zwar ist die Anzahl der Ausgangsmenge mit 655k deutlich
höher, dennoch werden lediglich 66 Paare gefunden, die einen gemeinsamen
Blockschlüssel haben, was zum Trainieren eines Klassifikators nicht ausreichend
ist. Interessanter wird es erst ab $lt=0.3$. Hier wird das erste Mal das Maximum
der Ausgangsmenge mit 1377k erreicht. Die gefilterten Non-Matches betragen 2430,
was im Vergleich zu den Matches immer noch sehr wenig ist, aber durchaus für das
Klassifikatortraining genügt. Mit $lt=0.4$ erhöht sich diese Anzahl nochmals um
das 5-fache. Ein Blick auf @fig:match_histo zeigt aber, dass sich dadurch der
Großteil der Matches in den Non-Matches befindet. Die Linien für Matches und
Non-Matches schneidet sich im Bereich 0.1 - 0.5 bei 0.28. Unterhalb gehen
viele Non-Matches verloren und oberhalb viele Matches. Deshalb wird sowohl $lt$
als auch $ut$ auf 0.3 festgelegt, da ab hier auch genügend Paare zum Trainieren
eines Klassifikators zur Verfügung stehen.

#### Maximale Paare der Ground Truth

Sowohl mit, also auch ohne Ground Truth Match werden dem Label Generator
mitgeteilt wie viele Matches $max_p$ bzw. Non-Matches $max_n$ in der Ground
Truth enthalten sein dürfen. Im Fall ohne Ground Truth Matches werden jeweils
die $max_p$ Matches, bzw. $max_n$ Non-Matches ausgewählt, welche die höchste
Ähnlichkeit haben. Im Fall mit Ground Truth Matches werden sowohl Matches als
auch Non-Matches nach ihrer Ähnlichkeitsverteilung ausgewählt. Mithilfe dieser
Ground Truth sucht der DNF Blocking Schema Lerner nach dem besten
Blocking Schema. Anschließend wird die Ground Truth anhand des Blocking Schema
gefiltert, sodass diese nur Paare enthält, die einen gemeinsamen Blockschlüssel
haben. In diesem Schritt werden sehr viele Non-Matches herausgefiltert, da das
der primäre Zweck des Blocking Schema ist. Damit für den Fusion-Lerner genügend
Non-Matches zur Verfügung stehen, werden die Non-Matches mit allen generierten
Non-Matches des Label Generators angereichert. Hierbei spielt $max_n$ keine
Rolle. Die Auswirkungen dieser Parameter wurden auf dem NCVoter Datensatz mit
Ground Truth und ohne Ground Truth mit Fenstergröße $w=2$ und Schwellen
$lt=ut=0.3$ evaluiert. Ausgangssituation ist (0.1, 0.25) mit $max_p$ 10 % und
$max_n$ 25 % der Gesamtmenge, welche bereits zur Ermittlung der Fenstergröße und
der Schwellen genutzt wurden. Bei allen getesteten Paarungen ist das Limit der
Non-Matches höher als das der Matches, um das Verhältnis im Datensatz zu
repräsentieren. Getestet wurden zunächst größere Werte mit (0.5, 2.5), (1, 5),
(5, 25), (10, 50). Dabei wuchsen die Matches leicht auf 650k und die Non-Matches
bis stark auf 19 Mio. Auf das Blocking Schema hat die vergrößerte Ground Truth
in keinem Fall einen Einfluss. Die Betrachtung der Matches im größten Fall (10,
50) ergibt, dass diese sich lediglich um ca. 100k verändert hat. Die neu
hinzugekommen Matches wurden durch die Filterung auf ein paar Hundert reduziert.
Das bedeutet, dass die erweiterten Matches die Ausdrücke des Blocking Schema, im
Bezug auf Pairs Completeness, abwerten und zwar je mehr, desto größer die Anzahl
der Matches. Allerdings haben sich die Non-Matches mit 19 mio. dramatisch
erhöht, durch die Filterung bleiben lediglich die bekannten 2k übrig, sodass
die erhöhte Menge sich positiv auf das Reduction Ratio und die Pairs Quality
auswirkt. Damit wird der Negativeffekt auf die Pairs Completeness aufgehoben und
das Blocking Schema bleibt dasselbe. Neben größeren Werten wurden auch kleinere
getestet (0.0001, 0.00025), (0.001, 0.0025), (0.01, 0.025). Für (0.01, 0.25)
werden in etwa so viele Matches ausgewählt wie tatsächlich enthalten sind.
Recall und Precision sind allerdings mit 15 % und 0.1 % sehr schlecht. Dasselbe
Ergebnis zeigt sich bei den noch kleineren Paaren. Daraus folgt, dass das
Ausgangspaar (0.1, 0.25) bereits gut gewählt war und auch robuste Werte liefert.

### Fusion-Lerner {#sec:fp_fusion}

Für den Fusion-Lerner müssen noch vier freie Parameter bestimmt werden:

* Anzahl der k Teilmengen für die Kreuzvalidierung
* Anzahl der maximalen Paare beim Subsampling und deren Verhältnis zu Matches
  und Non-Matches
* Qualitätsmaß zur Bewertung der trainierten Modelle

Bei der Kreuzvalidierung durch das Stratified K-Fold Verfahren wird die Anzahl
für $K$ mit 3 bestimmt, da mit anderen Werten kein nennenswerter Unterschied
gemessen wurde und dieser der Standardwert in Scikit-learn ist.

Während der Entwicklung hat sich eine Grid Search auf 5.000 Paaren als genügend
effizient herausgestellt, dabei wurde eine Verhältnis von einem Match auf drei
Non-Matches benutzt. Im Rahmen dieser Arbeit war es zeitlich nicht mehr möglich
andere Werte zu evaluieren, weshalb diese für die Evaluation übernommen wurden.
Das Sampling der Paare wird dabei anhand der Ähnlichkeitsverteilung
durchgeführt.

Ein Qualitätsmaß muss sowohl Recall als auch Precision berücksichtigen, deshalb
wurden hierzu das F-measure und die Average Precision betrachtet. In mehreren
Durchläufen wurden die beiden Maße jeweils für einen Klassifikator genutzt.
Dabei wurde zunächst festgestellt, dass Recall und Precision immer gleich sind,
unabhängig davon welcher Klassifikator und welche Parameter durch die Grid
Search bestimmt wurden. Die Ergebnisse wurden gegen Baseline verglichen, bei
welcher kein Klassifikator genutzt wurde und die Kandidatenmenge $C$ der
Ergebnismenge $R$ entspricht. Hierbei wurde festgestellt, dass mit dem
Klassifikator die Precision dieselbe und der Recall um 13 % niedriger ist. Zweck
des Klassifikators ist es jedoch, möglichst ohne Verlust des Recalls die
Precision so nah wie möglich an 1 zu bringen. Aufgrund dieses Ergebnisses kann
keine endgültige Entscheidung über das Qualitätsmaß getroffen werden. Das
Problem wird deshalb in @sec:base_par_full genau analysiert. Für diese
Untersuchung wird jedoch ein Qualitätsmaß benötigt, weshalb zunächst das
F-measure ausgewählt wird, welches bereits für das Blocking Verfahren gute
Dienste leistet.

## Einfluss der Ähnlichkeitsvektoren {#sec:base_par_full}

![Precision-Recall Kurven für Varvektor (var), Teilvektor (par) und Vollvektor
(full) Ähnlichkeiten, gruppiert nach Klassifikator (clf): Decision Tree (dt),
SVM mit RBF-Kernel (svmrbf) und SVM mit Linearkernel (svmlinear).
](./images/basevsparvsfull/var_par_full_prc.pdf){#fig:baseparfull_prc}

Der Grund für das Versagen der Klassifikatoren bei der Auswahl der freien
Parameter des Klassifikators liegt vermutlich an der reduzierten Menge von
Ähnlichkeitswerten, die der MDySimIII für Kandidatenpaare zurückgibt, da nur
Ähnlichkeiten für Attribute angegeben werden, die einen gemeinsamen Block haben.
Beim Blick auf die Kandidatenmenge wurde festgestellt, dass dies für fast alle
Paare lediglich ein Attribut ist. Damit der Ähnlichkeitsvektor in mehr Stellen
besetzt ist wurde der MDySimIII in zwei Varianten modifiziert. Die erste
Variante berechnet die fehlenden Ähnlichkeitswerte der Attribute, die Teil des
Blocking Schema sind. Die zweite Variante berechnet alle fehlenden
Ähnlichkeitswerte, sodass der Vektor anschließend vollbesetzt ist. Das
ursprüngliche Verfahren mit variable besetzem Vektor wird im Folgenden als
Varvektor (par) bezeichnet, die erste neue Variante als Teilvektor (par) und die
zweite neue Variante als Vollvektor (full). Zum Vergleich wurde jeweils ein
Kontrollexperiment (KE) ohne Klassifikator, aber mit entsprechender
Ähnlichkeitsberechnung durchgeführt. Die Ähnlichkeitsberechnung beeinflusst
hierbei lediglich die Laufzeit, da ohne Klassifikator die Ähnlichkeiten nicht
weiter verwendet werden und die Kandidatenmenge $C$ der Ergebnismenge $R$
entspricht. Die folgenden sechs Konfigurationen werden hierzu evaluiert:

* Var-KE, MDySimIII ohne Klassifikator mit vorausberechneten Ähnlichkeiten
* Teil-KE, MDySimIII ohne Klassifikator mit Blocking Schema
  Attributsähnlichkeitsberechnung
* Voll-KE, MDySimIII ohne Klassifikator mit voller Ähnlichkeitsberechnung
* Varvektor, MDySimIII mit Klassifikator mit vorausberechneten Ähnlichkeiten
* Teilvektor, MDySimIII mit Klassifikator mit Blocking Schema
  Attributsähnlichkeitsberechnung
* Vollvektor, MDySimIII mit Klassifikator mit voller Ähnlichkeitsberechnung

Alle sechs Konfigurationen nutzen dazu dieselbe Ground Truth mit
vorklassifizierten Matches, dasselbe Blocking Schema, sowie die gleichen vom
Similarity Lerner bestimmten Ähnlichkeitsmetriken. Des Weiteren wurden
Varvektor, Teilvektor und Vollvektor jeweils mit Decision Tree (dt), SVM mit
RBF-Kernel (svmrbf) und SVM mit Linearkernel (svmlinear) getestet, um die
Ergebnisse sinnvoll miteinander vergleichen zu können.

![Anfragen pro Sekunde für Varvektoren (var), Teilvektoren (par) und
Vollvektoren (full) bei Klassifikation durch verschiedene Klassfifkatoren.
](./images/basevsparvsfull/MDySimIII_query_ips.pdf){#fig:baseparfull_qps
width=100%}

@fig:baseparfull_prc gruppiert die Precision-Recall Kurven von Varvektor,
Teilvektor und Vollvektor nach Klassifikator. In jeder Kurve ist durch einen
Punkt das Recall-Precision Paar hervorgehoben, das anhand der im entsprechenden
Klassifikator benutzen Wahrscheinlichkeitsschwelle erreicht wurde. Das
Recall-Precision Paar der Kontrollexperimente ist jeweils durch einen schwarzen
Punkt am Ende der Kurven gekennzeichnet. Auf den drei Varvektorkurven sind
insgesamt nur vier Punkte zu erkennen. Jeder Punkt entspricht einer
Wahrscheinlichkeitsschwelle, das bedeutet, dass die auf den Ähnlichkeiten
berechneten Wahrscheinlichkeiten der Zugehörigkeit zu den Matches sich auf vier
Wahrscheinlichkeiten beschränkt. Der Grund dafür liegt im erlernten Blocking
Schema: [(`Vorname`, `ID`) $\land$ (`Nachname`, `ID`)] $\lor$ [(`Zweitname`,
`ID`) $\land$ (`Strasse`, `ID`)]. Dieses besteht ausschließlich aus ID
Prädikaten. Folglich werden nur Datensätze mit gleichen Attributen gruppiert,
deren berechnete Ähnlichkeit offensichtlich 1.0 ist. Dementsprechend sind die
Ähnlichkeitsvektoren der Kandidaten für Vorname und Nachname mit 1.0 besetzt
oder für Zweitname und Strasse oder beides. Der vierte Wert ist eine 1.0 nur für
die Strasse. Dieser kommt zustande, wenn beide Datensätze eine Übereinstimmung
in der Strasse haben und keine Zweitnamen besitzen. In diesem Fall besteht der
Blockschlüssel nur aus dem Strassenamen und die Ähnlichkeit bei mindestens einem
leeren Attribut ist per Definition immer 0. Dementsprechend bekommt der
Klassifikator nur ungenügend Informationen um eindeutig die Non-Matches
herauszufiltern, was die schlechten Werte aus @sec:fp_fusion erklärt. Die drei
Kurven der Varvektoren zeigen zudem, dass es nicht möglich ist einen besseren
Wert anzunehmen als der des Kontrollexperimentes. Im Gegensatz dazu bietet
sowohl Teilvektor als auch Vollvektor dem Klassifikator deutlich mehr
Unterscheidungsmerkmale. Dies lässt sich in den Precision-Recall Kurven daran
erkennen, dass deren Kurven mit deutlich mehr Punkten besetzt sind. Überraschend
ist, dass der Teilvektor trotz weniger Ähnlichkeiten für den Decision Tree und
die SVM mit RBF-Kernel besser abschneidet als der Vollvektor und für die SVM
mit Linearkernel eine fast identische Kurve hat. Die beiden SVMs mit Teilvektor
sind dazu dem Decision Tree deutlich überlegen, welcher über 20 % an Recall
verliert und die Precision lediglich um 11 % steigern kann. Bei den SVMs mit
Teilvektor liegt der Recall verlust zwar auch bei über 20 %, allerdings ist die
Precision mit ca. 60 % eine deutliche Verbesserung. Zudem lässt sich in den
Kurven ablesen, dass die Precision des Klassifikators durch Kalibrierung der
Wahrscheinlichkeitsschwelle auf bis zu 80 % bei fast gleichem Recall optimiert
werden kann.

In @fig:baseparfull_qps werden die Konsequenzen auf die Effizienz in Anfragen
pro Sekunde der verschieden Konfiguration dargestellt. Mit deutlichem Abstand
am meisten Anfragen pro Sekunde können durch Var-KE erzielt werden. Bei Par-KE
sind es bereits ca. 70 % weniger und bei Full-KE ganze 80 % weniger Anfragen pro
Sekunde. Aber nicht ausschließlich die Ähnlichkeitsberechnung benötigt viel
Zeit, wie bei den nicht Kontrollexperimenten zu sehen ist, bremst die
Klassifikation ebenfalls erheblich, sogar stärker als die
Ähnlichkeitsberechnung. Der Klassfikationsaufwand ist unabhängig von der Anzahl
der Stellen, an welchen der Ähnkeitsvektor besetzt ist. Die beste Performanz
bieten entsprechend die Varvektoren, welche jedoch eine extrem schlechte
Qualität haben. Die Teilvektoren sind über die Hälfte langsamer als die
Varvektoren, aber ca. 70 % schneller als die Fullvektoren. Des Weiteren sind die
Qualitätswerte mindestens genauso gut, z.T. sogar besser als die der
Fullvektoren.

Beim Auswerten der Daten gab es eine Unstimmigkeit. Und zwar wurde als
Ähnlichkeitsmaß für alle Attribute immer Bag-Distanz gewählt. Insgesamt wurden
in dem Similarity Lerner die vier Metriken Bag-Distanz, Levenshtein-Distanz,
Damerau-Distanz und Jaro-Distanz zur Auswahl übergeben. Ein genauer Blick auf
die vom Similarity Lerner berechnete Average Precision zeigt, dass diese pro
Attribut für jedes Ähnlichkeitsmaß den gleichen Wert hat. Dementsprechend wurde
immer die zuerst getestete Metrik ausgewählt. Dieses Verhalten des Similarity
Lerners wird in @sec:humanvslearn genauer analysiert.

## Einfluss der Ähnlichkeitsmetriken {#sec:esim_metrik}

![Precision-Recall Kurven von 7 Ähnlichkeitsmetriken gruppiert nach
Klassifikator (Zeilen) und Vektortyp (Spalten). Durch einen Punkt ist die
aktuelle Wahrscheinlichkeitsschwelle des Klassifikators hervorgeboben.
](./images/simtest/par_full_prc.pdf){#fig:sim_prc}

Die Auswertung der Ähnlichkeitsmetriken in @sec:base_par_full zeigt, dass der
Similarity Lerner für jedes Attribute für die meisten Metriken denselben Average
Precision Wert berechnet. Daraus folgt, dass entweder die Ähnlichkeitsmetriken
keinen oder nur geringen Einfluss auf das Urteilsvermögen des Klassifikators
haben, oder dass die Average Precision kein geeignetes Maß ist, um eine
Ähnlichkeit auszuwählen. Infolgedessen wird der Einfluss der
Ähnlichkeitsmetriken geprüft, indem pro Durchlauf jeweils eine Metrik für alle
Attribute manuell bestimmt wird. Dabei werden die Durchläufe für jeden der drei
Klassifikatoren wiederholt. In @sec:base_par_full wurde bzgl. Qualität und
Effizienz eine Tendenz zu den Teilvektoren gegenüber den Vollvektoren
festgestellt. Zum Zweck diese Tendenz zu bestärken bzw. zu entkräften wird jeder
der Klassifikatoren einmal mit dem Teilvektor und einmal mit dem Vollvektor
geprüft. Für die Analyse wurde die Damerau-Distanz weggelassen, da sich deren
berechnete Ähnlichkeiten bei genauer Betrachtung zu fast 100 % mit der
Levenshein-Distanz deckt. Zusätzlich wurden ausführlichkeitshalber 4 weitere
Ähnlichkeitsmetriken hinzugenommen:

* Bag-Distanz
* Compression-Distanz
* Hamming-Distanz
* Jaro-Distanz
* Jaro-Winkler-Distanz
* Jaccard-Koeffizent
* Levenshtein-Distanz

![Precision-Recall Kurven der Teilvektoren (par) durch die gelernten
Ähnlichkeitsmaße und Teilvektoren mit jeweils der besten einheitlich
Ähnlichkeit.](./images/simtest/par_learn_prc.pdf){#fig:sim_prc_learn width=80%}

Bei 7 Ähnlichkeitsmetriken, drei 3 Klassifikatoren und 2 Vektortypen wurden
insgesamt 42 Durchläufe ausgeführt und ausgewertet. Des Weiteren wurde für alle
Durchläufe dieselbe Ground Truth mit vorklassifizierten Matches und dasselbe
Blocking Schema verwendet. Das Blocking Schema ist erneut [(`Vorname`, `ID`)
$\land$ (`Nachname`, `ID`)] $\lor$ [(`Zweitname`, `ID`) $\land$ (`Strasse`,
`ID`)]. Die für den Similarity Learner und Klassifikator interessante gefilterte
Ground Truth besteht aus 48.256 Matches, 61.764 Non-Matches.

Das Ergebnis ist in den Precision-Recall Kurven in @fig:sim_prc zu sehen. Dabei
wurden die Kurven für die Ähnlichkeitsmetriken jeweils nach Klassifikator und
Vektortyp gruppiert. In der linken Spalte sind die Vollvektoren, in der rechten
die Teilvektoren und in den Zeilen von oben nach unten der Decision Tree, die
SVM mit Linearkernel und die SVM mit RBF-Kernel. Die Vermutung, dass der
Einfluss der Ähnlichkeitsmetriken nicht vorhanden bzw. gering ist, kann auf
einen Blick wiederlegt werden. Beispielsweise ist im Plot für die SVM mit
Linearkernel und Teilvektor ein deutlicher Unterschied zwischen den Kurven der
Compression-Distanz und der Jaro-Winkler-Distanz zu erkennen, womit erwiesen
ist, dass diese Metriken entscheidenden Einfluss auf das Urteilsvermögen des
Klassifikators haben. Des Weiteren bestätigt der Vergleich der Vollvektorkurven
mit den Teilvektorkurven die Entscheidung zu letzterem, da dieser dem
Klassifikator bessere Werte liefert und somit die Kurven mindestens gleich,
meist aber besser sind. Insgesamt schneidet die SVM mit Linearkernel am besten
ab. Allerdings sind die vom Klassifikator genutzten Wahrscheinlichkeitsschwellen
(hervorgehobener Punkt auf der Kurve) nicht optimal. Beispielsweise können die
Jaro-Distanz und die Jaro-Winkler-Distanz von einer Kalibrierung der Schwelle
massiv profitieren, weil dadurch die Precision bei fast gleichem Recall um ca.
40 % verbessert werden kann.

```tex
\begin{table}
\centering
\begin{tabular}{lrrrr}\toprule
Metrik       & Vorname & Nachname & Zweitname & Strasse \\ \midrule
Bag          & 0.7111  & 0.6982   & 0.7121    & 0.4295  \\
Compression  & 0.4286  & 0.4620   & 0.4174    & 0.3850  \\
Hamming      & 0.7111  & 0.6999   & 0.7121    & 0.4288  \\
Jaccard      & 0.7198  & 0.6995   & 0.7130    & 0.4269  \\
Jaro         & 0.7111  & 0.6989   & 0.7121    & 0.4295  \\
Jaro-Winkler & 0.7112  & 0.7005   & 0.7121    & 0.4289  \\
Levenshtein  & 0.7111  & 0.6989   & 0.7121    & 0.4295  \\ \bottomrule
\end{tabular}
\caption{Tabelle mit Average Precisions pro Attribut für 7 Ähnlichkeitsmetriken.}
\label{tbl:simlearn_avp}
\end{table}
```

Die zweite Vermutung, dass die Average Precision keine geeignete Metrik ist um
Ähnlichkeiten auszuwählen wurde überprüft, indem die Durchläufe mit Teilvektor
und allen 7 Ähnlichkeitsmetriken wiederholt wurden. Die Ground Truth und das
Blocking Schema sind gleich zu den 42 Durchläufen zuvor. Die ausgewählten
Ähnlichkeitsmetriken sind: Vorname $\rightarrow$ Jaccard-Distanz, Nachname
$\rightarrow$ Jaro-Winkler-Distanz, Zweitname $\rightarrow$ Jaccard-Distanz und
Strasse $\rightarrow$ Bag-Distanz. In @tbl:simlearn_avp sind die errechneten
Average Precision Werte pro Attribut aufgelistet. Die Compression-Distanz fällt
hierbei aus dem Rahmen und hat deutlich schlechtere Werte als der Rest. Die
Werte für Bag-Distanz, Jaro-Distanz und Levenshtein-Distanz sind wie zuvor
identisch. Für die Hamming-Distanz, Jaccard-Distanz und Jaro-Winkler-Distanz
gibt es dazu leicht aufweichende Werte, was zur Auswahl unterschiedlicher
Metriken geführt hat. In @fig:sim_prc_learn sind die drei Kurven der gelernten
Ähnlichkeitsmaße mit jeweils der besten Kurve des entsprechenden Klassifikator
aus @fig:sim_prc gegenübergestellt. Aufgrund der Auswahl sind die Kurven diesmal
deutlich besser. Für die SVMs ist die Kurve beinahe so gut wie die beste Kurve
und für den Decision Tree ist die Kurve sogar deutlich besser. Aufgrunddessen
ist die Average Precision ein geeignetes Maß, sofern genügend Metriken zur
Auswahl stehen.

## Human Baseline {#sec:humanvslearn}

In diesem Abschnitt wird das selbstkonfigurierende System mit einer
Ausgangskonfiguration (Baseline) verglichen. Diese Baseline wurde von einem
Kommilitonen erstellt und besteht aus einem Blocking Schema $BS$ und
Ähnlichkeitsfunktionen $S$ für die jeweiligen Attribute. Dabei wurden dem
Kommilitonen zuvor die Aufgabenstellung und Konsequenzen für Qualität und
Effizienz erläutert. Zudem wurden praktische Hinweise für ein gutes Blocking
Schema [@Chr:Data:12, S. 98 ff.] und gute Ähnlichkeitsfunktionen [@Chr:Data:12,
S. 126 f.], z. B. Attribute zu verknüpfen, welche unabhängig voneinander sind,
geteilt. Der Zweck dieses Vergleich ist zu betrachten, ob sich die
Selbstkonfiguration gegenüber einer menschlichen, manuellen Konfiguration
behaupten kann. In beiden Fällen wurden dieselben in diesem Kapitel
ermittelten freien Parameter benutzt. Die Auswahl der Ähnlichkeitsfunktionen
bestand für Mensch und Maschine aus den in @sec:eval_sim vorgestellten sieben
Funktionen. Die Evaluation der Konfigurationen wurde zunächst auf den
Trainingsdaten durchgeführt, wodurch die Robustheit der gewählten freien
Parameter erstmals auf die Probe gestellt wurde. Der Eventstrom in der
Query-Phase wurde einmal durch die Trainingsanfragen und ein zweites mal durch
die Testanfragen erzeugt. Beide beinhalten jeweils 1.3 Mio. Datensätze. Im
folgenden sind die Blocking Schemata des selbstkonfigurierende Systems und der
Baseline dargestellt.

**Baseline:**
$$\begin{aligned}
((\text{Nachname}, \text{ID}) &\land (\text{Strasse}, \text{Token}) \land
(\text{PLZ}, \text{ID}))~&\lor \\
((\text{Vorname}, \text{ID}) &\land (\text{Zweitname}, \text{ID})) &\lor \\
(\text{Telefonnummer}, \text{ID}) &~&~
\end{aligned}$$

**Selbstkonfiguration:**
$$\begin{aligned}
((\text{Vorname}, \text{ID}) &\land (\text{Nachname}, \text{ID})) &\lor \\
((\text{Zweitname}, \text{ID}) &\land (\text{Strasse}, \text{ID})) &~
\end{aligned}$$

```tex
\begin{table}
\centering
\begin{tabular}{lllllll}\toprule
Konfiguration & Vorname & Zweitname    & Nachname & Strasse & PLZ     & Tel.Nr. \\ \midrule
Baseline      & Jaccard & Hamming      & Jaccard  & Hamming & Jaccard & Jaccard \\
Selbstkonf.   & Jaccard & Jaro-Winkler & Jaccard  & Jaro    & -       & -       \\ \bottomrule
\end{tabular}
\caption{Gelernte und bestimmte Ähnlichkeitsmaße für die Baseline und die
Selbstkonfiguration.}\label{tbl:hvsl_sims}
\end{table}
```

Die @tbl:hvsl_sims Tabelle zeigt die pro Attribut ausgewählten
Ähnlichkeitsfunktionen. Für alle vier Durchläufe wurde vom Fusion-Lerner eine
SVM mit RBF-Kernel und Strafparameter $C$ der Fehlerfunktion gleich 1000
trainiert.

![Precision-Recall Kurven der Baseline- und Selbstkonfigurationsdurchläufe, für
Trainings- und Testdaten.
](./images/humanvslearn/humanvslearn_prc.pdf){#fig:hvsl_prc width=80%}

In @fig:hvsl_prc sind die Precision-Recall Kurven der beiden Baselines für
Anfragen auf den Trainingsdaten (train) und Testdaten (test), sowie die beiden
Kurven der Selbstkonfiguration für die Konfiguration auf den Trainingsdaten und
Anfragen auf den Trainingsdaten (train-train), bzw. Testdaten (train-test)
dargestellt. Dabei ist zunächst zu sehen, dass die Test- und Trainingskurven
jeweils sehr nahe beianander sind. Dem kann entnommen werden, dass bei der
Konfiguration keine Überanpassung stattgefunden hat und die Entity Resolution
entsprechend auf beiden annährend gleich gut funktioniert. Die Kurven sind
jeweils durch die Pairs Completeness und Pairs Quality limitiert, d.h. der
Recall kann nicht größer werden als die Pairs Completeness und die Precision
nicht geringer als die Pairs Quality in @tbl:hvsl_metrics. Obwohl die Kurven
der Selbstkonfiguration Recall und Precision besser maximieren können, ist der
Klassifikator mit einer Standardwahrscheinlichkeitsschwelle, in der Precision
dem Baseline Klassifikator deutlich unterlegen, wodurch die Baseline bessere
F-measurewerte erzielt. An den Kurven der Selbstkonfiguration ist zu erkennen,
dass ca. 85 % der Matches mit einer Precision von 65 % bis 70 % Prozent erkannt
werden. Darüberhinaus fällt die Precision in kleinen Recallschritten stark ab,
was darauf schließen lässt, dass der Klassifikator für diese Paare enorme
Schwierigkeiten hat Matches von Non-Matches zu trennen. Die Kurve der Baseline
dagegen ist einfacher. Hier können fast alle Matches mit einer Precision von ca.
60 % erkannt werden. Für die letzen Paar Prozent fällt die Kurve bis auf unter 1
% Precision. Daraus lässt sich ableiten, dass das Blocking Schema nur wenige
herausfordernde Paare selektiert und die Standardwahrscheinlichkeitsschwelle
deshalb fast Recall und Precision maximiert.

```tex
\begin{table}
\centering
\begin{tabular}{lrrrrrr}\toprule
Konfiguration       & PC    & PQ    & Recall & Precision & F-measure & Avg. Precision \\ \midrule
Baseline (train)    & 0.808 & 0.004 & 0.801  & 0.675     & 0.733     & 0.581 \\
Baseline (test)     & 0.808 & 0.004 & 0.789  & 0.671     & 0.729     & 0.600 \\
Selbstkonf. (train) & 0.956 & 0.127 & 0.868  & 0.440     & 0.584     & 0.648 \\
Selbstkonf. (test)  & 0.956 & 0.127 & 0.870  & 0.502     & 0.637     & 0.695 \\ \bottomrule
\end{tabular}
\caption{Vergleich der Qualitätsmetriken für die Baseline und die
         Selbstkonfiguration.}\label{tbl:hvsl_metrics}
\end{table}
```

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/humanvslearn/humanvslearn_mem.pdf}
        \caption{Vergleich des maximal benötigten Arbeitsspeichers zwischen
                 Baseline und Selbstkonfiguration}
        \label{fig:hvsl_mem}
    \end{minipage}\hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/humanvslearn/humanvslearn_ips.pdf}
        \caption{Vergleich der Einfügeoperationen pro Sekunde für Baseline und
                 Selbstkonfiguration}
        \label{fig:hvsl_ips}
    \end{minipage}\\
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/humanvslearn/humanvslearn_qps.pdf}
        \caption{Vergleich der Anfragen pro Sekunde für Baseline und
                 Selbstkonfiguration}
        \label{fig:hvsl_qps}
    \end{minipage}
\end{figure}
```

Bezüglich der Qualität der Ergenisse schneidet die Baseline recht gut ab und
ohne Kalibrierung der Klassfifkatoren ist diese dem selbstkonfigurierendem
System im F-measure über 10 % voraus. Als nächstes wird die Effizienz bzgl.
Speicherverbrauch (@fig:hvsl_mem), Einfügeoperationen pro Sekunde
(@fig:hvsl_ips) und Anfragen pro Sekunde (@fig:hvsl_qps) betrachtet. Die
Einfügeoperationen pro Sekunde sind abhängig von der Länge des Blocking Schema
und der Art der Prädikate. Die Baseline ist fast 3-Mal so langsam, weil diese
zum einen zwei spezifische Blockingprädikate mehr hat, wodurch mehr
Blockschlüssel erzeugt werden und zum anderen ein Prädikat Token erzeugt,
wohingegen die Selbstkonfiguration nur ID Prädikate besitzt. Der
Speicherverbrauch der Baseline ist gegenüber der Selbstkonfiguration über das
Doppelte. Dass die Baseline ein wenig mehr Arbeitsspeicher verbraucht war
absehbar, da durch das längere Blocking Schema mehr Blockschlüssel erzeugt und
damit mehr Blöcke gebaut werden. Der Grund, warum die Baseline mehr als doppelt
soviel Arbeitsspeicher benötigt, liegt in der Pairs Quality verborgen. Diese ist
nämlich um das 31-fache schlechter als die der Selbstkonfiguration. Damit gibt
es nicht nur mehr Blöcke, sondern die Blöcke sind im Durchschnitt auch deutlich
größer. Aufgrunddessen ist auch der Similarity Index deutlich größer, welcher
quadratisch zur Anzahl der Elemente eines Blockes wächst. Größere Blöcke
bedeuten auch mehr Kandidaten bei einer Anfrage, mehr Ähnlichkeitsberechnungen
und mehr Klassifikationen. Deshalb sind die Anfragen pro Sekunde der Baseline
fast 20-Mal weniger. Dieser Wert ist jedoch etwas pessimistisch, da ca. 3 GB
Arbeitsspeicher mehr benötigt wurden als im Testsystem verfügbar sind und
dadurch die Statistik durch Swapping verfälscht wurde. Nichtsdestotrotz
schneidet die Baseline bezüglich der Effizienz ziemlich schlecht ab und eine
Verwendung dieser Konfiguration in einem Event Stream Processing System ist
dadurch fraglich.

Über die beiden vorliegenden Konfigurationen wird im Folgenden die Performanz
des MDySimIII Verfahrens betrachtet. Dazu sind in den [@fig:hvsl_inserts;
@fig:hvsl_queries] die Einfügezeiten in den Index für alle 4.2 Mio. Einträge
des Base Datensatzes und die Anfragezeiten für alle 1.3 Mio. Einträge des
Trainings- bzw. Testdatensatzes zu sehen. Für die Baseline ist dabei zu sehen,
dass die Einfügezeiten mit der Anzahl der Datensätze im Index leicht wachsen.
Der Grund dafür sind die großen Blöcke. Je größer die Blöcke werden, desto mehr
Ähnlichkeitsberechnungen müssen beim Einfügen durchgeführt werden. Dahingegen
bleiben die Einfügezeiten für die Selbstkonfiguration unverändert. Dies ist zum
einen der Fall, weil die Blöcke wesentlich geringer sind, zudem besteht das
Blocking Schema nur aus ID Prädikaten, sodass beim Einfügen keine
Ähnlichkeitsberechnungen stattfinden, weil nur Datensätze mit gleichen
Attributen zusammen gruppiert werden und diese offensichtlich eine Ähnlichkeit
von 1.0 haben. Datensätze mit gleichen Attributen zusammen gruppiert werden und
diese offensichtlich eine Ähnlichkeit von 1.0 haben.

![Vergleich der Einfügezeiten für alle 1.3 Mio. Anfragen zwischen Baseline und
Selbstkonfiguration.
](./images/humanvslearn/humanvslearn_inserts.pdf){#fig:hvsl_inserts}

Die Anfragezeiten für die Baseline wachsen ebenfalls leicht an. Das liegt zum
einen an der wachsenden Einfügezeiten, da ein neuer Datensatz während einer
Anfrage eingefügt wird und zum anderen an der dadurch wachsenden Anzahl von
Kandidaten. Ein weiterer Grund ist, dass für die letzten 300k Anfragen das
Betriebssystem begonnen hat den Arbeitsspeicher zu swappen. Dahingegen bleiben
die Anfragezeiten bei der Selbstkonfiguration erneut gleich, weil die
Einfügezeiten nicht wachsen und aufgrund der kleinen Blöcke die Menge der
Kandidaten auch nicht größer wird.

![Vergleich der Anfragezeiten für alle 1.3 Mio. Anfragen zwischen Baseline und
Selbstkonfiguration.
](./images/humanvslearn/humanvslearn_queries.pdf){#fig:hvsl_queries}

## Grund Truth vs No Ground Truth {#sec:gtvsnogt}

Dieser Abschnitt vergleicht die Selbstkonfiguration mit einer synthetisierten
Ground Truth, welcher die Matches bekannt sind (gt) und einer
Selbstkonfiguration, welcher die Matches nicht bekannt sind (nogt). Der Aufbau
und die Parameter sind dabei identisch zum vorherigen @sec:humanvslearn. Mit
bekannten Matches ist das Blocking Schema identisch zum vorhierigen Abschnitt,
ohne Matches unterscheidet sich das Blocking Schema in einer Position. Statt dem
Ausdruck im zweiten Ausdruck wurde der Zweitname gegen den Bundestaat, bei
gleichem Prädikat, getauscht.

**Selbstkonf. mit Matches:** $$\begin{aligned} ((\text{Vorname}, \text{ID})
&\land (\text{Nachname}, \text{ID})) &\lor \\ ((\text{Zweitname}, \text{ID})
&\land (\text{Strasse}, \text{ID})) &~ \end{aligned}$$

**Selbstkonf. ohne Matches:** $$\begin{aligned} ((\text{Vorname}, \text{ID})
&\land (\text{Nachname}, \text{ID})) &\lor \\ ((\text{Bundesstaat}, \text{ID})
&\land (\text{Strasse}, \text{ID})) &~ \end{aligned}$$

![Precision-Recall Kurven für die Selbstkonfigurationen mit (gt) und ohne (nogt)
bekannte Matches für Trainings- und Testdaten.
](./images/gtvsnogt/gtvsnogt_prc.pdf){#fig:gtvsnogt_prc}

Die sehr ähnlichen Blocking Schemata resultieren in sehr ähnlichen Ergebnissen
für Pairs Completeness, welche mit 95.6 % identisch ist und Pairs Quality, die
ohne Matches mit 10.8 % lediglich 2 % schlechter ist, als mit Matches
(@tbl:gtvsnogt_metrics). Diese Werte sind hinsichtlich der deutlich
schlechteren Ground Truth erstaunlich (vgl. @fig:match_histo). Allerdings endet
hier der Erfolg der automatisch erzeugten Ground Truth. Nachdem diese durch das
Blocking Schema gefiltert wurde, stehen 300k Matches nur 3k Non-Matches
gegenüber, die aufgrund der Ähnlichkeitsschwelle von 0.3 nicht sonderlich
herausfordernd für einen Klassifikator sind. Diese gefilterte Ground Truth wird
im Verhältnis 1:3, zu Gunsten der Non-Matches, für den Klassifikator
gesubsampelt. Bekannt ist, dass die tatsächlichen Matches lediglich 50k
betragen, deshalb ist die Wahrscheinlichkeit, dass die gesubsampelte Ground
Truth eine gute Repräsentation des Datensatzes ist, relativ gering.
Dementsprechend sind die Resultate für Recall und Precision extrem schlecht. In
der Precision-Recall Kurve ist gut zu sehen, dass auch durch eine Kalibrierung
der Wahrscheinlichkeitsschwelle des Klassifikators in keinem Punkt eine
Verbesserung der Precision möglich ist. Die Kurven mit Matches sind ähnlich zu
den Selbstkonfiguration aus @sec:humanvslearn. Während die train-train Kurve
nahzu identisch ist, unterscheidet sich die train-test Kurve leicht. Der Grund
dafür ist zum einen, dass für den Fusion-Lerner die gesubsamplete Ground Truth
eine andere ist, da die Paare zwar nach Ähnlichkeit, aber trotzdem per Zufall
gezogen werden und zum anderen, dass die Varianz der Bewertungen, des
Fusion-Lerner für die verschiedenen SVM Parameter, unter einem Prozent ist.
Daher kann selbst eine geringfügig verschiedene Ground Truth zu komplett anderen
Parametern führen. Für die train-test Daten wurde dadurch eine SVM mit
RBF-Kernel und einem $C$ von 10 trainiert. Die zugehörige Kurve zeigt, dass
dieser $C$ Parameter besser funktioniert und auch die
Standardwahrscheinlichkeitsschwelle befindet sich jetzt fast im Maximum. Aus
diesen Erkenntnissen lässt sich schließen, dass das Verfahren zum Subsampling
für die Ground Truth ohne bekannte Matches ungeeignet ist und dass das F-measure
als Maß für den Fusion-Lerner zur Parameterbestimmung nicht gut funktioniert.

```tex
\begin{table}
\centering
\begin{tabular}{lrrrrrr}\toprule
Konfiguration            & PC    & PQ    & Recall & Precision & F-measure & Avg. Precision \\ \midrule
Selbstkonf. GT   (train) & 0.956 & 0.127 & 0.870  & 0.440     & 0.585     & 0.631 \\
Selbstkonf. GT   (test)  & 0.956 & 0.127 & 0.861  & 0.696     & 0.770     & 0.709 \\
Selbstkonf. NoGT (train) & 0.956 & 0.108 & 0.136  & 0.093     & 0.111     & 0.095 \\
Selbstkonf. NoGT (test)  & 0.956 & 0.108 & 0.136  & 0.090     & 0.109     & 0.095 \\ \bottomrule
\end{tabular}
\caption{Vergleich der Qualitätsmetriken für die Selbstkonfiguration mit (GT) und
ohne (NoGT) bekannte Matches.}\label{tbl:gtvsnogt_metrics}
\end{table}
```

Die Effizienz ist in beiden Konfigurationen fast identisch, was allerdings nicht
verwunderlich ist, da das Blocking Schema ähnlich ist, die Prädikatsfunktionen
dieselben sind und auch die Pairs Quality keine große Abweichung zeigt.

## Datensatzvergleich {#sec:ds_eval}

In diesem Abschnitt wird evaluiert, wie gut die Selbstkonfiguration für andere
Datensätze als den NCVoter funktioniert. Dabei findet die Build-Phase und die
Query-Phase jeweils auf der Hälfte der Daten statt. In @fig:datasets_prc sind
die Precision-Recall Kurven für den jeweiligen Datensatz dargestellt.
Eindeutiger Gewinner ist der Restaurantdatensatz, welcher alle Duplikate findet
und bei einer Precision von 98 % fast keinen Fehler macht. Auf den
Ferbl-Datensatzen funktioniert die Selbstkonfiguration ebenfalls gut, dennoch
können ca. 20 % der Duplikate nicht gefunden werden, da diese auf
Rechtschreibfehlern basieren, die durch die Prädikate ID und Token nicht erfasst
werden. Die Publikationsdatensätze (DBLP-ACM und DBLP-Scholar) erreichen auch
gute Werte, wobei der DBLP-ACM Datensatz wie erwartet besser abschneidet. Für
den als besonders schwer geltenden CORA Publikationsdatensatz, können mit 75 %
Recall zwar noch relativ viele Duplikate gefunden werden, allerdings ist es dem
Klassifikator nicht möglich die Ergebnismenge sinnvoll einzugrenzen. Der Grund
dafür ist, dass bei Duplikaten die Attribute häufig nicht übereinstimmen,
beispielsweise ist der Titel für den einen Datensatz im Attribut `Titel` und für
den anderen Datensatz im Attribut `Buchtitel`, sodass deren Titelähnlichkeit 0
beträgt. Dadurch gibt es weniger Merkmale für den Klassifikator und folglich
treten beim Klassifizieren dieselben Schwierigkeiten auf, wie bei den
Varvektoren. Komplett versagt hat die Selbstkonfiguration auf dem
Amazon-GoogleProdukts Datensatz. Das liegt vor allem daran, dass die Attribute
aus vielen Token bestehen. Durch die Token werden jedoch viele Blöcke erzeugt,
was die Pairs Qualtity extrem senkt. Das gewählte Blocking Schema mit einer
Pairs Completeness von 11 % und einer Pairs Quality von 22 % ist dadurch im
F-Measure besser als die übrigen Kandidaten, die zwar einen besseren Recall
haben, allerdings ist die Pairs Quality in allen Fällen unter 0.01 %.

![Precision-Recall Kurven der Selbstkonfiguration auf allen Datensätzen im
Vergleich](./images/datasets_prc.pdf){#fig:datasets_prc}
