# Implementierung

In diesem Kapitel wird die Programmierumgebung, die Implementierung der Engine
und die Implementierung der Komponenten vorgestellt. Eine große Herausforderung
bei der Umsetzung der Algorithmen war es, diese für die begrenzten Ressourcen,
insbesondere Arbeitsspeicher und Rechenzeit, zu optimieren.

## Programmierumgebung

Als Programmiersprache für die Implementierung wurde Python und C eingesetzt.
Wobei C lediglich zur Implementierung der Ähnlichkeitsberechnung eingesetzt
wurde, alle anderen Teile wurden mit Python umgesetzt. Python hat den Vorteil,
dass es sehr einfach und schnell möglich ist, einen Prototypen eines
Algorithmuses zu entwickeln und zu testen. Zudem gibt es eine Vielzahl von
Qualitativ hochwertigen Paketen, die komkompfortable Standardfunktionalitäten
bereitstellen, beispielsweise das Einlesen und das Schreiben von großen
CSV-Dateien oder das Plotten von Graphen. Des Weiteren wird Python im Maschine
Learning Bereich oft genutzt, was dazu führt, dass es eine Vielzahl von
effizienten, ausgereiften und umfangreichen Frameworks gibt, um verschiedenste
Lernaufgaben zu behandeln. Vor allem der Fusion-Lerner und der Klassifikator
profitieren hiervon.

Der große Nachteil von Python ist das Global Interpreter Lock (GIL). Dieses
verhindert, dass Python-Code in mehreren Threads gleichzeitig ausgeführt werden
kann. Die Multithreading Bibliothek von Python ist daher lediglich geeignet, um
Programme mit hoher E/A-Last zu beschleunigen, da Schreib- bzw. Lesezugriffe das
GIL freigeben. Der Grund warum in Python ein GIL einzusetzen wird, dass dadruch
die Single-Thread Ausführung optimiert wird. Multithreading, im Sinne von
Gleichzeitigausführung, d.h. ein Prozess mit mehreren Threads, die auf
verschiedenen Prozessorkernen zur selben Zeit ausgeführt werden, wird dadruch
allerdings komplett unterbunden. Um denoch Python zu paralelisieren gibt es zwei
beliebte Möglichkeiten. Die erste Möglichkeit ist, statt Multithreading,
Multiprocessing einzusetzen. Das hat allderdings den Nachteil, dass Daten
zwischen Prozessen ausgetauscht werden müssen. Das lohnt sich offensichtlich nur
für rechenintensive Aufgaben, wo der Overhead des Datenaustausches keine Rolle
spielt. Die zweite Möglichkeit ist das Multithreading in einer anderen
Programmiersprache umzusetzen, beispielsweise in C. Dies ist möglich, da das GIL
lediglich die Mehrfachausführung von Python-Code verhindert. Allerdings erweist
sich dies oft als relativ schwierig, da selbst einfache Datenklassen,
beispielsweise `set` oder `dict`, keine Entsprechung in C haben und daher
manuell, in beide Richtungen Python zu C und C zu Python, z.T. aufwendig
konvertiert werden müssen.

## Engine

## Label Generator

Dabei empfiehlt es sich die Daten vorher zu sortieren, um deterministische
Ergebnisse zu erzielen.

## DNF Blocks Learner

Die Algorithmen des DNF Blocks Lerners haben bei der Implementierung das
Problem, das nur eine bestimmte Menge an Arbeitsspeicher zur Verfügung steht.
Der kritische Teil des Algorithmus ist die Erzeugung der Paarkombinationen, für
jeden Block. Angenommen die beiden Datensatzidentifier eines Paares $(p1.id,
p2.id)$ sind Integerwerte und der Datensatz hat nicht mehr als 2^30 Einträge,
dann benötigt ein Integerwert 28 Bytes. Um möglichst effizient auf die Paare
zuzugreifen, ist die Menge von Paarkombinationen als `set` implementiert. Damit
ein `set` $s$ ein Zugriffkomplexität von $O(1)$ ermöglichen kann, wird für jedes
Element in der Menge ein Hashwert berechnet. Auf einem 64-bit System beträgt die
Größe dieses Hashwertes $h$ 8 Bytes. Somit benötigt ein Eintrag $(h_j, p1_j.id,
p2_j.id) \in s$ 64 Bytes. Angenommen es werden für einen Block mit 1.000
Einträgen Paarkombinationen erzeugt. Bei Attributen mit wenigen möglichen Werten
können Blöcke entstehen, die sehr viele Datensätze enthalten. Beispielsweise hat
ein Block mit 10.000 Einträgen 49.995.000 Paare und benötigt 2.9 GB an
Arbeitsspeicher. Somit kann bereits ein rießiger Block den zur Verfügung
stehenden Arbeitsspeicher sprengen und führt damit zum Abbruch des Programmes.
Aus diesem Grund wurde der Algorithmus dahingehend erweitert, dass die Erzeugung
der Paare bei Ausdrücken, die zu viele Paare erzeugen würden, unterbunden wird
und dies Ausdrücke mit der niedrigsten Wert der Bewertungsskala bewertet werden.

Zur genaueren Analyse des Problems, wird die Verteilung der Blöcke, anhand ihrer
Größe (Anzahl von Datensätzen), betrachtet. Um die Verteilungen in Gute,
benötiget weniger Arbeitsspeicher als zur Verfügung steht und Schlechte,
benötiget mehr Arbeitsspeicher als zur Verfügung steht, zu kategorisieren, wurde
eine Schwelle $t$ eingeführt. Anhand dieser Schwelle wird ein Block $B$ bei $|B|
< t$ als guter Block und bei $|B| > t$ als schlechter Block bewertet. Daraus
kann für jede Verteilung berechnet werden, wie viel Prozent gute bzw. schlechte
Blöcke es gibt. Dadurch ist es möglich bei Audrücken mit einer höheren
schlechten Blockrate von $b$, beispielsweise $b=0.1$, die Erzeugung der
Blockpaare zu verhindern und die weitere Verarbeitung abzubrechen. Da aber
bereits ein einziger schlechter Block, mit genügend Einträgen, den
Arbeitsspeicher überfüllen kann, wird mit der Schwelle $b$ lediglich eine
Vorauswahl, besonders schlechter Ausdrücke, getroffen. Für den Fall, dass es nur
wenige schlechte Blöcke gibt, bestehen deren Blockschlüssel meistens aus
Stopwörtern, beispielsweise bei Strassennamen `Strasse`, `Weg`, oder `Platz`.
Dieses Problem kann folglich durch eine bessere Vorverarbeitung der Daten gelöst
werden. Da es das Ziel ein selbstkonfigurierendes System ist, muss die Engine,
die auf diese Weise gefundenen Stopwörter nutzen und den Lernvorgang mit der
erweiterten Vorverarbeitung der Daten wiederholen. Dieser Prozess sorgt
allerdings dafür, dass das Lernen der Konfiguration deutlich länger dauert. Eine
einfacherere Möglichkeit ist, für jeden Ausdruck eine Liste mit verbotenen
Blockschlüsseln anzulegen und die Blockschlüssel schlechter Blöcke dort
hinzuzufügen. In der Build- und Query-Phase dürfen diese Blöckschlüssel vom
Indexer demnach nicht genutzt werden.

\TODO{Ungeeignete Prädikatsfunktionen in Implementierung} Beispielsweise Monge &
Elkan, ungeordnete Q-Gramme.
Q-Gramme sind eher schlecht weil..., Monge-Elkan Prädikat schlecht weil...
