# Evaluation \label{chap:evaluation}

TBW

Qualtität vs Effizienz

## Experimenteller Aufbau

Der experimentelle Aufbau beschreibt alle Komponenten und Schritte die benötigt
werden, um die Evaluation durchzuführen und auszuwerten. Zunächst wird
beschrieben, wie die Metriken berechnet wurden. Anschließend werden die
Datensätze und deren Aufbereitung für die Evaluierung beschrieben. Danach wird
beschrieben in welchen Schritten welche Metriken berechnet wurden und
abschließend wird die genutzt Hardware bekannt gemacht.

### Berechnung der Metriken für dynamisches Enity Resolution

Während im statischen Entity Resolution die Metriken (vgl. @sec:measurements) am
Ende des Verfahrens einmalig berechnet werden können, ist dies im dynamischen
Falle nicht möglich, da ein Datenstrom kein definiertes Ende hat. Das bedeutet,
die Metriken müssen inkrementell mit jeder Anfrage $q$ erhoben werden. Für
Anfragen ohne tatsächliche Matches wird lediglich das Reduction Ratio erhoben,
da alle anderen Metriken keine Aussagekraft haben, beispielsweise wäre der
Recall undefiniert und die Precision immer 0. Zur Berechnung der
Effizienzmaße Pairs Completeness, Pairs Quality und Reduction Ratio
werden die tatsächlichen Matches $n_M$, die tatsächlichen Non-Matches $n_N$, die
Matches in der Kandidatenmenge $s_M$ und Non-Matches in der Kandidatenmenge
$s_N$ benötigt. Die Kandidatenmenge wird mit $C$ bezeichnet, die tatsächlichen
Matches mit $P$ und die Menge der Datensätze des Indexer mit $IX$. $s_M$ ist die
Anzahl der Matches zur Anfrage $q$ in $C$, $s_N$ ist die Anzahl der Non-Matches
zu $q$ in $C$, $n_M$ ist die Gesamtanzahl der Matches zu $q$ in den Matches $P$
und $n_N$ ist die Gesamtanzahl an Non-Matches zu $q$ in $IX$. Für $n_N$ muss der
Anfragedatensatz von der Gesamtmenge abgezogen werden, da dieser zu Begin jeder
Anfrage vom Indexer in den Datenbestand aufgenommen wird bzw. wenn er dort schon
vorhanden ist keine Rolle für die Enity Resolution spielt, da er herausgefiltert
wird. Mit jeder Anfrage werden $s_M, s_N$ und $n_M$ mit den vorherigen Werten
aufsummiert, sodass die Effizienzmaße Bezug auf alle bisher gestellten Anfragen
nehmen. Die Anzahl $n_N$ nimmt Bezug auf eine wachsende Menge, sodass beim
Aufsummieren die frühen Anfragen abgewertet werden. $n_M$ wird zur Berechnung
des Reduction Ratio benötigt, sodass dieses für jede Anfrage berechnet,
gespeichert und auf Abruf gemittelt wird.

Die Qualitätsmaße Recall, Precision, F-measure und Average Precision werden über
die True Positives (TP), False Positives (FP) und False Negatives (FN) bestimmt.
Deren Berechnung ist identisch zu den Werten der Effizienzmaße mit der
Abweichung, dass diese auf der Ergebnismenge $R$ gemessen werden. Die True
Negatives werden nicht berechnet, da diese in den Metriken nicht benötigt
werden. Auch hier werden die Werte für jede Anfrage summiert, sodass die aus der
Summe berechneten Metriken alle bisherigen Anfragen berücksichtigen.

### Ähnlichkeitsmaße

Der Similarity Lerner wählt aus einer Menge von Ähnlichkeitsfunktionen die
besten aus. Für die Evaluierung wurden dem Similarity Lerner die folgenden 7
Metriken übergeben, wobei die letzen vier aus @sec:similarity bekannt sind:

* Bag-Distanz [@BCP:String:02]
* Compression-Distanz [@CV:Clustering:05]
* Hamming-Distanz [@Ham:Error:50]
* Jaro-Distanz
* Jaro-Winkler-Distanz
* Jaccard-Koeffizent
* Levenshtein-Distanz

Dazu wird die Implementierung der Algorithmen aus der *Harry* Bibliothek
[@RW:Harry:16] von Rieck & Wressnegger genutzt. Dabei handelt es sich um eine
Open-Source C-Bibliothek. Eine Problematik, die bei der Verwendung aufgetreten
ist, dass die Variablen der Bibliothek ausschließlich auf dem Stack gehalten
werden. Da eine Ähnlichkeitsfunktion vor Benutzung konfiguriert werden muss, ist
es in einem Prozess folglich nur möglich ein Ähnlichkeitsmaß gleichzeitig zu
verwenden und häufiges Wechseln der Funktion ist dadurch mit hohem Mehraufwand
verbunden. Deshalb wurden im Rahmen dieser Arbeit die Berechnung und
Normalisierung der Ähnlichkeiten umgeschrieben, sodass alle Variablen auf den
Heap allokiert werden. Als Folge dessen ist es nach einmaliger Konfiguration
möglich, beliebig viele Ähnlichkeitsfunktionen gleichzeitig zu nutzen. Zudem
wurde eine Pythonschnittstelle entwickelt, damit diese Funktionen durch den
Similarity Lerner aufgerufen werden können. Die modifizierte Bibliothek ist
unter dem Namen *libsimilarity*[^simlib] auf Github, als Open-Source Software,
verfügbar. Für den Similarity Lerner wurden alle genutzten Ähnlichkeitsmaße
derart konfiguriert, dass beim Normalisieren der Distanzen, auf den Wertebereich
zwischen 0 und 1, die Dreieckungleichung eingehalten wird.

[^simlib]: [https://github.com/sappo/libsimilarity](https://github.com/sappo/libsimilarity)

### Datensätze {#sec:datasets}

Im folgenden werden die Datensätze beschrieben, die in der Evaluation genutzt
werden. @tbl:datasets_overview bietet einen Überblick über alle Datensätze.
Dabei ist der NCVoter Datensatz mit Abstand der größte, weshalb diesem besondere
Aufmerksamkeit in der Evalution geschenkt wird. Die Duplikatspaare beinhalten
sowohl Matches zwischen zwei Datensätzen, als auch Matches zwischen Cliquen
(mehr als 2 Datensätze), wobei die Cliquen vollständig als Paare aufgelöst sind.
Beispielsweise gibt es für eine Clique (1,2,3) die Paare (1,2), (1,3) und (2,3).
Die Duplikatspaare wurden im Falle von Cora und Restaurant von Hand
identifiziert und für die anderen Datensätze semi-automatisch über verschiedene
Verfahren bestimmt.

| Datensatz             |  Einträge | Duplikatspaare | Attribute |
|-----------------------+----------:+---------------:+----------:|
| Abt-Buy               |     2.171 |          1.096 |         4 |
| Amazon-GoogleProducts |     4.587 |          1.299 |         4 |
| Cora                  |     1.879 |         64.577 |         5 |
| DBLP-ACM              |     4.908 |          2.223 |         4 |
| DBLP-Scholar          |    66.877 |          5.346 |         4 |
| NCVoter               | 8.261.839 |        155.470 |        17 |
| Restaurant            |       864 |            112 |         4 |

: Überblick der verwendeten Datensätze {#tbl:datasets_overview}

#### CORA[^cora]

Der CORA Datensatz beinhaltet 1879 bibliographische Einträge über
wissenschaftliche Veröffentlichungen aus dem Maschine Learning Bereich. Die
Einträge bestehen aus Autoren, Titel, Publikationsjahr und Konferenz bzw.
Journal. Insgesamt beinhaltet dieser Datensatz 64.577 Duplikate. Dieser
Datensatz ist besonders schwierig zu Deduplizieren, da teilweise nur Initialen
der Autoren vorhanden sind bzw. Attribute zusammengefügt oder getauscht wurden.

#### Abt-Buy & Amazon-GoogleProducts[^fever]

Diese beiden Datensätze beinhalten Produkte aus dem Onlinehandel verschiedener
Plattformen mit Name, Beschreibung, Hersteller und Preis. Der Abt-Buy Datensatz
beinhaltet 2171 Einträge mit 1096 Duplikaten. Im Amazon-GoogleProducts Datensatz
sind es 4587 Einträge mit 1299 Duplikaten.

#### DBLP-ACM & DBLP-Scholar[^fever]

Diese beiden Datensätze beinhalten bibliografische Einträge mit Titel,
Autor(en), Konferenz, und Jahr. Der DBLP-ACM Datensatz beinhaltet 4908 Einträge
und 2223 Duplikate. Im DBLP-Scholar Datensatz sind sind 66877 Einträge mit 5346
Duplikaten. Dabei ist zu beachten, dass der DBLP-ACM Datensatz einfach zu
klassifizieren ist, da ein Großteil der Daten durch eine Instanz gepflegt wird.

#### Restaurant[^res]

Der Restaurant Datensatz ist ein kleiner mit lediglich 864 Einträgen, welche aus
Restaurantname, Adresse, Telefonnummer und der Küchenart bestehen. Es gibt
insgesamt 112 Restaurantduplikate, welche doppelt vorkommen.

[^cora]: https://hpi.de/naumann/projects/repeatability/datasets/cora-dataset.html
[^fever]: http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution
[^res]: http://hpi.de/naumann/projects/data-quality-and-cleansing/dude-duplicate-detection.html

#### NCVoter

Der NC Voter Registration (NCVoter) Datensatz beinhaltet ca. 8 mio Datensätze aus
dem Wählerverzeichnis des Bundestates North Carolina in den USA. Eine genaue
Analyse des Datensatzes wurde von Christen [@Chr:Preparation:13] durchgeführt.
Der Datensatz beinhaltet ca. 145.000 Duplikate zwischen zwei Einträgen, sowie
3.500 zwischen drei und mehr Einträgen. Die Zuordnung der Duplikate wurde dabei
über die Wählerregistriernummer getätigt. Weitere Attribute sind Namenspräfix,
Vorname, Zweiter, Vorname, Nachname, Namenssuffix, Alter, Geschlecht,
Rassenziffer, Ethnizitätsziffer, Strasse + Hausnummer, Stadt, Bundesland,
Postleitzahl, Telefonnummer, Geburtsort und Registrierdatum.

#### Febrl

Die Febrl-Datensätze wurden synthetisch durch den Febrlgenerator [@Chr:Febrl:08]
erzeugt. Die Attributsdaten dafür liefert ein australisches Telefonbuch. Die
generierten Einträge haben folgende Attribute: Kultur, Geschlecht, Alter,
Geburtsdatum, Titel, Vorname, Nachname, Bundesland, Vorort, Postleitzahl,
Hausnummer, Straße und Telefonnummer.

* Febrl-4k-1k: 5.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-9k-1k: 10.000 Einträge mit 1.000 Duplikaten zwischen zwei Datensätzen
* Febrl-90k-10k: 100.000 Einträge mit 10.000 Duplikaten zwischen zwei Datensätzen

### Aufbereitung der Datensätze

![Aufteilung der Datensätze in Validierungsmenge, Trainingsmenge und Testmenge.
Tupel in den Mengen sind durch Punkte markiert und Duplikate durch eine Line
zwischen zwei Tupeln. Die farbigen Linen zeigen, wie die jeweilige Untermenge
gebildet wird. ](./images/testsets.svg){#fig:testsets width=80%}

Für die Durchführung der Evaluierung wurde jeder Datensatz aus @sec:datasets in
jeweils vier disjunkte Teile gesplittet. Diese Aufteilung ist in @fig:testsets
dargestellt. Die Hälfte der Datensätze eines Datensatzes befindet sich in der
Base und die andere Hälfte ist zu gleichen Teilen in Validierung, Training und
Testing aufgeteilt. Datensätze in den Mengen sind durch schwarze Punkte
markiert. Matches sind durch Linien verbunden. In der Build-Phase wird der
initiale Index stets aus den Datensätzen der Base gebaut. Der Anfragestrom, in
der Query-Phase, wird durch Datensätze aus Validierung, Training oder Testing
zusammengestellt. Durch die Verteilung der Matches ist sichergestellt, dass
dadurch für jedes Match eine Query durchgeführt wird, in welcher das jeweilige
andere in der Base gefunden werden kann. In der Fit-Phase werden die Duplikate
zusammen benötigt, weshalb jeweils Validierung, Training und Testing mit der
Base zusammengefasst werden, wie durch die rote, grüne bzw. blaue Umrandung
dargestellt ist.

![Vorgehen zur Aufteilung eines Datensatzes in vier Teilmengen. Datensätze
werden in drei Kategorien zugeordnet: Non-Matches (rot), Matches (grün),
Matchcliquen (blaugrün). Diese werden seperat in Teil 2, 3 und 4 aufgeteilt.
](./images/testsetssplitting.svg){#fig:testsetssplit}

Das Vorgehen der Aufteilung eines Datensatzes ist in @fig:testsetssplit
dargestellt. Dazu werden die Datensätze in drei Kategorien eingeteilt (Teil 1):
Non-Matches (rot), Matches zwischen zwei Datensätzen (grün) und Cliquen von
Matches (blaugrün). Zuerst werden die Cliquen auf die Mengen verteilt (Teil 2).
Dafür wird jeweils ein Datensatz bestimmt, der der Base zugewiesen wird, hier
`1` und `16`. Anschließend werden die restlichen Datensätze der Cliquen per
Round-Robin auf Validation, Training und Testing verteilt. Für die erste Clique
bedeutet das `1` in die Base, `2` in Validation und `3` in Training. Bei der
zweiten Clique kommt `16` in die Base, `17` in Testing, da das Round-Robin der
vorherigen Clique vorgesetzt wird, `18` in Validation und `19` in Training.
Danach werden die übrigen Paare ebenfalls über Round-Robin aufgeteilt (Teil 3).
Jeweils ein Datensatz der Paare wird der Base zugewiesen (`7`, `9`, `11`) und
der andere wird auf Validation, Training und Testing verteilt, wobei der
Round-Robin Mechanismus unabhängig von dem der Cliquen ist. Zum Schluss werden
die Non-Matches aufgeteilt (Teil 4). Dazu werden jeweils drei Datensätze der
Base zugewiesen und anschließend drei per Round-Robin auf Validation, Training
und Testing verteilt. Dieser Round-Robin Mechanismus ist ebenfalls unabhängig
von den beiden anderen. Durch die drei unabhängigen Round-Robin Aufteilungen,
kann es dazu kommen, dass der Testing Datensatz bis zu drei Datensätze weniger
hat als Validation oder Testing. Bei tausenden bzw. Millionen von Datensätzen
ist dies jedoch nicht ausschlaggebend.

### Durchführung

Die Durchfürung der Evaluation erfolgt pro Durchlauf in drei Schritten. Im
ersten Schritt konfiguriert sich das System selbst und die Konfiguration wird
abgespeichert. Anschließend wird die Build- und Query-Phase zum erstem Mal
durchgeführt, um die Metriken für Qualtität und Effizienz zu bestimmen. Im
letzten Schritt wird die Build- und Query-Phase ein zweites Mal durchgeführt, um
Laufzeiten zu messen, die durch die Erhebung der Metriken in Schritt 2
verfälscht wurden. Insbesondere wird die Zeit, zum Einfügen in den Index und zum
Anfragen der Duplikate für jeden Datensatz, gemessen. Dieses Vorgehen bezieht
sich auch auf die Auswahl der Komponenten (@sec:sel_comp) und freien Parameter
(@sec:free_params).

Die Evaluation wurde auf 19 komponentengleichen Linuxrechnern, mit einer
Intel(R) Core(TM) i7-6700 CPU mit 3.40GHz und 8 Prozessorkernen, sowie 32 GB
Arbeitsspeicher, durchgeführt. Diese Rechner sind Bestandteil eines Rechnerpools
an der Hochschule RheinMain, der für die Nutzung von Studenten bereit steht.
Während der Tests wurde die Hardware nicht von anderen Studenten genutzt.

## Auswahl der Komponenten {#sec:sel_comp}

Die Komponenten des selbstkonfigurierenden Systems wurden in der Analyse in
Kapitel 3 und im Design in Kapitel 4 vorgestellt. Dabei ist der Label Generator
(@sec:ana_lbl), der Blocking Schema Lerner (@sec:ana_bs), und der Similarity
Lerner (@sec:fit_comp) fix. Alternativen gibt es für jeweils für Parser,
Präprozessor, Fusion-Lerner, Klassifikator und Indexer und werden hier
untersucht.

Die Datensätze aus @sec:datasets liegen alle Samt im CSV-Format vor. Weshalb
auch die gesplitteten Datensätze ins CSV-Format geschrieben wurden. Deshalb ist
der Parser ein CSV-Parser. Da während der Thesis nicht genügend Zeit war, um die
Datentypen der Attribute zuverlässig zu erkennen, werden alle Attribute als
Strings behandelt, was unter anderem die Wahl geeigneter Prädikate für den
Blocking Schema Lerner vereinfacht.

Weiterhin sind alle Datensätze aus @sec:datasets in englischer Sprache, weshalb
der Präprozessor bekannt englische Stopwörter herausfiltert und anschließend
alle Attribute in kleinschreibweise konvertiert. Weitere Attributs- bzw.
Datensatzspezifische Anpassungen werden nicht durchgeführt.

Um die Hyperparameter der Klassifikatoren zu erlernen muss der
Fusion-Lerner insbesondere Wissen, wie deren API Schnittstelle ist, damit er die
Modelle trainieren und auswerten kann. Aufgrunddessen und weil die
Implementierung von verschiedenen Lernverfahren und Klassifikatoren nicht
Schwerpunkt der Thesis ist, wurde für die Umsetzung die Python Maschine Learning
Bibliothek Scikit-learn [@PVG.EA:Scikitlearn:11] eingesetzt. Diese bietet ein
breites Spektrum an Funktionen:

* Klassifikation, bestimmen zu welcher Klasse ein Objekt gehört.
* Regression, einen fortlaufenden Wert eines Objektes vorhersagen.
* Clustering, automatisches gruppieren von Objekten.
* Dimensionsreduktion, reduzieren der Anzahl zu betrachtender zufälliger
  Variablen.
* Modellauswahl, vergleichen, validieren und auswählen von Parametern und
  Modellen.
* Vorverarbeitung, Eingabetransformation und Normalisierung.
* Evaluation, berechnen der Effizienz und Qualität von Modellen.

Für den Fusion-Lerner sind dabei das Module zur Modellauswahl, zur Evaluation
und zur Klassifikation interessant. Dieser ist zudem die einzige Komponente, die
ihre Aufgabe parallelisieren kann, da dies in Scikit-learn transparent
implementiert ist. Zur Auswahl stehen eine Grid Search und eine zufällige Suche
mit begrenzter Tiefe. Tests mit dem größten Datensatz (NCVoter) haben ergeben,
dass die Zeit, die eine Grid Search benötigt (> 1 Stunde), für die Evaluation
vertretbar ist. Deshalb wird die Klasse `GridSearchCV` verwendet. Für die
Kreuzvalidierung wird der Stratified K-Fold (implementiert in `StratifiedKFold`)
verwendet, da dieser das Verhältnis der Ground Truth beibehält.

Die als Klassifikator nutzbaren Komponenten müssen zur Grid Search kompatibel
sein. Das Scikit-learn Klassifikationsmodul beinhaltet dazu SVMs, DecisionTrees,
neuronale Netze und mehr. Diese Implementierungen können ohne Anpassungen mit
der Scikit-learn `GridSearchCV` verwendet werden. Die vorgestellten
Klassifikatoren in @sec:autolearn verwenden hauptsächlich Decision Trees und
Support Vector Machines. Deshalb werden diese beiden für die Evaluation
eingesetzt. Die entsprechenden Scikit-learn Klassen sind
`DecisionTreeClassifier`, `SVC` für SVM mit RBF-Kernel und `LinearSVC` für SVM
mit Linearkernel.

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_index_bt.pdf}
        \caption{MDySimII vs MDySimIII - Bauzeit}
        \label{fig:IIvsIIIbt}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{./images/mdysimIIvsIII/MDySimIII_MDySimII_memusg.pdf}
        \caption{MDySimII vs MDySimIII - Speicherverbrauch}
        \label{fig:IIvsIIImem}
    \end{minipage}
\end{figure}
```

Der Indexer kann durch den MDySimII oder den MDySimIII aus @sec:anaindxer
besetzt werden. Beide Indexer wurden auf dem ferbl-9k-1k-1 Datensatz mit Ground
Truth gegeneinander getestet, weil das MDySimII Verfahren auf dem
NCVoter-Datensatz nicht in angemessener Zeit durchgeführt werden konnte. Für den
MDySimII wurde ein Blocking Schema mit einem zweistelligen Ausdruck gelernt und
für den MDySimIII wurde ein Blocking Schema mit einem zweistelligen und einem
einstellingen Ausdruck. In @fig:IIvsIIIbt sind die Bauzeiten der beiden Indexer
verglichen. Aufgrund der komplexeren Struktur und des längeren Blocking Schemas
schneidet der MDySimIII hier erwartungsgemäß leicht schlechter ab.
@fig:IIvsIIImem zeigt den Speicherbedarf beider Indexer. Überraschend ist, dass
der MDySimII fast das doppelte an Speicher benötigt, obwohl dieser die Vorteile
in der Struktur und dem einfacheren Blocking Schema hat. Ein Hinweis dafür kann
in der Precision-Recall Kurve in @fig:IIvsIIIprc abgelesen werden. Zwar erreicht
der MDySimII einen Recall von über 80 %, doch die Precision ist nahezu 0 %. Im
Gegensatz dazu errreicht der MDySimIII knapp 60 % Recall, dafür ist die
Precision nahe 100 %. Daraus folgt, dass der MDySimII im Durchschnitt deutlich
größere Blöck erzeugt als der MDySimIII. Dementsprechend wächst auch der
Similarity Index, da dieser die Ähnlichkeiten aller Attribute eines Blockes
untereinander verknüpft. Die schlechte Precision des MDySimII macht sich direkt
in den Anfragezeiten in @fig:IIvsIIIqry bemerkbar. Die durchschnittliche
Anfragezeit für den MDySimII liegt bei 10^-2^ und ist damit um 10^-2^ dramatisch
schlechter als die des MDySimIII mit 10^-4^. Der Datensatz ist mit 10.000
Einträgen relativ klein, dennoch macht sich bereits hier ein deutlicher
Unterschied bemerkbar, sowohl in der Qualität als auch der Effizienz. Aufgrund
der schlechten Precision skaliert der MDySimII nicht und erfüllt daher bei
großen Datensätzen nicht die Anforderungen an die niedrigen Latenzen. Deswegen
wird in der weiteren Evaluation der MDySimIII als Indexer genutzt.

```tex
\begin{figure}
    \centering
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_prc.pdf}
        \caption{MDySimII vs MDySimIII - Precision-Recall Kurve}
        \label{fig:IIvsIIIprc}
    \end{minipage} \\
    \begin{minipage}{1.0\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/mdysimIIvsIII/GT-Dis3-Con3-III_GT-Dis3-Con3-II_tc_query.pdf}
        \caption{MDySimII vs MDySimIII - Anfragezeiten}
        \label{fig:IIvsIIIqry}
    \end{minipage}
\end{figure}
```

## Auswahl der Freien Parameter {#sec:free_params}

Auf der Validierungsmenge wurden robuste, freie Parameter für die Evaluierung
gewählt. Robust bedeutet, dass diese nicht optimal für jeden Datensatz sind,
sondern gute Ergebnisse für alle Datensätze liefern und gleichzeitig verhindern,
dass die Entity Resolution katastrophal versagt. Um die freien Parameter zu
bestimmen, wurden diese anhand des NCVoter Datensatzes ausprobiert und
ausgewertet. Für diesen Datensatz wurden dazu die folgenden acht Attribute
genutzt: Geburtsdatum, Vorname, Nachname, Bundesstaat, Ort, PLZ, Straße und
Telefonnummer. Zunächst werden die Parameter des Blocking Schema Lerners
bestimmt, da das Blocking Schema zur Bewertung der Label Generator und der
Fusion-Lerner Parameter benötigt wird.

### Blocking Schema Lerner

Für den Blocking Schema Lerner aus @sec:ana_bs müssen folgende freie Parameter
bestimmt werden:

* Blockschlüsselgenerator
* Blockingprädikate
* Maximale Konjunktion und Disjunktion
* Blockfilter (Größe/Ratio)

#### Blockschlüsselgenerator

Von den drei unterschiedlichen Möglichkeiten zur Erzeugung zusammengesetzer
Blockschlüssel aus @sec:bkv_gen wurde im Rahmen dieser Thesis nur der
vorgestellte Algorithmus \ref{alg:bkvs} implementiert. Weshalb auch dieser für
die Evaluation genutzt wird.

#### Geeignete Prädikate

```tex
\begin{figure}
    \centering
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_memusg.pdf}
        \caption{Maximaler Arbeitsspeicherverbrauch der Prädikatspaare in der
                 Query-Phase.}
        \label{fig:pred_mem}
    \end{minipage}\hfill
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_index_ips.pdf}
        \caption{Einfügeoperation pro Sekunde in der Build-Phase.}
        \label{fig:pred_ips}
    \end{minipage}\\
    \begin{minipage}{0.46\textwidth}
        \centering
        \includegraphics[width=1.0\textwidth]{./images/dnf_pred/MDySimIII_query_ips.pdf}
        \caption{Anfragen pro Sekunde in der Query-Phase.} \label{fig:pred_qps}
    \end{minipage}
\end{figure}
```

Die Blockingprädikate sind Hauptbestandteil eines Blocking Schema und haben
deswegen den größten Einfluss auf die Qualität und Effizienz des Gesamtsystems.
Um geeignete auszuwählen, wurden folgende 6 Prädikate betrachtet.

* Identität eines Attributes (ID)
* Token eines Attributes die durch Leerzeichen getrennt sind (Tok)
* Prefixe eines Attributes, der Längen 2-4 (Pre)
* Suffixe eines Attributes, der Längen 2-4 (Suf)
* Bigram eines Attributes, sind n-Gramme der Länge 2 (Bi)
* Trigram eines Attributes, sind n-Gramme der Länge 3 (Tri)

Aus Performanzgründen wurden immer nur zwei Prädikate zusammen getestet. Jeder
Durchlauf wurde folglich mit 16 spezifischen Blockingprädikaten durchgeführt,
eines pro Attribut pro Prädikat. Da die Blockschlüsselerzeugung über ID
offensichtlich am effizientesten ist, wurden alle Kombinationen der anderen
Prädikate zusammen mit ID getestet. Zusätzlich wurden (Prefix, Sufix), (Bigram,
Trigram) und (ID, Token, Prefix, Suffix, Bigram, Trigram) getestet. Da einige
dieser Kombinationen einen deutlichen Einfluss auf die Effizienz haben, wurde
aus Zeitgründen der Febrl-4k-1k Datensatz genutzt.

In @fig:pred_mem ist der Arbeitsspeicherverbrauch des gebauten Indexes
dargestellt. Alle generierten Blocking Schemata bestehen aus zwei zweistelligen
Ausdrücken. Das speicherhungriste Blocking Schema wurde durch (ID, Trigram)
erzeugt und besteht ausschließlich aus spezifischen Blockingprädikaten der
Trigramme. Dieses ist dafür die Ausnahme, denn alle anderen Blocking Schemata
nutzen beide Prädikate. Das speichersparenste Schema wird durch (ID, Token)
erzeugt. In den [@fig:pred_ips;@fig:pred_qps] sind die Einfügeoperationen und
Anfragen pro Sekunde dargestellt. Dabei zeigt sich, dass (ID, Token) mit
deutlichem Abstand in beiden Abbildungen dominiert. Dies ist zum einen auf die
geringerer Anzahl der Blockschlüssel zurückzuführen, die pro Attribute erzeugt
werden und zum anderen auf die deutlich bessere Pairs Quality (PQ), welche in
@tbl:pred_qual zu sehen ist. Hierbei erreicht (ID, Token) einen Wert von 0.96,
das nächstbeste Paar (Bi-Tri) kommt lediglich auf 0.33. Die hohe Pairs Quality
kommt allerdings auf Kosten der Pairs Completeness (PC), welche von allen
Kombinationen mit 0.83 die Schlechteste ist. Anderen Paare erreichen hier bis zu
0.99, beispielsweise für (Prefix, Suffix). Die Prädikate ID und Token sind zwar
Qualitativ leicht unterlegen, zeigen jedoch bei der Effizienz eine deutliche
Überlegenheit. Alle anderen Prädikate überzeugen zwar Qualitätiv, skalieren
jedoch nicht und können die Anforderungen an die niedrigen Latzen dadurch nicht
erfüllen. Für die weitere Evaluation werden deshalb nur noch die Prädikate ID
und Token benutzt.

|    | Bi-Tri | ID-Bi | ID-Pre | ID-Tok-Pre-Suf-Bi-Tri | ID-Suf | ID-Tok | ID-Tri | Pre-Suf |
|----+-------:+------:+-------:+----------------------:+-------:+-------:+-------:+--------:|
| PC |   0.97 |  0.90 |   0.99 |                  0.99 |   0.97 |   0.83 |   0.95 |    0.99 |
| PQ |   0.33 |  0.09 |   0.33 |                  0.14 |   0.09 |   0.96 |   0.28 |    0.18 |

: Pairs Completeness und Pairs Quality der Prädikatkombinationen {#tbl:pred_qual}

#### Maximale Konjunktion/Disjunktion

![Bauzeiten des Indexers bei unterschiedlicher maximaler Länge der Konjunktion
der Ausdrücke des Blocking Schema.
](./images/dnf/MDySimIII_index_bt.pdf){#fig:dnf_bt width=50%}

Die maximale Konjunktion $max_k$ von spezifischen Blockingprädikaten von
Ausdrücken und die maximale Disjunktion $max_d$ von Ausdrücken können
entscheidend sein, um ein gutes Blocking Schema zu bilden. Einen konjunktiver
Ausdruck wird bewertet, indem die Blöcke durch Indexer gebaut werden, was
relativ zeitintensiv ist. Auf der anderen Seite werden für die Disjunktion der
Ausdrücke nur boolsche Vektoren verodert und miteinander verglichen, was
deutlich effizienter ist. Im Allgemeinen gilt, je weniger Konjunktionen erlaubt
sind, desto höher ist die Wahrscheinlichkeit, dass ein Ausdruck Non-Matches
nicht korrekt ausschließen kann und je weniger Disjunktionen, desto höher ist
die Wahrscheinlichkeit, dass nicht alle Matches erfasst werden. In
[@KM:Unsupervised:13] geben Kejriwal & Miranker an, in der Evaluation ihres DNF
Blocking Schema Lerners, keine Verbesserung für $max_k > 2$ gemessen zu haben.
Im Gegensatz dazu wird $max_d$ von Ihnen nicht beschränkt, zudem wird über die
maximal gemessenen Disjunktion keinerlei Aussage gemacht. Für die Evaluation des
Algorithmus aus @sec:ana_bs wurde $max_d$ für alle Durchläufe auf 5 gesetzt, da
das Verfahren hierfür effizient genug ist und es keinen anderen Grund gibt diese
zu restriktieren. Für $max_k$ wurden die Werte 1, 2 und 3 getestet. Bei den
ausgwählten 8 Attributen des NCVoter Datensatzes mit den zwei spezifischen
Blockingprädikaten beträgt die Anzahl der gebildeten Ausdrücke für $max_k = 1$
gleich 16, für $max_k = 2$ gleich 136, dass sind 8.5 Mal mehr Ausdrücke und für
$max_k = 3$ gleich 696 Ausdrücke, was nochmals 5 Mal soviele sind. Dabei ist zu
beachten, dass $max_k$ jeweils die Ausdrücke von $max_k - 1$ beinhaltet. Die
Anzahl der zu prüfenden Ausdrücke, hat deutlichen Einfluss auf die gesamte
Lernzeit. Bei $max_k = 1$ dauert das Lernen nur 22 Minuten, bei $max_k = 2$
dauert es 5.5 Mal solange mit 2 Stunden und bei $max_k = 3$ dauert es nochmal
12.5 Mal solange, mit einem Tag und einer Stunde. Während von $max_k = 2$ auf
$max_k = 3$ die Anzahl der Ausdrücke um das fünffache wächst, ist dies bei der
Lernzeit über das 12fache. Dies lässt sich mit den Bauzeiten des Indexers
erklären. In @fig:dnf_bt sind die Bauzeiten des jeweils besten Blocking Schema
im jeweiligen Durchlauf dargestellt. Während der Bauzeit wurden ca. 4 Mio
Datensätze eingefügt. Dabei besteht das beste Blocking Schema in allen drei
Fällen stets aus Ausdrücken der Länge $max_k$. Die Blocking Schemata für $max_k
= 1$ und $max_k = 2$ haben jeweils zwei Ausdrücke und das für $max_k = 3$
besteht aus drei Ausdrücken. Die Bauzeiten verraten, dass je länger die
Ausdrücke werden, desto länger benötigt der Indexer zum Erzeugen der
Blockschlüssel, wodurch sich die Bauzeit verlängert. Da die Blockschlüssel auch
für jede Anfrage erzeugt werden, verlängern sich diese ebenfalls. Während bei
$max_k = 1$ noch 50k Anfragen/s beantwortet werden, sind es bei $max_k = 2$ nur
noch 22k Anfragen/s und bei $max_k = 3$ lediglich noch 13k Anfragen/s. Neben der
Effizienz muss allerdings auch die Qualität überzeugen, was bei $max_k = 1$
nicht der Fall ist. Die Pairs Completeness beträgt lediglich 0.16 und die Pairs
Qualtity 0.1. Für $max_k = 2$ liegt die Pairs Completeness bei guten 0.95, auch
die Pairs Quality ist mit 0.13 leicht besser. Für $max_k = 3$ kann diese sich
noch auf 0.98 verbessern, die Pairs Quality bleibt mit 0.13 jedoch gleich.
Sowohl $max_k = 2$ als auch $max_k = 3$ bieten einen guten Recall, da $max_k
= 2$ jedoch deutlich effizienter ist, wird dieser Wert für die Evaluation
genutzt. Die maximale Disjunktion lag bei drei in Verbindung mit $max_k = 3$,
weshalb für $max_d$ der Wert 3, für die Evaluation ausreichend erscheint.

#### Blockfilter

![Precision-Recall Kurve des zweiten, guten Blocking Schema bei sinkendem $t$.
](./images/fp_dnf/MDySimIII_ncvoter_block_filters_prc.pdf){#fig:tvsg_prc
width=50%}

In Abschnitt @sec:eval_dnflearner wurden zwei Filter für den Blocking Schema
Lerner eingeführt, die dafür sorgen, dass offensichtlich schlechte Ausdrücke und
schlechte Blockschlüssel nicht im Detail betrachtet werden, wenn diese zur
Überlastung der Arbeitsspeicherkapazitäten führen können. Der erste Filter ist
die Schwelle $t$, ab welcher ein Block mit mehr Einträgen als schlechter Block
gilt. Der zweite Filter ist die minimale gute Blockrate, ist die Anzahl der
Einträge in guten Blöcken prozentual kleiner als $g$ wird der komplette Ausdruck
verworfen. Erfüllt ein Ausdruck die gute Blockrate, aber erzeugt Blockschlüssel
größer $t$, dann werden diese verboten und nur die Blöcke kleiner $t$ benutzt.
Bei den meisten schlechten Blockschlüsseln handelt es sich um Stoppwörter, die
in der Vorverarbeitung nicht korrekt aussortiert wurden. In der Evaluation wurde
$t$ für die Werte 25, 50, 100, 200, 500 und 1000 jeweils auf $g$ 0.75, 0.8,
0.85, 0.9, 0.95 und 1 angewandt. @tbl:tvsg_pr zeigt Recall und Presion für alle
Kombinationen von $g$ und $t$. Für $t$ gleich 500 und 1000 konnten keine
Ergebnisse ausgewertet werden, da diese zu Abbruchen aufgrund zu hoher
Speicheranforderungen geführt haben. Weitere Abbrüche gab es für $t$ gleich 200
in Verbindung mit $g$ gleich 0.75 und 0.85. Zudem ist eine minimale gute
Blockrate offensichtlich ungeeignet, da nur wenige Ausdrücke dieser genüge
tragen können. Die übrigen Paarungen beschränken sich auf zwei Blocking
Schemata. Eines mit schlechter Pairs Completeness und mittelmäßiger Pairs
Quality und eines mit guter Pairs Completeness und schlechter Pairs Quality.
Über einen Klassifikator kann eine schlechte Pairs Quality zu einer guten
Precision verbessert werden, der Recall hingegen ist jedoch durch die Pairs
Completeness beschränkt (Recall $\leq$ Pairs Completeness). Deswegen ist das
zweite Blocking Schema mit höherer Pairs Completeness zu bevorzugen. In
@fig:tvsg_prc ist die Precision-Recall Kurve (hier: Precision=Pairs Quality,
Recall=Pairs Completeness) für das zweite Blocking Schema mit steigender
maximaler Blockgröße abgebildet. Je größer $t$ desto größer ist entsprechend
auch der Recall, da weniger Blockschlüssel verboten werden. Allerdings fällt
gleichzeitig die Precision. Eine robuste maximale Blockgröße, die für alle
gewählten $g$ funktioniert, wird mit  100 ausgewählt. Für die minimale gute
Blockrate wird 0.85 gewählt, da dort jeweils nach unten und oben noch Puffer
ist, in welchem das präferierte Blocking Schema ebenfalls ausgewählt wurde. Für
deutlich größere Datensätze als den NCVoter, wird die Blockgröße von 100
vermutlich zu Verschlechterungen der Effizienz führen, da allerdings in der
Evaluation kein größerer Datensatz genutzt wird, ist die gewählte Blockgröße
vertretbar.

| t\\g | 0.75      | 0.80      | 0.85      | 0.90      | 0.95      | 1.0       |
|------+-----------+-----------+-----------+-----------+-----------+-----------|
| 25   | 0.87/0.27 | 0.16/0.47 | 0.16/0.47 | 0.16/0.47 | 0.16/0.47 | 0.06/0.57 |
| 50   | 0.92/0.18 | 0.92/0.18 | 0.92/0.18 | 0.16/0.44 | 0.16/0.44 | 0.06/0.57 |
| 100  | 0.95/0.13 | 0.95/0.13 | 0.95/0.13 | 0.95/0.13 | 0.16/0.41 | 0.14/0.25 |
| 200  |           |           | 0.98/0.09 | 0.98/0.09 | 0.98/0.09 | 0.14/0.25 |
| 500  |           |           |           |           |           |           |
| 1000 |           |           |           |           |           |           |

: Tabelle mit Recall=Pairs Completeness und Precision=Pairs Quality für
unterschiedliche maximale Blockgrößen und minimale gute Blockrate. {#tbl:tvsg_pr}

### Label Generator

Die freien Parameter des Labelgenerators sind die Fenstergröße, die untere und
obere Schwelle, sowie die maximalen Matches und Non-Matches. Dabei werden die
Schwellen nur benötigt falls keine Ground Truth existiert und der Label
Generator diese selbstständig erzeugt. Die Auswertung der eigenständig
generierten Ground Truth erfolgt durch die bekannten Matches und wird in
Relation zu einem Kontrollexperitment, dass mit Ground Truth Matches
durchgeführt wurde, gesetzt.

#### Fenstergröße {#sec_lblwin}

Zur Bestimmung einer geeigneten Fenstergröße wurde der Label Generator ohne
Matches betrachtet (vgl. @sec:ana_lbl). Diese Variante reagiert, im Vergleich
zur Variante mit Matches, deutlich empfindlicher auf die unterschiedlichen
Parameter, wodurch der Effekt der unterschiedlichen Fenstergrößen $w$ einfacher
ausgewertet werden kann. Dazu wurde die untere Schwelle $lt$ und die obere
Schwelle $ut$ in 0.1 Schritten bis 0.5 erhöht jeweils mit den Fenstergrößen 2,
5, 10 und 20 ausprobiert. Die maximalen Matches wurden mit 10 % der Gesamtmenge
und die maximalen Non-Matches mit 25 % der Gesamtmenge bestimmt. Bei dem
genutzten NCVoter Datensatz sind somit die maximalen Matches bei 551k und die
maximalen Non-Matches bei 1.337k. In der @tbl:windows sind die Ergebnisse für
die unterschiedlichen Fenstergrößen dargestellt. Die obere Schwelle $ut$ hatte
dabei keine entscheidende Auswirkung, sodass lediglich die untere Schwelle $lt$
betrachtet wird. Für jede Schwelle wurden Matches (P), Non-Matches (N), Pairs
Completeness (PC) und Pairs Qualtity (PQ) analysiert. Die Pairs Quality liefert
keinen entscheidenden Hinweis auf ein geeignetes Fenster, da diese sich
lediglich zwischen 1 % und 11 % hin und her bewegt. Dies bedeutet zwar einen
Performanzunterschied, welcher jedoch nicht ausschlaggebend signifikant ist.
Beim Blick auf die Pairs Completeness zeigt sich, dass diese sich zwischen 13 %
und 16 % für alle Fenstergrößen bewegt, mit Außnahme von $w=2$. Dort ist die
Pairs Completeness für $lt \leq 0.3$ mit 95 % deutlich besser. Das
Kontrollexperiment mit $w=2$ erreicht ebenfalls eine Pairs Completeness von 95 %
und die Pairs Qualtity ist mit 13 % lediglich 2 % besser. Für $w=2$ werden am
wenigsten Paare gebildet, allerdings sind die Paare die gebildet werden, die mit
der höchsten Ähnlichkeit zueinander, da nur Blocknachbarn betrachtet werden.
Aufgrund dessen werden viele deutlich verschiedene Paare ausgeschlossen, wie
sich bei $lt=0.1$ bemerkbar macht, da hier lediglich 8k Non-Matches generiert
wurden. Bei $lt=0.2$ gibt es mit 655k dann allerdings schon eine große Auswahl
an Non-Matches und mit $lt=0.3$ wurden bereits mehr Paare generiert als das
Maximum. Aufgrund des guten Abschneidens gegenüber dem Kontrollexperiment und
keiner wirklichen Konkurrenz durch andere Fenstergrößen wird diese für die
weitere Evaluation mit 2 bestimmt.

```tex
\begin{table}
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=2) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 8k     & 0.95 & 0.11 \\
0.2      & 551k & 655k   & 0.95 & 0.11 \\
0.3      & 551k & 1,377k & 0.95 & 0.11 \\
0.4      & 433k & 1,377k & 0.15 & 0.10 \\
0.5      & 433k & 1,377k & 0.16 & 0.10 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=5) & P    & N      & PC   & PQ   \\ \midrule
0.1      & 551k & 32k    & 0.15 & 0.01 \\
0.2      & 551k & 1,377k & 0.15 & 0.01 \\
0.3      & 551k & 1,377k & 0.15 & 0.01 \\
0.4      & 551k & 1,377k & 0.16 & 0.11 \\
0.5      & 551k & 1,377k & 0.16 & 0.11 \\ \bottomrule
\end{tabular}
\end{minipage}\\\vskip 2.5em
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=10) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 66k    & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.16 & 0.13 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\begin{minipage}{0.45\textwidth}
\begin{tabular}{rrrrr}\toprule
lt (w=20) & P    & N      & PC   & PQ   \\ \midrule
0.1       & 551k & 119k   & 0.15 & 0.01 \\
0.2       & 551k & 1,377k & 0.15 & 0.01 \\
0.3       & 551k & 1,377k & 0.15 & 0.01 \\
0.4       & 551k & 1,377k & 0.16 & 0.11 \\
0.5       & 551k & 1,377k & 0.16 & 0.12 \\ \bottomrule
\end{tabular}
\end{minipage}\hfill
\label{tbl:windows}
\caption{Auswertung der Konfiguration mit selbständig erzeugter Ground Truth für
         verschiedenen Fenstergrößen. Die im Kontrollexperiment ermittelte Pair
         Completeness beträgt im Vergleich 0.95 und Pairs Quality 0.13.}
\end{table}
```

#### Untere und Obere Schwelle

![Ähnlichkeitsverteilung der TF/IDF Ähnlichkeiten der Ground Truth des NCVoter
Datensatzes, welche anhand der tatsächlichen Matches erzeugt wurde. Die
Datensätze Matches bzw. Non-Matches wurden in 5 % Schritten nach Ähnlichkeit
zusammengefasst.](./images/ncvoter_matches_histo.pdf){#fig:match_histo}

Die untere Schwelle $lt$ legt fest, bis zu welchem Ähnlichkeitswert Paare als
Non-Matches betrachtet werden und die obere Schwelle $ut$ legt fest, ab welchem
Ähnlichkeitswert Paare als Matches betrachtet werden, dabei gilt stets $lt \leq
ut$. In einem Experiment wurden $lt$ und $ut$ in 0.1 Schritten betrachtet und so
alle Konfigurationen bis 1.0, auf dem NCVoter-Datensatz getestet. Für das
Fenster wurde der bereits bestimmte Wert von 2 gesetzt. Zur Auswertung wurden
Pairs Completeness, Pairs Quality und die Ground Truth analysiert. Anhand der
Pairs Completeness und Pairs Qualtity kann betrachtet werden, wie gut ein
Blocking Verfahren auf der generierte Ground Truth funktioniert. Durch die
gefilterte Ground Truth hingegen kann herausgefunden werden, wie viele Ground
Truth Paare für den Fusion-Lerner zur Verfügung stehen. Die [@tbl:recall;
@tbl:fp; @tbl:fn] betrachten nacheinander die Pairs Completeness, die Matches
und die Non-Matches. Die Pairs Quality ist uninteressant, da deren Werte relativ
konstant bei 0.1 liegen, mit einer Varianz von 0.03. In @tbl:recall ist gut zu
sehen, dass die Pairs Quality zwischen einer $ut$ von 0.1 und 0.4 immer eine
gute Pairs Qualtity von 95 % erzeugt. Der Blick auf das erlernte Blocking Schema
zeigt, dass dieses immer dasselbe und gleich zu dem aus @sec_lblwin ist. Dies
trifft auch noch teilweise für $ut=0.5$ zu, allerdings nur für $lt \leq 0.3$.
Für alle $ut > 0.5$ variieren die Blocking Schemata, wobei unabhängig von $lt$
keines über 17 % Pairs Completeness kommt. In @fig:match_histo ist die
Ähnlichkeitsverteilung der Ground Truth, des Kontrollexperiments dargestellt,
die aus den tatsächlichen Matches erzeugt wurde. Der Großteil der Matches hat
eine Ähnlichkeit zwischen 0.2 und 0.5, wohingegen der Großteil der Non-Matches
sich überlappend zwischen 0.1 und 0.3 befindet. Für $ut > 0.5$ fällt der Recall
dramatisch ab, weil ein Großteil der Matches nicht mehr erfasst wird und für $ut
= 0.5$ sind die Recallwerte für $lt \leq 0.3$ noch gut. Wird $lt$ allerdings
weiter erhöht fällt der Recall, da sich nun zu viele tatsächliche Matches in den
Non-Matches befinden.

```tex
\begin{table}
\centering
\begin{tabular}{rrrrrrrrrrr}\toprule
PC  & 0.1  & 0.2  & 0.3  & 0.4  & 0.5  & 0.6  & 0.7  & 0.8  & 0.9  & 1.0  \\ \midrule
0.1 & 0.95 & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.2 &      & 0.95 & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.15 & 0.15 & 0.13 \\
0.3 &      &      & 0.95 & 0.95 & 0.95 & 0.15 & 0.15 & 0.16 & 0.13 & 0.13 \\
0.4 &      &      &      & 0.95 & 0.15 & 0.16 & 0.16 & 0.16 & 0.14 & 0.14 \\
0.5 &      &      &      &      & 0.16 & 0.16 & 0.13 & 0.14 & 0.14 & 0.06 \\
0.6 &      &      &      &      &      & 0.13 & 0.14 & 0.14 & 0.16 & 0.06 \\
0.7 &      &      &      &      &      &      & 0.14 & 0.10 & 0.16 & 0.06 \\
0.8 &      &      &      &      &      &      &      & 0.10 & 0.16 & 0.06 \\
0.9 &      &      &      &      &      &      &      &      & 0.14 & 0.06 \\
1.0 &      &      &      &      &      &      &      &      &      & 0.06 \\ \bottomrule
\end{tabular}
\caption{Pairs Completeness bei verschiedenen Schwellen}\label{tbl:recall}
\begin{tabular}{rrrrrrr}\toprule
Matches & 0.1       & 0.2       & 0.3       & 0.4       & 0.5   \\ \midrule
0.1 & 551k/300k & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.2 &           & 551k/300k & 551k/300k & 551k/300k & 443k/288k \\
0.3 &           &           & 551k/300k & 551k/300k & 443k/288k \\
0.4 &           &           &           & 551k/300k & 443k/287k \\
0.5 &           &           &           &           & 443k/273k \\ \bottomrule
\end{tabular}
\caption{Anzahl der Matches und gefilterted Matches bei verschiedenen Schwellen}\label{tbl:fp}
\begin{tabular}{rrrrrrr}\toprule
Non-Matches & 0.1     & 0.2        & 0.3         & 0.4         & 0.5        \\ \midrule
0.1         & 8k/0    & 8k/0       & 8k/0        & 8k/0        & 8k/0       \\
0.2         &         & 655k/66    & 655k/66     & 655k/66     & 655k/66    \\
0.3         &         &            & 1377k/2430  & 1377k/2430  & 1377k/2430 \\
0.4         &         &            &             & 1377k/13916 & 1377k/11k  \\
0.5         &         &            &             &             & 1377k/8k   \\ \bottomrule
\end{tabular}
\caption{Anzahl der Non-Matches und gefilterten Non-Matches bei verschiedenen
         Schwellen}\label{tbl:fn}
\end{table}
```

Deshalb werden in den [@tbl:fp; @tbl:fn] lediglich $ut$-Werte kleiner 0.6
betrachtet. Die maximalen Matches, die der Label Generator erzeugen darf, liegen
bei 10 % der Gesamtmenge und betragen 551k. Bei den Non-Matches ist das Limit 25
% und damit 1,3 mio. Für die künstliche Anreicherung der gefilterten
Non-Matches, stehen jedoch alle erzeugten Non-Matches zur Verfügung,
dementsprechend je höher $lt$ desto mehr Non-Matches und umgekehrt für $ut$. In
@tbl:fp ist zu sehen, dass für $ut \leq 0.4$ die Ausgangsmenge der Matches auf
das Maximum beschränkt wurde. Da jeweils die Matches mit der höchsten
Ähnlichkeit genutzt werden, sind diese Mengen identisch, weshalb auch die
gefilterte Mengen mit jeweils 300k Datensätzen, aufgrund desselben Blocking
Schema, identisch sind. Für $ut=0.5$ ist die Ausgangsmenge kleiner als das
Maximum. Die gefilterte Menge beträgt in diesem Fall 288k, was mehr als genügend
Matches sind, um einen Klassifikator zu trainieren. Zu beachten ist, dass diese
Menge 6-Mal soviele Matches beinhaltet, wie die Menge tatsächlichen Matches.

In @tbl:fn wird die Anzahl der Non-Matches dargestellt. Für $lt=0.1$ sind
insgesamt nur 8k Paare erzeugt worden, weil das TF/IDF Blocking die meisten der
sehr unähnlichen Paare ausschließt. Nach dem Filtern durch das Blocking Schema
sind keine Non-Matches mehr vorhanden, da das Blocking Schema verhindert, dass
diese offensichtlichen Non-Matches zusammen gruppiert werden. Für $lt=0.2$ gibt
es ein ähnliches Bild. Zwar ist die Anzahl der Ausgangsmenge mit 655k deutlich
höher, dennoch werden lediglich 66 Paare gefunden, die einen gemeinsamem
Blockschlüssel haben, was zum Trainieren eines Klassifikators nicht ausreichend
ist. Interessanter wird es erst ab $lt=0.3$. Hier wird erste Mal das Maximum der
Ausgangsmenge mit 1377k erreicht. Die gefilterten Non-Matches betragen 2430, was
im Vergleich zu den Matches immer noch sehr wenig ist, aber durchaus für das
Klassifikatortraining genügt. Mit $lt=0.4$ erhöht sich diese Anzahl nochmals, um
das 5-fache. Ein Blick auf @fig:match_histo zeigt aber, dass sich dadurch der
Großteil der Matches in den Non-Matches befindet. Die Linien für Matches und
Non-Matches schneidet sich in im Bereich 0.1 - 0.5 bei 0.28. Unterhalb gehen
viele Non-Matches verloren und oberhalb viele Matches. Deshalb wird sowohl $lt$
als auch $ut$ auf 0.3 festgelegt, da ab hier auch genügend Paare zum Trainieren
eines Klassifikators zur Verfügung stehen.

#### Maximale Paare der Ground Truth

Sowohl mit, also auch ohne Ground Truth Match werden dem Label Generator
mitgeteilt, wie viele Matches $max_p$ bzw. Non-Matches $max_n$ in der Ground
Truth enthalten sein dürfen. Im Fall ohne Ground Truth Matches werden jeweils
die $max_p$ Matches bzw. $max_n$ Non-Matches ausgewählt, die die höchste
Ähnlichkeit haben. Im Fall mit Ground Truth Matches werden sowohl Matches als
auch Non-Matches nach ihrer Ähnlichkeitsverteilung ausgewählt. Mithilfe dieser
Ground Truth sucht der Blocks DNF Generator nach dem besten Blocking Schema.
Anschließend wird die Ground Truth, anhand des Blocking Schema, gefiltert,
sodass diese nur Paare enthält, die einen gemeinsamen Blockschlüssel haben. In
diesem Schritt werden sehr viele Non-Matches herausgefiltert, da das der primäre
Zweck des Blocking Schema ist. Damit für den Fusion-Lerner genügend Non-Matches
zur Verfügung stehen, werden die Non-Matches mit allen generierten Non-Matches
des Label Generators angereichert. Hierbei spielt $max_n$ keine Rolle. Die
Auswirkungen dieser Parameter wurden auf dem NCVoter Datensatz mit Ground Truth
und ohne Ground Truth mit Fenstergröße $w=2$ und Schwellen $lt=ut=0.3$
evaluiert. Ausgangssituation ist (0.1, 0.25) mit $max_p$ 10 % und $max_n$ 25 %
der Gesamtmenge, welche bereits zur Ermittlung der Fenstergröße und der
Schwellen genutzt wurden. Bei allen getesteten Paarungen ist das Limit der
Non-Matches höher als das der Matches, um das Verhältnis im Datensatz zu
repräsentieren. Getestet wurden zunächst größere Werte mit (0.5, 2.5), (1, 5),
(5, 25), (10, 50). Dabei wuchsen die Matches leicht auf 650k und die Non-Matches
bis stark auf 19 Mio. Auf das Blocking Schema hat die vergrößerte Ground Truth
in keinem Fall einen Einfluss. Die Betrachtung der Matches im größten Fall (10,
50) ergibt, dass diese sich lediglich um ca. 100k verändert hat. Die neu
hinzugekommen Matches wurden durch die Filterung auf ein paar Hundert reduziert.
Das bedeutet, dass die erweiterten Matches die Ausdrücke des Blocking Schema, im
Bezug auf Pairs Completeness, abwerten und zwar je mehr desto größer die Anzahl
der Matches. Allerdings haben sich die Non-Matches mit 19 mio. dramatisch
erhöhtet und durch die Filterung bleiben lediglich die bekannten 2k übrig,
sodass die erhöhte Menge sich positiv auf das Reduction Ratio und die Pairs
Qualtity auswirkt. Damit wird der Negativeffekt auf die Pairs Completeness
aufgehoben und das Blocking Schema bleibt dasselbe. Neben größeren Werten wurden
auch kleinere getestet (0.0001, 0.00025), (0.001, 0.0025), (0.01, 0.025). Für
(0.01, 0.25) werden in etwa so viele Matches ausgewählt wie tatsächlich
enthalten sind. Recall und Precision sind allerdings mit 15 % und 0.1 % sehr
schlecht. Dasselbe Ergebnis zeigt sich bei den noch kleineren Paaren. Daraus
folgt, dass das Ausgangspaar (0.1, 0.25) bereits gut gewählt war und auch
robuste Werte liefert.

### Fusion-Lerner {#sec:fp_fusion}

Für den Fusion-Lerner müssen noch vier freie Parameter bestimmt werden:

* Anzahl der k Teilmengen für die Kreuzvalidierung
* Anzahl der maximale Paare beim Subsampling und deren Verhältnis zu Matches und
  Non-Matches
* Qualitätsmaß zur Bewertung der trainierten Modelle

* Bei der Kreuzvalidierung durch das Stratified K-Fold Verfahren, wird die
  Anzahl für $K$ mit 3 bestimmt, da mit anderen Werten kein nennenswerter
  Unterschied gemessen wurde und dieser der Standardwert in Scikit-learn ist.
* Während der Entwicklung hat sich eine Grid Search auf 5.000 Paaren als
  genügend effizient herausgestellt, dabei wurde eine Verhältnis von ein Match
  auf drei Non-Matches benutzt. Im Rahmen dieser Thesis war es zeitlich nicht
  mehr die Werte zu analysieren, weshalb diese für die Evaluation übernommen
  wurden. Das Sampling der Paare wird dabei anhand der Ähnlichkeitsverteilung
  durchgeführt.
* Ein Qualitätsmaß muss sowohl Recall als auch Precision berücksichtigen,
  deshalb wurden hierzu das F-measure und die Average Precision betrachtet. In
  mehreren Durchläufen wurden die beiden Maße jeweils für einen Klassifikator
  genutzt. Dabei wurde zunächst festgestellt, dass Recall und Precision immer
  gleich sind, unabhängig davon welcher Klassifikator und welche Parameter durch
  die Grid Search bestimmt wurden. Die Ergebnisse wurden gegen Baseline
  verglichen, bei welcher kein Klassifikator genutzt wurde und die
  Kandidatenmenge $C$ der Ergebnismenge $R$ entspricht. Hierbei wurde
  festgestellt, dass mit Klassifikator die Precision dieselbe ist und der Recall
  um 13 % niedriger. Zweck des Klassifikator ist es jedoch, möglichst ohne
  Verlust des Recalls, die Precision so nah wie möglich an 1 zu bringen.
  Aufgrund dieses Ergebnisses, kann keine endgültige Entscheidung über das
  Qualitätsmaß getroffen werden. Das Problem wird deshalb in @sec:base_par_full
  genau analysiert. Für diese Untersuchung wird jedoch ein Qualitätsmaß
  benötigt, weshalb zunächst das F-measure ausgewählt wird, welches bereits für
  das Blocking Verfahren gute Dienste leistet.

## Einfluss der Ähnlichkeitsvektoren {#sec:base_par_full}

![Precision-Recall Kurven für Varvektor (var), Teilvektor (par) und Vollvektor
(full) Ähnlichkeiten, gruppiert nach Klassifikator (clf): Decision Tree (dt),
SVM mit RBF-Kernel (svmrbf) und SVM mit Linearkernel (svmlinear).
](./images/basevsparvsfull/var_par_full_prc.pdf){#fig:baseparfull_prc}

Der Grund für das Versagen der Klassifikatoren, bei der Auswahl der freien
Parameter des Klassifikators, liegt vermutlich an der reduzierten Menge von
Ähnlichkeitswerten, die der MDySimIII für Kandidatenpaare zurückgibt, da nur
Ähnlichkeiten für Attribute angegeben werden, die einen gemeinsamen Block haben.
Beim Blick auf die Kandidatenmenge, wurde festgestellt, dass dies für fast alle
Paare lediglich ein Attribut ist. Damit der Ähnlichkeitsvektor in mehr Stellen
besetzt ist wurde der MDySimIII in zwei Varianten modifiziert. Die erste
Variante berechnet die fehlenden Ähnlichkeitswerte, der Attribute die Teil des
Blocking Schema sind. Die zweite Variante berechnet alle fehlenden
Ähnlichkeitswerte, sodass der Vektor anschließend vollbesetzt ist. Das
ursprüngliche Verfahren, mit variable besetzem Vektor, wird im Folgenden als
Varvektor (par) bezeichnet, die erste neue Variante als Teilvektor (par) und die
zweite neue Variante als Vollvektor (full). Zum Vergleich wurde jeweils ein
Kontrollexperiment (KE) ohne Klassifikator, aber mit entsprechender
Ähnlichkeitsberechnung durchgeführt. Die Ähnlichkeitsberechnung beeinflusst
hierbei lediglich die Laufzeit, da ohne Klassifikator die Ähnlichkeiten nicht
weiter verwendet werden und die Kandidatenmenge $C$ der Ergebnismenge $R$
entspricht. Die folgenden sechs Konfigurationen werden hierzu evaluiert:

* Var-KE, MDySimIII ohne Klassifikator mit vorausberechneten Ähnlichkeiten
* Teil-KE, MDySimIII ohne Klassifikator mit Blocking Schema
  Attributsähnlichkeitsberechnung
* Voll-KE, MDySimIII ohne Klassifikator mit voller Ähnlichkeitsberechnung
* Varvektor, MDySimIII mit Klassifikator mit vorausberechneten Ähnlichkeiten
* Teilvektor, MDySimIII mit Klassifikator mit Blocking Schema
  Attributsähnlichkeitsberechnung
* Vollvektor, MDySimIII mit Klassifikator mit voller Ähnlichkeitsberechnung

Alle sechs Konfigurationen nutzen dazu dieselbe Ground Truth mit
vorklassifizierten Matches, dasselbe Blocking Schema, sowie die gleichen vom
Similarity Lerner bestimmten Ähnlichkeitsmetriken. Des Weiteren wurden
Varvektor, Teilvektor und Vollvektor jeweils mit Decision Tree (dt), SVM mit
RBF-Kernel (svmrbf) und SVM mit Linearkernel (svmlinear) getestet, um die
Ergebnisse sinnvoll miteinander vergleichen zu können.

![Anfragen pro Sekunde
](./images/basevsparvsfull/MDySimIII_query_ips.pdf){#fig:baseparfull_qps
width=100%}

@fig:baseparfull_prc gruppiert die Precision-Recall Kurven von Varvektor,
Teilvektor und Vollvektor nach Klassifikator. In jeder Kurve ist durch einen
Punkt das Recall-Precision Paar hervorgehoben, dass anhand der im entsprechenden
Klassifikator benutzen Wahrscheinlichkeitsschwelle erreicht wurde. Das
Recall-Precision Paar der Kontrollexperimente ist jeweils durch einen schwarzen
Punkt am Ende der Kurven gekennzeichnet. Auf den drei Varvektorkurven sind
insgesamt nur vier Punkte zu erkennen. Jeder Punkt entspricht einer
Wahrscheinlichkeitsschwelle, das bedeutet, dass die auf den Ähnlichkeiten
berechneten Wahrscheinlichkeiten, der Zugehörigkeit zu den Matches, sich auf
vier Wahrscheinlichkeiten beschränkt.. Der Grund dafür liegt im erlernten
Blocking Schema: [(`Vorname`, `ID`) $\land$ (`Nachname`, `ID`)] $\lor$
[(`Zweitname`, `ID`) $\land$ (`Strasse`, `ID`)]. Dieses besteht ausschließlich
aus ID Prädikaten. Folglich werden nur Datensätze mit gleichen Attributen
gruppiert, deren berechnete Ähnlichkeit offensichtlich 1.0 ist. Dementsprechend
sind die Ähnlichkeitsvektoren der Kandidaten für Vorname und Nachname mit 1.0
besetzt oder für Zweitname und Strasse oder beides. Der vierte Wert ist eine 1.0
nur für die Strasse. Dieser kommt zustande, wenn beide zwei Datensätze eine
Übereinstimmung in der Strasse haben und keine Zweitnamen besitzen. In diesem
Fall besteht der Blockschlüssel nur aus dem Strassenamen und die Ähnlichkeit bei
mindestens einem leerem Attribut ist per Definition immer 0. Dementsprechend
bekommt der Klassifikator nur ungenügend Informationen, um eindeutig die
Non-Matches herauszufiltern, was die schlechten Werte aus @sec:fp_fusion
erklärt. Die drei Kurven der Varvektoren zeigen zudem, dass es nicht möglich ist
einen besseren Wert anzunehmen, als der des Kontrollexperimentes. Im Gegensatz
dazu bietet sowohl Teilvektor als auch Vollvektor dem Klassifikator deutlich
mehr Unterscheidungsmerkmale. Dies lässt sich in den Precision-Recall Kurven
daran erkennen, dass deren Kurven mit deutlich mehr Punkten besetzt sind.
Überraschend ist, dass der Teilvektor, trotz weniger Ähnlichkeiten, für den
Decision Tree und die SVM mit RBF-Kernel besser abschneidet, als der Vollvektor
und für die SVM mit Linearkernel eine fast identische Kurve hat. Die beiden SVMs
mit Teilvektor sind dazu dem Decision Tree deutlich überlegen, welcher über 20 %
an Recall verliert und die Precision lediglich um 11 % steigern kann. Bei den
SVMs mit Teilvektor liegt der Recall verlust zwar auch bei über 20 %, allerdings
ist die Precision mit ca. 60 % eine deutliche Verbesserung. Zudem lässt sich in
den Kurven ablesen, dass die Precision des Klassifikator, durch Kalibrierung der
Wahrscheinlichkeitsschwelle, auf bis zu 80 % bei fast gleichem Recall optimiert
werden kann.

In @fig:baseparfull_qps werden die Konsequenzen auf die Effizienz in Anfragen
pro Sekunde, der verschieden Konfiguration dargestellt. Mit deutlichem Abstand
am meisten Anfragen pro Sekunde können durch Var-KE erzielt werden. Bei Par-KE
sind es bereits ca. 70 % weniger und bei Full-KE ganze 80 % weniger Anfragen pro
Sekunde. Aber nicht ausschließlich die Ähnlichkeitsberechnung benötigt viel
Zeit, wie bei den nicht Kontrollexperimenten zu sehen ist, bremst die
Klassifikation ebenfalls erheblich, sogar stärker als die
Ähnlichkeitsberechnung. Die Aufwand für die Klassfikation ist unabhängig davon,
an wie vielen Stellen der Ähnkeitsvektor besetzt ist. Die beste Performanz
bieten entsprechend die Varvektoren, welche jedoch eine extrem schlechte
Qualität haben. Die Teilvektoren sind über die Hälfte langsamer als die
Varvektoren, aber ca. 70 % schneller als die Fullvektoren. Des Weiteren sind die
Qualitätswerte mindestens genauso gut, z.T. sogar besser als die der
Fullvektoren.

Beim Auswerten der Daten gab es eine Unstimmigkeit. Und zwar wurden als
Ähnlichkeitsmaß für alle Attribute immer Bag-Distanz gewählt. Insgesamt wurden
in dem Similarity Lerner die vier Metriken Bag-Distanz, Levenshtein-Distanz,
Damerau-Distanz und Jaro-Distanz zur Auswahl übergeben. Ein genauer Blick auf
die vom Similarity Lerner berechnete Average Precision zeigt, dass diese pro
Attribut für jedes Ähnlichkeitsmaß den gleichen Wert hat. Dementsprechend wurde
immer die zuerst getestete Metrik ausgewählt. Dieses Verhalten des Similarity
Lerners wird in @sec:simlearnvsmanual genauer analysiert.

## Einfluss der Ähnlichkeitmetriken {#sec:esim_metrik}

![Precision-Recall Kurven von 7 Ähnlichkeitsmetriken gruppiert nach
Klassifikator (Zeilen) und Vektortyp (Spalten). Durch einen Punkt ist die
aktuelle Wahrscheinlichkeitsschwelle des Klassifikators hervorgeboben.
](./images/simtest/par_full_prc.pdf){#fig:sim_prc}

Die Auswertung der Ähnlichkeitsmetriken in @sec:base_par_full zeigt, dass der
Similarity Lerner für jedes Attribute für die meisten Metriken denselben Average
Precision Wert berechnet. Daraus folgt, dass entweder die Ähnlichkeitmetriken
keinen oder nur geringen Einfluss auf das Urteilsvermögen des Klassifikators
haben, oder dass die Average Precision kein geeignetes Maß ist, um eine
Ähnlichkeit auszuwählen. Infolgedessen wird der Einfluss der
Ähnlichkeitsmetriken geprüft, indem pro Durchlauf jeweils eine Metrik für alle
Attribute manuell bestimmt wird. Dabei werden die Durchläufe für jeden der drei
Klassifikatoren wiederholt. In @sec:base_par_full wurde bzgl. Qualität und
Effizienz eine Tendenz zu den Teilvektoren gegenüber den Vollvektoren
festgestellt. Zum Zweck diese Tendenz zu bestärken bzw. zu entkräften wird jeder
der Klassifikatoren einmal mit dem Teilvektor und einmal mit dem Vollvektor
geprüft. Für die Analyse wurde die Damerau-Distanz weggelassen, da sich deren
berechnete Ähnlichkeiten bei genauer Betrachtung zu fast 100 % mit der
Levenshein-Distanz deckt. Zusätzlich wurden ausführlichkeitshalber 4 weitere
Ähnlichkeitmetriken hinzugenommen:

* Bag-Distanz
* Compression-Distanz
* Hamming-Distanz
* Jaro-Distanz
* Jaro-Winkler-Distanz
* Jaccard-Koeffizent
* Levenshtein-Distanz

![PRC](./images/simtest/par_learn_prc.pdf){#fig:sim_prc_learn width=80%}

Bei 7 Ähnlichkeitsmetriken drei 3 Klassifikatoren und 2 Vektortypen, wurden
insgesamt 42 Durchläufe ausgeführt und ausgewertet. Des Weiteren wurde für alle
Durchläufe dieselbe Ground Truth mit vorklassifizierten Matches und dasselbe
Blocking Schema verwendet. Das Blocking Schema ist erneut [(`Vorname`, `ID`)
$\land$ (`Nachname`, `ID`)] $\lor$ [(`Zweitname`, `ID`) $\land$ (`Strasse`,
`ID`)]. Die für den Similarity Learner und Klassifikator interessante gefilterte
Ground Truth besteht aus 48.256 Matches, 61.764 Non-Matches.

Das Ergebnis ist in den Precision-Recall Kurven in @fig:sim_prc zu sehen. Dabei
wurden die Kurven für die Ähnlichkeitsmetriken jeweils nach Klassifikator und
Vektortyp gruppiert. In der linken Spalte sind die Vollvektoren, in der rechten
die Teilvektoren und in den Zeilen von oben nach unten der Decision Tree, die
SVM mit Linearkernel und die SVM mit RBF-Kernel. Die Vermutung, dass der
Einfluss der Ähnlichkeitsmetriken keiner bzw. gering ist, kann auf einen Blick
wiederlegt werden. Beispielsweise ist im Plot für die SVM mit Linearkernel und
Teilvektor eine deutlicher Unterschied zwischen den Kurven der
Compression-Distanz und der Jaro-Winkler-Distanz zu erkennen, womit erwiesen
ist, dass diese Metriken entscheidenden Einfluss auf das Urteilsvermögen des
Klassifikators haben. Des Weiteren bestätigt der Vergleich der Vollvektorkurven
mit den Teilvektorkurven die Entscheidung zu letzterem, da dieser dem
Klassifikator bessere Werte liefert und somit die Kurven mindestens gleich,
meist aber besser sind. Insgesamt schneidet die SVM mit Linearkernel am besten
ab. Allerdings sind die vom Klassifikator genutzten Wahrscheinlichkeitsschwellen
(hervorgehobener Punkt auf der Kurve) nicht optimal. Beispielsweise können die
Jaro-Distanz und die Jaro-Winkler-Distanz von einer Kalibrierung der Schwelle
massiv profitieren, weil dadurch die Precision bei fast gleichem Recall um ca.
40 % verbessert werden kann.

Die zweite Vermutung, dass die Average Precision keine geeignete Metrik ist, um
Ähnlichkeiten auszuwählen, wurde überprüft, indem die Durchläufe mit Teilvektor
wiederholt wurden und diesmal mit allen 7 Ähnlichkeitmetriken. Die Ground Truth
und das Blocking Schema sind gleich zu den 42 Durchläufen zuvor. Die
ausgewählten Ähnlichkeitsmetriken sind: Vorname $\rightarrow$ Jaccard-Distanz,
Nachname $\rightarrow$ Jaro-Winkler-Distanz, Zweitname $\rightarrow$
Jaccard-Distanz und Strasse $\rightarrow$ Bag-Distanz. In @tbl:simlearn_avp sind
die errechneten Average Precision Werte pro Attribut aufgelistet. Die
Compression-Distanz fällt hierbei aus dem Rahmen und hat deutlich schlechtere
Werte, als der Rest. Die Werte für Bag-Distanz, Jaro-Distanz und
Levenshtein-Distanz sind wie zuvor identisch. Für die Hamming-Distanz,
Jaccard-Distanz und Jaro-Winkler-Distanz gibt es dazu leicht aufweichende Werte,
was zur Auswahl unterschiedlicher Metriken geführt hat. In @fig:sim_prc_learn
sind die drei Kurven der gelernten Ähnlichkeitsmaße, jeweils der besten Kurve
des entsprechenden Klassifikator aus @fig:sim_prc gegenübergestellt. Aufgrund
der Auswahl sind die Kurven diesmal deutlich besser. Für die SVMs ist die Kurve
beinahe so gut, wie die beste Kurve und für den Decision Tree ist die Kurve
sogar deutlich besser. Aufgrunddessen ist die Average Precision ein geeignetes
Maß, wenn genügend Metriken zur Auswahl stehen.

| Metrik       | Vorname | Nachname | Zweitname | Strasse |
|--------------+---------+----------+-----------+---------|
| Bag          | 0.7111  | 0.6982   | 0.7121    | 0.4295  |
| Compression  | 0.4286  | 0.4620   | 0.4174    | 0.3850  |
| Hamming      | 0.7111  | 0.6999   | 0.7121    | 0.4288  |
| Jaccard      | 0.7198  | 0.6995   | 0.7130    | 0.4269  |
| Jaro         | 0.7111  | 0.6989   | 0.7121    | 0.4295  |
| Jaro-Winkler | 0.7112  | 0.7005   | 0.7121    | 0.4289  |
| Levenshtein  | 0.7111  | 0.6989   | 0.7121    | 0.4295  |

: Tabelle mit Average Precisions für 7 Ähnlichkeismetriken pro Attribut
{#tbl:simlearn_avp}

## Human Baseline

* Train/Train
* Train/Test

## Grund Truth vs No Ground Truth

* Train/Train
* Train/Test
