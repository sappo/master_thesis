# Evaluation der Implementierung

In diesem Kapitel wird die Programmierumgebung, die Implementierung der Engine
und die Implementierung der Komponenten vorgestellt. Eine große Herausforderung
bei der Umsetzung der Algorithmen war es, diese für die begrenzten Ressourcen,
insbesondere Arbeitsspeicher und Rechenzeit, zu optimieren.

## Programmierumgebung

Als Programmiersprache für die Implementierung wurde Python und C eingesetzt.
Wobei C lediglich zur Implementierung der Ähnlichkeitsberechnung eingesetzt
wurde, alle anderen Teile wurden mit Python umgesetzt. Python hat den Vorteil,
dass es sehr einfach und schnell möglich ist, einen Prototypen eines
Algorithmuses zu entwickeln und zu testen. Zudem gibt es eine Vielzahl von
Qualitativ hochwertigen Paketen, die komkompfortable Standardfunktionalitäten
bereitstellen, beispielsweise das Einlesen und das Schreiben von großen
CSV-Dateien oder das Plotten von Graphen. Des Weiteren wird Python im Maschine
Learning Bereich oft genutzt, was dazu führt, dass es eine Vielzahl von
effizienten, ausgereiften und umfangreichen Frameworks gibt, um verschiedenste
Lernaufgaben zu behandeln. Vor allem der Fusion-Lerner und der Klassifikator
profitieren hiervon.

Der große Nachteil von Python ist das Global Interpreter Lock (GIL). Dieses
verhindert, dass Python-Code in mehreren Threads gleichzeitig ausgeführt werden
kann. Die Multithreading Bibliothek von Python ist daher lediglich geeignet, um
Programme mit hoher E/A-Last zu beschleunigen, da Schreib- bzw. Lesezugriffe das
GIL freigeben. Der Grund warum in Python ein GIL einzusetzen wird ist, dass
dadurch die Single-Thread Ausführung optimiert wird. Multithreading, im Sinne
von Gleichzeitigausführung, d.h. ein Prozess mit mehreren Threads, die auf
verschiedenen Prozessorkernen, zur selben Zeit ausgeführt werden, wird dadurch
allerdings komplett unterbunden. Um denoch Python zu paralelisieren gibt es zwei
beliebte Möglichkeiten. Die erste Möglichkeit ist, statt Multithreading,
Multiprocessing einzusetzen. Das hat allderdings den Nachteil, dass Daten
zwischen Prozessen ausgetauscht werden müssen. Das lohnt sich offensichtlich nur
für rechenintensive Aufgaben, wo der Overhead des Datenaustausches keine Rolle
spielt. Die zweite Möglichkeit ist das Multithreading in einer anderen
Programmiersprache umzusetzen, beispielsweise in C. Dies ist möglich, da das GIL
lediglich die Mehrfachausführung von Python-Code verhindert. Allerdings erweist
sich dies oft als relativ schwierig, da selbst einfache Datenklassen,
beispielsweise `set` oder `dict`, keine Entsprechung in C haben und daher
manuell, in beide Richtungen Python zu C und C zu Python, z.T. aufwendig
konvertiert werden müssen.

Aufgrund der genannten Nachteile von Python wurde die Engine und sämtliche
Komponenten lediglich in einem Thread ausgeführt. Ideen dies zu optimieren
konnten nicht im Zeitrahmen der Thesis umgesetzt werden. Dabei kann vor allen in
der Fit-Phase durch Multi-Threading und Parallel Programming Laufzeit eingespart
werden. Die längste Laufzeit haben dabei der DNF Blocks Lerner und der
Fusion-Lerner.

## Label Generator

Der Label Generator wurde gegenüber dem Algorithmus \ref{alg:weaklabels} von
Kejriwal & Mirankern [@KM:Unsupervised:13] und dessen Anpassung mit Ground Truth
Matches in Algorithmus \ref{alg:labels} in zwei Punkten modifiziert. Zunächst
werden die Datensätze in den Blöcken alphabetisch sortieren. Damit ist es
möglich deterministische Ergebnisse zu bekommen und daraufbasierend geeignete
Testfälle zu schreiben. Des Weiteren werden wie beim klassischen Sorted
Neighborhood Verfahren, dadurch ähnliche Datensätze näher zusammengebracht, was
die Wahrscheinlichkeit erhöht aussagekräftige Paare zu selektieren. Die zweite
Anpassung ist sowohl eine Laufzeit-, als auch ein Arbeitsspeicheroptimierung.
Ähnlich zum Record Identifier Index der Similarity-Aware Inverted-Index
Verfahren, kann es durch das Blocking auf Basis der Token ebenfalls dazu kommen,
dass rießige Blöcke erzeugt werden. Selbst wenn ähnliche Attribute durch
Sortierung näher zueinander sortiert wurden, ist in diesen Blöcken ein größes
Fenster nötig, um aussagekräftig Paare zu finden. Dies wiederum führt zu einer
Explosion der Kandidatenmenge und damit des Arbeitsspeichers und der Laufzeit.
Zur Optimierung wird ein Blockfilter eingeführt, sodass lediglich Kandidaten in
Blöcken generiert werden, deren Anzahl an Datensätzen kleiner einer Schwelle $z$
sind.

## DNF Blocks Learner

Die Algorithmen des DNF Blocks Lerners haben bei der Implementierung das
Problem, das nur eine bestimmte Menge an Arbeitsspeicher zur Verfügung steht.
Der kritische Teil des Algorithmus ist die Erzeugung der Paarkombinationen, für
jeden Block. Angenommen die beiden Datensatzidentifier eines Paares $(p1.id,
p2.id)$ sind Integerwerte und der Datensatz hat nicht mehr als 2^30 Einträge,
dann benötigt ein Integerwert 28 Bytes. Um möglichst effizient auf die Paare
zuzugreifen, ist die Menge von Paarkombinationen als `set` implementiert. Damit
ein `set` $s$ ein Zugriffkomplexität von $O(1)$ ermöglichen kann, wird für jedes
Element in der Menge ein Hashwert berechnet. Auf einem 64-bit System beträgt die
Größe dieses Hashwertes $h$ 8 Bytes. Somit benötigt ein Eintrag $(h_j, p1_j.id,
p2_j.id) \in s$ 64 Bytes. Angenommen es werden für einen Block mit 1.000
Einträgen Paarkombinationen erzeugt. Bei Attributen mit wenigen möglichen Werten
können Blöcke entstehen, die sehr viele Datensätze enthalten. Beispielsweise hat
ein Block mit 10.000 Einträgen 49.995.000 Paare und benötigt 2.9 GB an
Arbeitsspeicher. Somit kann bereits ein rießiger Block den zur Verfügung
stehenden Arbeitsspeicher sprengen und führt damit zum Abbruch des Programmes.
Aus diesem Grund wurde der Algorithmus dahingehend erweitert, dass die Erzeugung
der Paare bei Ausdrücken, die zu viele Paare erzeugen würden, unterbunden wird
und dies Ausdrücke mit der niedrigsten Wert der Bewertungsskala bewertet werden.

Zur genaueren Analyse des Problems, wird die Verteilung der Blöcke, anhand ihrer
Größe (Anzahl von Datensätzen), betrachtet. Um die Verteilungen in Gute,
benötiget weniger Arbeitsspeicher als zur Verfügung steht und Schlechte,
benötiget mehr Arbeitsspeicher als zur Verfügung steht, zu kategorisieren, wurde
eine Schwelle $t$ eingeführt. Anhand dieser Schwelle wird ein Block $B$ bei $|B|
< t$ als guter Block und bei $|B| > t$ als schlechter Block bewertet. Daraus
kann für jede Verteilung berechnet werden, wie viel Prozent gute bzw. schlechte
Blöcke es gibt. Dadurch ist es möglich bei Audrücken mit einer höheren
schlechten Blockrate von $b$, beispielsweise $b=0.1$, die Erzeugung der
Blockpaare zu verhindern und die weitere Verarbeitung abzubrechen. Da aber
bereits ein einziger schlechter Block, mit genügend Einträgen, den
Arbeitsspeicher überfüllen kann, wird mit der Schwelle $b$ lediglich eine
Vorauswahl, besonders schlechter Ausdrücke, getroffen. Für den Fall, dass es nur
wenige schlechte Blöcke gibt, bestehen deren Blockschlüssel meistens aus
Stopwörtern, beispielsweise bei Strassennamen `Strasse`, `Weg`, oder `Platz`.
Dieses Problem kann folglich durch eine bessere Vorverarbeitung der Daten gelöst
werden. Da es das Ziel ein selbstkonfigurierendes System ist, muss die Engine,
die auf diese Weise gefundenen Stopwörter nutzen und den Lernvorgang mit der
erweiterten Vorverarbeitung der Daten wiederholen. Dieser Prozess sorgt
allerdings dafür, dass das Lernen der Konfiguration deutlich länger dauert. Eine
einfacherere Möglichkeit ist, für jeden Ausdruck eine Liste mit verbotenen
Blockschlüsseln anzulegen und die Blockschlüssel schlechter Blöcke dort
hinzuzufügen. In der Build- und Query-Phase dürfen diese Blöckschlüssel vom
Indexer demnach nicht genutzt werden.

Trotz dieser Optimierungen hat der DNF Blocks Generator immer noch hohe
Arbeitsspeicheranforderungen, welche verhindern das Multithreading oder
Multiprocessing auf einem Rechner zur Laufzeitoptimierung eingesetzt werden
können. Denkbar ist aber die Verteilung auf ein Cluster von Rechnern,
beispielsweise per Hadoop, wodurch deutlich mehr Prädikate in kürzerer Zeit
überprüft werden können.

## Fusion-Lerner und Klassifikatoren

Der Fusion-Lerner und die Klassifikatoren sind eng miteinander verbandelt. Um
die Hyperparameter der Klassifikatoren zu lernen muss dieser insbesondere
Wissen, wie deren API Schnittstelle ist, damit er Modelle trainieren und
auswerten kann. Aufgrunddessen und weil die Implementierung von verschiedenen
Klassifikatoren nicht Schwerpunkt der Thesis ist, wurde für die Umsetzung die
Python Maschine Learning Bibliothek Scikit-learn [@PVG.EA:Scikitlearn:11]
eingesetzt. Diese bietet ein breites Spektrum an Funktionen:

* Klassifikation, bestimmen zu welcher Klasse ein Objekt gehört.
* Regression, einen fortlaufenden Wert eines Objektes vorhersagen.
* Clustering, automatisches gruppieren von Objekten.
* Dimensionsreduktion, reduzieren der Anzahl zu betrachtender zufälliger
  Variablen.
* Modellauswahl, vergleichen, validieren und auswählen von Parametern und
  Modellen.
* Vorverarbeitung, Eingabetransformation und Normalisierung.
* Evaluation, berechnen der Effizienz und Qualität von Modellen.

Für den Fusion-Lerner sind dabei das Module zur Modellauswahl und Evaluation
interessant. Dieser ist zudem die einzige Komponente, die ihre Aufgabe
parallelisieren kann, da dies in Scikit-learn transparent implementiert ist. Von
der Engine bekommt der Fusion-Lerner eine Liste von Klassifikatoren mit
entsprechenden, möglichen Parametern übergeben. Für jeden Klassifikator findet
der Fusion-Lerner, anhand eines geeigneten Qualtiätsmaßes, die beste
Konfiguration. Die einfachste Möglichkeit dafür ist eine Grid Search. Diese
erzeugt ein Parameternetz aller möglichen Parameterkombinationen und sucht
dieses vollständig ab. Dadurch ist zwar sichergestellt, dass die besten
Parameter gefunden werden, je mehr Parameter es gibt, desto länger dauert diese
Suche allerdings. Eine einfache Möglichkeit den Suchraum zu reduzieren ist, per
Zufall nur eine bestimmte Anzahl an Parameterkombinationen zu bestimmten und nur
diese zu vergleichen. Neben diesen beiden dynamischen Methoden zum Lernen der
Hyperparamter, welche modellübergreifend funktionieren, gibt es in Scikit-learn
auch spezialisierte Parametersuchen, beispielsweise für Logistic Regression,
welche effizienter die Parameter für ihr Klassifikationsmodell finden. Der
Fusion-Lerner sucht für jeden Klassifikator seperat die besten Parameter. Dabei
wird das Ergebnis der besten Konfiguration mit den besten Konfigurationen
anderer Klassifikatoren verglichen. Zum Schluss wird der Klassifikator
ausgewählt, dessen beste Konfiguration, bei gegeben Qualitätsmaß das beste
Ergebnis liefert.

Egal welches Suchverfahren genutzt wird, wichtig ist, dass beim Vergleichen,
Validieren und Auswählen der Parameter bzw. Modelle darauf geachtet wird, dass
das Modell nicht überanpasst und dadurch ausschließlich auf den Trainingsdaten
gute Ergebnisse erzielt werden. Um dies zu unterbinden, werden
Kreuzvalidierungsverfahren genutzt, welche ebenfalls in Scikit-learn
implementiert sind. Dabei unterscheidet man zwischen vollständiger
Kreuzvalidierung und nicht-vollständiger Kreuzvalidierung. Ein Beispiel für die
vollständige Kreuzvalidierung ist Leave-one-out. Dabei werden aus den
Trainingsdaten mit $n$ Objekten $n$ Untermengen gebildet, bei welchen jeweils
ein Element fehlt. Eine Menge wird als Validierungsmenge ausgewählt, anhand
welcher ein trainiertes Modell überprüft wird. Die anderen werden als
Trainingsdaten genutzt. Das Verfahren wird $n$-Mal wiederholt, bis jede Menge
als Validierungsmenge genutzt wurde. Das Ergebnis ist das Mittel aller
Durchläufe. Da dies z.T. sehr lange dauert, gibt es nicht vollständige Verfahren
wie das K-Fold. Dieses bildet zufällig $k$ gleichmächtige Untermengen der
Trainingsdaten. Eine dieser Mengen wird, analog zum Leave-one-out, zum
Validieren ausgewählt. Die anderen $k-1$ Mengen werden als Trainingsdaten
genutzt. Das ganze wird $k$-Mal wiederholt, bis jede Menge einmal als
Validierungsmenge genutzt wurde. Das Ergebnis der $k$ Durchläufe wird ebenfalls
gemittelt und als ein Wert zurückgegeben. Ist $k=n$ entspricht K-Fold dem
Leave-one-out Verfahren. Eine beliebte Erweiterung des K-Fold ist der Stratified
K-Fold. Dieser unterscheidet sich lediglich in der Generierung der Untermengen.
Dabei werden Objekte ebenfalls zufällig aus der Trainingsmenge selektiert,
jedoch wird darauf geachtet, dass das Verhältnis der Klassenzugehörigkeit der
Objekte bestehen bleibt. Befinden sich in der Ausgangstrainingsmenge,
beispielsweise 30% Matches und 70% Non-Matches, dann haben alle $k$ Untermengen
ebenfalls dieses 30% zu 70% Verhältnis. Damit wird sichergestellt, dass jede
Untermenge eine gute Repräsentation des Ganzen ist und folglich die Ergebnisse
mehr Aussagekraft haben.

Daraus ergeben sich drei freie Parameter für den Fusion-Lerner, welche in
@sec:free_params ausgewählt werden. Zunächst die Suchverfahren Grid Search
oder Randomized Search. Spezialisierte Verfahren werden nicht näher betrachtet.
Anschließend die Kreuzvalidierung, aus Effizienzgründen, K-Fold oder Stratified
K-Fold. Sowie das Qualitätsmaß zur Bewertung der trainierten Modelle,
beispielsweise das F-measure.

Die als Klassifikator nutzbaren Komponenten müssen zu den Suchverfahren
kompatibel sein. Das Scikit-learn Klassifikationsmodul beinhaltet dazu SVMs,
DecisionTrees oder neuronale Netze. Diese Implementierungen können ohne
Anpassungen mit den Scikit-learn Suchverfahren verwendet werden. Die
automatische Auswahl von Klassifkatoren und möglicher Parameter ist nicht
Bestandteil dieser Thesis. Weshalb die Klassifikatoren ebenfalls freie Parameter
sind, die in @sec:free_params für die Evaluierung festgelegt werden.
