% **************************************************
% Document Class Definition
% **************************************************
\documentclass[%
	paper=A4,					% paper size --> A4 is default in Germany
	twoside=true,				% onesite or twoside printing
	openright,					% doublepage cleaning ends up right side
	parskip=full,				% spacing value / method for paragraphs
	chapterprefix=true,			% prefix for chapter marks
	11pt,						% font size
	headings=normal,			% size of headings
	bibliography=totoc,			% include bib in toc
	listof=totoc,				% include listof entries in toc
	titlepage=on,				% own page for each title page
	captions=tableabove,		% display table captions above the float env
	draft=false,				% value for draft version
]{scrreprt}%

% **************************************************
% Debug LaTeX Information
% **************************************************
%\listfiles

% **************************************************
% Information and Commands for Reuse
% **************************************************
\newcommand{\thesisTitle}{The Clean Thesis Style}
\newcommand{\thesisName}{Ricardo Langner}
\newcommand{\thesisSubject}{Documentation}
\newcommand{\thesisDate}{August 26, 2015}
\newcommand{\thesisVersion}{My First Draft}

\newcommand{\thesisFirstReviewer}{Jane Doe}
\newcommand{\thesisFirstReviewerUniversity}{\protect{Clean Thesis Style University}}
\newcommand{\thesisFirstReviewerDepartment}{Department of Clean Thesis Style}

\newcommand{\thesisSecondReviewer}{John Doe}
\newcommand{\thesisSecondReviewerUniversity}{\protect{Clean Thesis Style University}}
\newcommand{\thesisSecondReviewerDepartment}{Department of Clean Thesis Style}

\newcommand{\thesisFirstSupervisor}{Jane Doe}
\newcommand{\thesisSecondSupervisor}{John Smith}

\newcommand{\thesisUniversity}{\protect{Clean Thesis Style University}}
\newcommand{\thesisUniversityDepartment}{Department of Clean Thesis Style}
\newcommand{\thesisUniversityInstitute}{Institut for Clean Thesis Dev}
\newcommand{\thesisUniversityGroup}{Clean Thesis Group (CTG)}
\newcommand{\thesisUniversityCity}{City}
\newcommand{\thesisUniversityStreetAddress}{Street address}
\newcommand{\thesisUniversityPostalCode}{Postal Code}

% **************************************************
% Load and Configure Packages
% **************************************************
\usepackage[utf8]{inputenc}		% defines file's character encoding
\usepackage[ngerman]{babel} % babel system, adjust the language of the content
\usepackage[					% clean thesis style
	figuresep=colon,%
	sansserif=false,%
	hangfigurecaption=false,%
	hangsection=true,%
	hangsubsection=true,%
	colorize=full,%
	colortheme=bluemagenta,%
	bibsys=bibtex,%
	bibfile=bib-refs,%
	bibstyle=alphabetic,%
]{cleanthesis}

\hypersetup{					% setup the hyperref-package options
	pdftitle={\thesisTitle},	% 	- title (PDF meta)
	pdfsubject={\thesisSubject},% 	- subject (PDF meta)
	pdfauthor={\thesisName},	% 	- author (PDF meta)
	plainpages=false,			% 	-
	colorlinks=false,			% 	- colorize links?
	pdfborder={0 0 0},			% 	-
	breaklinks=true,			% 	- allow line break inside links
	bookmarksnumbered=true,		%
	bookmarksopen=true			%
}

% Sort lot and lof by chapter
%\makeatletter
%\newcommand{\saved@chapter}{}
%\let\saved@chapter\chapter
%\renewcommand{\chapter}{%
  %\@ifstar {\saved@chapter*}{\@dblarg\my@chapter}%
%}
%\newcommand*{\my@chapter}[2][]{%
  %\saved@chapter[#1]{#2}%
  %\global\setbool{newchap}{true}
%}
%\makeatother

% **************************************************
% Additional pandoc Packages and Config
% **************************************************

\usepackage{subfig}
\AtBeginDocument{%
\renewcommand*\figurename{Figure}
\renewcommand*\tablename{Table}
}
\AtBeginDocument{%
\renewcommand*\listfigurename{List of Figures}
\renewcommand*\listtablename{List of Tables}
}
\usepackage{float}
\floatstyle{ruled}
\makeatletter
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\makeatother
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{42,33,28}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.26,0.66,0.93}{\textbf{{#1}}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{\underline{{#1}}}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.27,0.67,0.26}{{#1}}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.27,0.67,0.26}{{#1}}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.27,0.67,0.26}{{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.02,0.61,0.04}{{#1}}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.02,0.61,0.04}{{#1}}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.02,0.61,0.04}{{#1}}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.02,0.61,0.04}{{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.02,0.61,0.04}{{#1}}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.00,0.40,1.00}{\textit{{#1}}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.00,0.40,1.00}{\textit{{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.00,0.40,1.00}{\textbf{\textit{{#1}}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{1.00,0.58,0.35}{\textbf{{#1}}}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.26,0.66,0.93}{\textbf{{#1}}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{\textbf{{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.00,0.40,1.00}{\textbf{\textit{{#1}}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{1.00,1.00,0.00}{\textbf{{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,1.00,0.00}{{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,1.00,0.00}{\textbf{{#1}}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.74,0.68,0.62}{{#1}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
    \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% **************************************************
% Document CONTENT
% **************************************************
\begin{document}

% --------------------------
% rename document parts
% --------------------------
\renewcaptionname{ngerman}{\figurename}{Abb.}
\renewcaptionname{ngerman}{\tablename}{Tab.}
\renewcommand\listfigurename{}  % Clear default header
\renewcommand\listtablename{}   % Clear default header
\renewcommand\listoflistings{\listof{codelisting}{Auflistungsverzeichnis}}
%\renewcaptionname{english}{\figurename}{Fig.}
%\renewcaptionname{english}{\tablename}{Tab.}

% --------------------------
% Front matter
% --------------------------
\pagenumbering{roman}			% roman page numbing (invisible for empty page style)
\pagestyle{empty}				% no header or footers
\input{content/titlepages}		% INCLUDE: all titlepages
\cleardoublepage

%
\input{content/acknowledgement} % INCLUDE: acknowledgement
\cleardoublepage
%
\setcounter{tocdepth}{2}		% define depth of toc
\tableofcontents				% display table of contents
\cleardoublepage

% --------------------------
% Body matter
% --------------------------
\pagenumbering{arabic}			% arabic page numbering
\setcounter{page}{1}			% set page counter
\pagestyle{maincontentstyle} 	% fancy header and footer

\chapter{Einleitung}\label{einleitung}

Die Masterarbeit soll in Zusammenarbeit mit der Firma Detim Consulting
GmbH geschrieben werden. Dazu wird ein Problemfeld bei dem Kunden
Universum Group, der Detim Consulting GmbH, in deren Geschäftsfeld dem
Inkasso-, Liquiditäts-, und Risikomanagement, gewählt.

\chapter{Problemfeld Universum Group}\label{problemfeld-universum-group}

Die Universum Group bietet Lösungen für Onlineshops zur Bonitäts- und
Adressprüfung, sowie dem Forderungsankauf, der Onlineshop-Kunden. Dabei
wird dem Händler bei entsprechender Bonität seines Kunden das Angebot
gemacht, die Forderung, nach Ablauf einer Zahlungsperiode, zu 100~\% zu
übernehmen. Damit eine möglichst zuverlässige Aussage, über die Bonität
des Kunden, getroffen werden kann, muss zunächst herausgefunden werden,
ob der Kunde bereits bei der Universum Group bekannt ist. Das Problem an
dieser Stelle ist, dass der Kunde Online seine Daten selbst erfasst und
diese nicht anhand von Personalausweis oder ähnlichen Dokumenten
überprüft werden können. Fehler bei der Datenerhebung sind,
beispielsweise unterschiedliche Schreibweisen, insbesondere bei
Adressen, Tippfehler, welche bei Namen nicht offensichtlich sind,
unterschiedliche Konventionen, etwa Str. für Straße, oder akademische
Titel und Adelstitel, welche in Onlineformularen nicht standardisiert
erfasst werden. Bei der Bonitätsprüfung dient die
Personenidentifizierung dazu, Kunden mit positiver oder negativer
Zahlungsmoral zu erkennen und anzunehmen bzw. abzulehnen. Je genauer die
Personenidentifikation ist, desto aussagekräftiger sind die
Bonitätsauskunfte von externen Dienstleister, beispielsweise der Schufa.
Beim Inkassomanagement gilt zudem das sog. Schadensminderungsprinzip.
Das bedeutet, das alle angekauften Forderungen eines Kunden nur einmalig
abgemahnt werden dürfen. Daher müssen hier Personendubletten gefunden
und zusammengeführt werden.

Das aktuelle System zur Personenidentifizierung funktioniert nur bei der
Bonitätsprüfung und ist durch einen externen Dienstleister realisiert.
Dieser bereinigt und prüft Namen und Adressen. Allerdings skaliert das
System dabei nur innerhalb eines vorgegebenen monatlichen Kontingents.

\chapter{Duplikatserkennung}\label{duplikatserkennung}

Die Methoden zur Duplikatserkennung stammen ursprünglich aus dem
Gesundheitsbereich (Felegi \& Sunter 1969). Je nach Fachgebiet gibt es
unterschiedliche Fachbegriffe. Statistiker und Epidemiologen sprechen
von \emph{record} oder \emph{data linkage} während Informatiker das
Problem unter \emph{entity resolution}, \emph{data} oder \emph{field
matching}, \emph{duplicate detection}, \emph{object identification} oder
\emph{merge/purge} kennen. Dabei geht es nicht um die reine
Personenidentifikation, sondern vielmehr um die Identifikation von
Entitäten aller Art, beispielsweise Kunden, Patienten, Produkte oder
Orte. Dabei können die Entitäten nicht durch ein einzigartiges Attribut
identifiziert werden. Zudem sind die Datensätze oft fehlerhaft,
beispielsweise durch Rechtschreibfehler oder unterschiedliche
Konventionen. Die Methoden zur Entitätsauflösung arbeiten meist auf
Datensatzpaaren und liefern als Ergebnis eine Menge von
Übereinstimmungen. Eine Übereinstimmung verknüpft zwei Entitäten.
Zusätzlich kann über einen optionalen Ähnlichkeitswert (engl. similarity
score), normalerweise zwischen 0 und 1, die Intensität der
Übereinstimmung angegeben {[}1{]}.

Zur Bestimmung der Ähnlichkeit eines Datensatzpaares unterscheiden
Elmagarmid et al. {[}2{]} zwischen Attributsvergleichs- (engl. field
matching) und Datensatzvergleichsmethoden (engl. record matching).
Methoden zum Attributsvergleich sind zeichenbasierend (edit distance,
affine gap distance, Jaro distance metric oder Q-gram distance),
tokenbasierend (atomic strings, Q-grams mit tf.idf), phonetisch
(soundex) oder nummerisch. Die Datensatzvergleichsmethoden sind
probabilistisch (Naive Bayes), überwachtes bzw. semi-überwachtes Lernen
(SVMLight, Markov Chain Monte Carlo), aktives Lernen (ALIAS),
distanzebasierend (siehe Attributsvergleich - Datensatz als
konkatenierter String) oder regelbasierend (AJAX). Die Ausführung der
Vergleichsmethoden ist enorm teuer, da diese das Kreuzprodukt zweier
Mengen bilden müssen. Um die Ausführungszeit zu reduzieren wird versucht
den Suchraum auf die wahrscheinlichsten Duplikatsvorkommen zu begrenzen.
Diese Vorgehen werden als Blocking oder Indexing bezeichnet. Elmagarmid
et al. nennen Standard Blocking, Sorted Neighboorhodd Approach,
Clustering und Canopies, sowie Set Joins als Vorgehensweisen.
(Referenzen zu den Methoden folgen noch!)

Da es keine Methode zur Entity Resolution gibt, welche allen anderen
überlegen ist, wurden Ende der 90er Jahre begonnen Frameworks zu
entwickeln, welche verschiedene Methoden miteinander kombinieren. Einen
Vergleich dieser Frameworks wurde durch Köpcke \& Rahm 2010 {[}1{]}
durchgeführt. Ein Framework besteht aus verschiedenen Matchern. Ein
Matcher ist dabei ein Algorithmus, welcher die Ähnlichkeit zweier
Datensätze ermittelt. Ähnlich wie Elmagarmid et al. unterscheiden Köpcke
\& Rahm zwischen attributs- und kontextbasierenden Matchern. Als Kontext
bezeichnen Sie die semantische Beziehung bzw. Hierarchie zwischen den
Attributen. Um die Matcher miteinander zu kombinieren nutzen die
Frameworks min. eine Matching Strategie. Eine Strategie ist, die
Ähnlichkeitswerte verschiedener Matcher nummerisch zu kombinieren,
beispielsweise durch eine gewichtete Summe oder einen gewichteten
Durchschnitt. Ein anderer Ansatz ist regelbasierend. Eine einfache Regel
besteht aus einer logischen Verbindung und einer Match-Kondition,
beispielsweise einem Schwellenwert. Die dritte und komplexeste Strategie
ist workflow-basierend. Hierbei kann beispielsweise eine Sequenz von
Matchern die Ergebnisse iterativ einschränken. Grundsätzlich können
Workflows beliebig komplex werden. Einen passenden Workflow zu finden
kann selbst Domainexperten vor eine große Herausforderung stellen. Daher
gibt es trainingbasierende Ansätze passende Parameter für Matcher oder
Kombinationsfunktionen (z.B. Gewicht für Matcher) zu bestimmen. Solche
Ansätze sind etwa, Naive Bayes, Logistic Regression, Support Vector
Maschine oder Decision Trees. (Referenzen zu den Ansätzen folgen noch!)

Ein Großteil der Forschung in Entity Resolution konzentriert sich auf
die Qualität der Vergleichsergebnisse. Die von Köpcke \& Rahm
verglichenen Frameworks konzentrieren sich alle Samt darauf zwei
statische Mengen zu miteinander vergleichen. Bei großen Datenmengen kann
dies durchaus mehrere Stunden dauern. Daher gibt es in den letzten Jahre
einige Ansätze und Frameworks, welche MapReduce zum Skalieren nutzen
{[}3{]}{[}4{]}. Zudem gibt es immer mehr Bedarf, Vergleichsergebnisse in
nahe Echtzeit zu liefern. Erste Ergebnisse Entity Resolution skalierbar
und in nahe Echtzeit zu erreichen, präsentieren Christen \& Gayler in
{[}5{]} 2008, unter Verwendung von Inverted Indexing Techniken, welche
normalerweise bei der Websuche anwendung finden. Dabei betrachten Sie
vor allem die Anforderungen eines Anfragestroms (engl. query stream).
Ihre Anforderungen sind einen Strom von Anfragedatesätzen, gegen
potentielle riesige Datenmengen, im Subsekundenbereich pro Anfrage
abzuarbeiten. Dabei sollen die Treffer der Anfrage mit einem
Ähnlichkeitswert versehen sein. Zudem muss es möglich sein die Menge an
Anfragen zu skalieren. Das Hauptproblem ist hierbei die Skalierung. Um
skalieren zu können wird versucht die Abarbeitung des Suchraums zu
parallelisieren. Eine Studie von Kwon, Balazinska, Howe, \& Rolia
{[}6{]} in MapReduce Anwendungen zeigt, das selbst geringe
Ungleichgewichte bei der Verteilung des Suchraum auf Mapper bzw.
Reducer, aufgrund der Komplexität der Matching Algorithmen, zu deutlich
längeren Laufzeiten und damit Gesamtlaufzeiten führt. In einem ihrer
Beispiele sind bei einer Gesamtzeit von 5 Minuten die meisten Mapper
innerhalb von 30 Sekunden fertig. Auch beim Streaming kann diese sog.
Datenschiefe (engl. data skew) den Durchsatz eines Clusters signifikant
mindern. Einen weiteren Ansatz die Laufzeit für nahe Echtzeit
Anwendungen zu optimieren präsentieren Whang et al. {[}7{]}. Anstatt
eine Ergebnismenge nach Abschluss eines Algorithmus zu liefern, zeigen
Sie Möglichkeiten partielle Ergebnisse während der Laufzeit des
Algorithmus zu erhalten.

\chapter{Zielsetzung}\label{sec:ziele}

Im Rahmen der Thesis soll ein Entity Resolution Framework für
Datensatzströme entstehen. Als Basis soll ein (Event) Stream Processing
Framework genutzt werden. Das Framework soll eine Reihe von Matchern,
sowie Kombinationsfunktionen der Matcher unterstützen. Hauptaugenmerk
ist jedoch die Skalierbarkeit. Gelöst werden soll das Data Skew Problem
bei verschiedenen Blocking Strategien. Eine weitere Schwierigkeit ist,
dass die Datenmenge nicht statisch ist, sondern neue Datensätze
jederzeit hinzukommen können. Beim Erweitern des Suchraums soll beachtet
werden, dass kein Data Skew auftritt. Dadurch soll vermieden werden,
dass der Durchsatz innerhalb des Clusters signifikant sinkt.
Idealerweise soll der Durchsatz, sowie die Qualität der Suchergebnisse,
mit bereits bekannten Veröffentlichungen verglichen werden. Das
Framework soll dabei kein Domainwissen eines bestimmten Entitätstypen
berücksichtigen.

\chapter{Methoden}\label{methoden}

Zur Umsetzung der in sec.~\ref{sec:ziele} beschriebenen Ziele muss
zunächst eine Wissensbasis durch Literaturarbeit in folgenden Grundlagen
geschaffen werden:

\begin{itemize}
\tightlist
\item
  Algorithmen zur Entity Resolution
\item
  Blocking und Indexing Strategien für Entity Resolution
\item
  Data Skew bei verteilten und parallelen Anwendungen
\item
  Entity Resolution Frameworks - traditionell, MapReduce, Streaming
\item
  (Event) Streaming Frameworks
\end{itemize}

Weitere Methoden sind:

\begin{itemize}
\tightlist
\item
  UML-Entwurf
\item
  Proof of Concept
\item
  Funktionelle Leistungsbewertung anhand von Datensätzen in
  wissenschaftlichen Publikationen
\end{itemize}

\chapter{Erwartete Ergebnisse}\label{erwartete-ergebnisse}

Die erwarteten Ergebnisse der Masterarbeit sind:

\begin{itemize}
\tightlist
\item
  Analyse von Entity Resolution Algorithmen
\item
  Analyse von Entity Resolution Frameworks
\item
  Analyse von (Event) Stream Processing Frameworks, für gegebenen
  Anwendungsfall
\item
  Design eines Entity Resolution Streaming Framework
\item
  Prototyp der wesentlichen Funktionen
\item
  Evaluation des Prototypen, gegen öffentliche Datensätze existierender
  Veröffentlichungen
\end{itemize}

\chapter{Vorbedingungen}\label{vorbedingungen}

\begin{itemize}
\tightlist
\item
  Datensätze zum Evaluieren und Trainieren des Frameworks bzw. der
  Algorithmen
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics{./hsrm_logo.png}
\caption{My Logo}\label{fig:label}
\end{figure}

\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl:label}My Table }\tabularnewline
\toprule
a & b & c\tabularnewline
\midrule
\endfirsthead
\toprule
a & b & c\tabularnewline
\midrule
\endhead
1 & 2 & 3\tabularnewline
4 & 5 & 6\tabularnewline
\bottomrule
\end{longtable}

\begin{codelisting}
\caption{Listing caption}

\hypertarget{lst:code}{\label{lst:code}}
\begin{Shaded}
\begin{Highlighting}[]
\OtherTok{main ::} \DataTypeTok{IO} \NormalTok{()}
\NormalTok{main }\FunctionTok{=} \NormalTok{putStrLn }\StringTok{"Hello World!"}
\end{Highlighting}
\end{Shaded}

\end{codelisting}

\chapter*{Literaturverzeichnis}\label{literaturverzeichnis}
\addcontentsline{toc}{chapter}{Literaturverzeichnis}

\hypertarget{refs}{}
\hypertarget{ref-kopcke_frameworks_2010}{}
{[}1{]} \textsc{Köpcke, Hanna} ; \textsc{Rahm, Erhard}: Frameworks for
Entity Matching: A Comparison. In: \emph{Data \& Knowledge Engineering}
Bd. 69 (2010), Nr.~2, S.~197--210

\hypertarget{ref-elmagarmid_duplicate_2007}{}
{[}2{]} \textsc{Elmagarmid, A. K.} ; \textsc{Ipeirotis, P. G.} ;
\textsc{Verykios, V. S.}: Duplicate Record Detection: A Survey. In:
\emph{IEEE Transactions on Knowledge and Data Engineering} Bd. 19
(2007), Nr.~1, S.~1--16

\hypertarget{ref-kolb_parallel_2013}{}
{[}3{]} \textsc{Kolb, Lars} ; \textsc{Rahm, Erhard}: Parallel Entity
Resolution with Dedoop. In: \emph{Datenbank-Spektrum} Bd. 13 (2013),
Nr.~1, S.~23--32

\hypertarget{ref-malhotra_graph-parallel_2014}{}
{[}4{]} \textsc{Malhotra, P.} ; \textsc{Agarwal, P.} ; \textsc{Shroff,
G.}: Graph-Parallel Entity Resolution Using LSH and IMM. In:
\emph{ResearchGate} Bd. 1133 (2014), S.~41--49

\hypertarget{ref-christen_towards_2008}{}
{[}5{]} \textsc{Christen, Peter} ; \textsc{Gayler, Ross}: Towards
Scalable Real-Time Entity Resolution Using a Similarity-Aware Inverted
Index Approach. In: \emph{Proceedings of the 7th Australasian Data
Mining Conference - Volume 87}, \emph{AusDM '08}. Darlinghurst,
Australia, Australia~: Australian Computer Society, Inc., 2008
---~ISBN~978-1-920682-68-2, S.~51--60

\hypertarget{ref-kwon_study_2011}{}
{[}6{]} \textsc{Kwon, YongChul} ; \textsc{Balazinska, Magdalena} ;
\textsc{Howe, Bill} ; \textsc{Rolia, Jerome}: A Study of Skew in
Mapreduce Applications. In: \emph{Open Cirrus Summit} (2011)

\hypertarget{ref-whang_pay-as-you-go_2013}{}
{[}7{]} \textsc{Whang, S. E.} ; \textsc{Marmaros, D.} ;
\textsc{Garcia-Molina, H.}: Pay-As-You-Go Entity Resolution. In:
\emph{IEEE Transactions on Knowledge and Data Engineering} Bd. 25
(2013), Nr.~5, S.~1111--1124

\cleardoublepage

% --------------------------
% Back matter
% --------------------------

\chapter*{Abbildungsverzeichnis}\label{abbildungsverzeichnis}
\addcontentsline{toc}{chapter}{Abbildungsverzeichnis}
\makeatletter
\@starttoc{lof}% Print List of Figures
\makeatother
\blindtext
\cleardoublepage

\listoflistings

\chapter*{Tabellenverzeichnis}\label{tabellenverzeichnis}
\addcontentsline{toc}{chapter}{Tabellenverzeichnis}
\makeatletter
\@starttoc{lot}% Print List of Tables
\makeatother
\cleardoublepage

\input{content/colophon}
\cleardoublepage

\input{content/declaration}
\clearpage
\newpage
\mbox{}

% **************************************************
% End of Document CONTENT
% **************************************************
\end{document}
