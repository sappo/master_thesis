
@article{kolb_parallel_2013,
  title = {Parallel Entity Resolution with Dedoop},
  volume = {13},
  timestamp = {2016-10-07T17:02:24Z},
  number = {1},
  urldate = {2016-10-06},
  url = {http://link.springer.com/article/10.1007/s13222-012-0110-x},
  journal = {Datenbank-Spektrum},
  author = {Kolb, Lars and Rahm, Erhard},
  year = {2013},
  pages = {23--32},
  file = {[PDF] uni-leipzig.de:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PAMWB8N8/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QKBKW45K/s13222-012-0110-x.html:text/html}
}

@inproceedings{shah_storm_2015,
  title = {Storm {{Pub}}-{{Sub}}: {{High Performance}}, {{Scalable Content Based Event Matching System Using Storm}}},
  shorttitle = {Storm {{Pub}}-{{Sub}}},
  doi = {10.1109/IPDPSW.2015.95},
  abstract = {Storm pub-sub is a novel high performance publish subscribe system designed to efficiently match events and the subscriptions with high throughput. Moving a content based pub-sub system first to a local cluster and then to a distributed cluster framework is for high performance and scalability. We depart from the use of broker overlays, where each server must support the whole range of operations of a pub-sub service, as well as overlay management and routing functionality. In this system different operations involved in pub-sub are separated to leverage their natural potential for parallelization using bolts. The storm pub-sub is compared with the traditional pub-sub system Siena, a broker based architecture. Through experimentation on local cluster as well as on distributed cluster we show that our approach of designing publish subscribe system on storm scales well for high volume of data. Storm pub-sub system approximately produces 2200 event/s on distributed cluster. In this paper we describe design and implementation of storm pub-sub and evaluate it in terms of scalability and throughput.},
  timestamp = {2016-10-07T17:02:24Z},
  booktitle = {Parallel and {{Distributed Processing Symposium Workshop}} ({{IPDPSW}}), 2015 {{IEEE International}}},
  author = {Shah, M. A. and Kulkarni, D. B.},
  month = may,
  year = {2015},
  keywords = {Algorithm design and analysis,bolts,broker based architecture,broker overlays,content based pub-sub system,Distributed cluster,distributed cluster framework,Fasteners,high performance publish subscribe system,high performance scalable content based event matching system,High Throughput,local cluster,Matching time,message passing,middleware,overlay management,parallel processing,pattern matching,Publish subscribe,Publish subscribe systems,pub-sub system Siena,routing functionality,Scalability,software reliability,Storm pub-sub,Storms,Throughput,Topology},
  pages = {585--590},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TVN25BWB/Shah and Kulkarni - 2015 - Storm Pub-Sub High Performance, Scalable Content .pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8JIUZ3UG/7284361.html:text/html}
}

@article{elmagarmid_duplicate_2007,
  title = {Duplicate {{Record Detection}}: {{A Survey}}},
  volume = {19},
  issn = {1041-4347},
  shorttitle = {Duplicate {{Record Detection}}},
  doi = {10.1109/TKDE.2007.250581},
  abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
  timestamp = {2016-10-07T07:10:54Z},
  number = {1},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Elmagarmid, A. K. and Ipeirotis, P. G. and Verykios, V. S.},
  month = jan,
  year = {2007},
  keywords = {Cleaning,Computer errors,Computer Society,Cost function,Couplings,database hardening,database management system,database management systems,data cleaning,data deduplication,data integration,data integrity,data mining,Detection algorithms,Duplicate detection,duplicate detection algorithm,duplicate record detection,entity matching.,entity resolution,fuzzy duplicate detection,identity uncertainty,instance identification,Mirrors,name matching,record linkage,Relational databases,Scalability,transcription error,Uncertainty},
  pages = {1--16},
  file = {[PDF] nyu.edu:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UQHE5W3P/Elmagarmid et al. - 2007 - Duplicate record detection A survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2C5RCMZH/4016511.html:text/html;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8VPZFZ4N/4016511.html:text/html}
}

@article{kopcke_frameworks_2010,
  title = {Frameworks for Entity Matching: {{A}} Comparison},
  volume = {69},
  issn = {0169-023X},
  shorttitle = {Frameworks for Entity Matching},
  doi = {10.1016/j.datak.2009.10.003},
  abstract = {Entity matching is a crucial and difficult task for data integration. Entity matching frameworks provide several methods and their combination to effectively solve different match tasks. In this paper, we comparatively analyze 11 proposed frameworks for entity matching. Our study considers both frameworks which do or do not utilize training data to semi-automatically find an entity matching strategy to solve a given match task. Moreover, we consider support for blocking and the combination of different match algorithms. We further study how the different frameworks have been evaluated. The study aims at exploring the current state of the art in research prototypes of entity matching frameworks and their evaluations. The proposed criteria should be helpful to identify promising framework approaches and enable categorizing and comparatively assessing additional entity matching frameworks and their evaluations.},
  timestamp = {2016-10-07T17:02:24Z},
  number = {2},
  urldate = {2016-10-06},
  url = {http://www.sciencedirect.com/science/article/pii/S0169023X09001451},
  journal = {Data \& Knowledge Engineering},
  author = {K{\"o}pcke, Hanna and Rahm, Erhard},
  month = feb,
  year = {2010},
  keywords = {Entity matching,entity resolution,Matcher combination,Match optimization,Training selection},
  pages = {197--210},
  file = {FrameworksForEntityMatchingAComparison_dke.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QK42BHG7/FrameworksForEntityMatchingAComparison_dke.pdf:application/pdf;ScienceDirect Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7CA5U7X4/S0169023X09001451.html:text/html}
}

@article{konda_magellan:_2016,
  title = {Magellan: {{Toward Building Entity Matching Management Systems}}},
  volume = {9},
  issn = {2150-8097},
  shorttitle = {Magellan},
  doi = {10.14778/2994509.2994535},
  abstract = {Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick "patching" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach.},
  timestamp = {2016-10-07T17:02:14Z},
  number = {12},
  urldate = {2016-10-07},
  url = {http://dx.doi.org/10.14778/2994509.2994535},
  journal = {Proc. VLDB Endow.},
  author = {Konda, Pradap and Das, Sanjib and {Suganthan G. C.}, Paul and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R. and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay},
  month = aug,
  year = {2016},
  pages = {1197--1208},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/I5JI48JD/Konda et al. - 2016 - Magellan Toward Building Entity Matching Manageme.pdf:application/pdf}
}

@incollection{cormode_summarizing_2005,
  series = {Proceedings},
  title = {Summarizing and {{Mining Skewed Data Streams}}},
  isbn = {978-0-89871-593-4},
  abstract = {Many applications generate massive data streams. Summarizing such massive data requires fast, small space algorithms to support post-hoc queries and mining. An important observation is that such streams are rarely uniform, and real data sources typically exhibit significant skewness. These are well modeled by Zipf distributions, which are characterized by a parameter, z, that captures the amount of skew. We present a data stream summary that can answer point queries with $\smallin$ accuracy and show that the space needed is only O($\smallin$--min\{1,1/z\}). This is the first o(1/$\smallin$) space algorithm for this problem, and we show it is essentially tight for skewed distributions. We show that the same data structure can also estimate the L2 norm of the stream in o(1/$\smallin$2) space for z $>$ \textonehalf, another improvement over the existing $\Omega$(1/$\smallin$2) methods. We support our theoretical results with an experimental study over a large variety of real and synthetic data. We show that significant skew is present in both textual and telecommunication data. Our methods give strong accuracy, significantly better than other methods, and behave exactly in line with their analytic bounds.},
  timestamp = {2016-10-07T13:51:46Z},
  urldate = {2016-10-07},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.5},
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Cormode, G. and Muthukrishnan, S.},
  month = apr,
  year = {2005},
  pages = {44--55},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MGR2ATBM/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/G6AP6IGJ/1.9781611972757.html:text/html}
}

@article{malhotra_graph-parallel_2014,
  title = {Graph-Parallel Entity Resolution Using {{LSH}} and {{IMM}}},
  volume = {1133},
  abstract = {In this paper we describe graph-based parallel algorithms for entity resolution that improve over the map-reduce approach. We compare two approaches to parallelize a Locality Sensitive Hashing...},
  timestamp = {2016-10-07T12:02:19Z},
  urldate = {2016-10-07},
  url = {https://www.researchgate.net/publication/288130692_Graph-parallel_entity_resolution_using_LSH_and_IMM},
  journal = {ResearchGate},
  author = {Malhotra, P. and Agarwal, P. and Shroff, G.},
  month = jan,
  year = {2014},
  pages = {41--49},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/HNSUD9A9/288130692_Graph-parallel_entity_resolution_using_LSH_and_IMM.html:text/html}
}

@article{brizan_._2015,
  title = {A. {{Survey}} of {{Entity Resolution}} and {{Record Linkage Methodologies}}},
  volume = {6},
  issn = {1941-6687},
  timestamp = {2016-10-06T13:34:41Z},
  number = {3},
  url = {http://scholarworks.lib.csusb.edu/ciima/vol6/iss3/5},
  journal = {Communications of the IIMA},
  author = {Brizan, David and Tansel, Abdullah},
  month = jan,
  year = {2015},
  file = {"A. Survey of Entity Resolution and Record Linkage Methodologies" by David Guy Brizan and Abdullah Uz Tansel:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PA4AUFPQ/5.html:text/html}
}

@article{kwon_study_2011,
  title = {A Study of Skew in Mapreduce Applications},
  timestamp = {2016-10-06T12:07:11Z},
  urldate = {2016-10-06},
  url = {http://ejournal.narotama.ac.id/files/A\%20study\%20of\%20skew\%20in\%20mapreduce\%20applications.pdf},
  journal = {Open Cirrus Summit},
  author = {Kwon, YongChul and Balazinska, Magdalena and Howe, Bill and Rolia, Jerome},
  year = {2011},
  file = {Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DNIQDC4X/Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:application/pdf}
}

@inproceedings{recasens_coreference_2010,
  title = {Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information},
  shorttitle = {Coreference Resolution across Corpora},
  timestamp = {2016-10-07T17:03:43Z},
  urldate = {2016-10-07},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858825},
  publisher = {{Association for Computational Linguistics}},
  author = {Recasens, Marta and Hovy, Eduard},
  month = jul,
  year = {2010},
  pages = {1423--1432},
  file = {Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/CPX98HXU/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:application/pdf;Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UIWID5C6/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:text/html}
}

@phdthesis{tirumali_efficient_2016,
  title = {{{EFFICIENT PAIR}}-{{WISE SIMILARITY COMPUTATION USING APACHE SPARK}}},
  abstract = {Entity matching is the process of identifying different manifestations of the same real world entity. These entities can be referred to as objects(string) or data instances. These entities are in turn split over several databases or clusters based on the signatures of the entities. When entity matching algorithms are performed on these databases or clusters, there is a high possibility that a particular entity pair is compared more than once. The number of comparison for any two entities depend on the number of common signatures or keys they possess. This effects the performance of any entity matching algorithm. This paper is the implementation of the algorithm written by Erhard Rahm et al. for performing redundancy free pair-wise similarity computation using MapReduce. As an improvisation to the existing implementation, this project aims to implement the algorithm in Apache Spark in standalone mode for sample of data and in cluster mode for large volume of data.},
  timestamp = {2016-10-07T17:05:11Z},
  urldate = {2016-10-07},
  url = {http://scholarworks.sjsu.edu/etd_projects/479},
  school = {San Jose State University},
  author = {Tirumali, Parineetha Gandhi},
  year = {2016},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/S5CC6TID/Tirumali - 2016 - EFFICIENT PAIR-WISE SIMILARITY COMPUTATION USING A.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4GCKPVVN/38.html:text/html}
}

@article{whang_pay-as-you-go_2013,
  title = {Pay-{{As}}-{{You}}-{{Go Entity Resolution}}},
  volume = {25},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2012.43},
  abstract = {Entity resolution (ER) is the problem of identifying which records in a database refer to the same entity. In practice, many applications need to resolve large data sets efficiently, but do not require the ER result to be exact. For example, people data from the web may simply be too large to completely resolve with a reasonable amount of work. As another example, real-time applications may not be able to tolerate any ER processing that takes longer than a certain amount of time. This paper investigates how we can maximize the progress of ER with a limited amount of work using ``hints,'' which give information on records that are likely to refer to the same real-world entity. A hint can be represented in various formats (e.g., a grouping of records based on their likelihood of matching), and ER can use this information as a guideline for which records to compare first. We introduce a family of techniques for constructing hints efficiently and techniques for using the hints to maximize the number of matching records identified using a limited amount of work. Using real data sets, we illustrate the potential gains of our pay-as-you-go approach compared to running ER without using hints.},
  timestamp = {2016-10-07T17:10:20Z},
  number = {5},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Whang, S. E. and Marmaros, D. and Garcia-Molina, H.},
  month = may,
  year = {2013},
  keywords = {Approximation algorithms,Clustering algorithms,Companies,data cleaning,Data structures,entity resolution,Erbium,Partitioning algorithms,pay-as-you-go,Tin},
  pages = {1111--1124},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UX23534Q/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VP2G2BSJ/6155721.html:text/html}
}

@phdthesis{kulkarni_recommendation_2015,
  title = {A {{Recommendation Engine Using Apache Spark}}},
  abstract = {The volume of structured and unstructured data has grown at exponential scale in recent days. As a result of this rapid data growth, we are always inundated with plethora of choices in any product or service. It is very natural to get lost in the amazon of such choices and finding hard to make decisions. The project aims at addressing this problem by using entity recommendation. The two main aspects that the project concentrates on are implementing and presenting more accurate entity recommendations to the user and another is dealing with vast amount of data. The project aims at presenting recommendation results according to user's query with efficiency and accuracy. Project makes use of ListNet ranking algorithm to rank the recommendation results. Query independent features and query dependent features are used to come up with ranking scores. Ranking scores decide the order in which the recommendation results are presented to the user. Project makes use of Apache Spark, a distributed bigdata processing framework. Spark gives the advantage of handling iterative and interactive algorithms with efficiency and minimal processing time as compared to traditional mapreduce paradigm. We performed the experiments for recommendation engine using DBPedia as the dataset and tested the results for movie domain. We used both queryindependent (pagerank) and querydependent (clicklogs) features for ranking purposes. We observed that ListNet algorithm performs really well by making use of Apache Spark as the RDDs provide faster way for iterative algorithms to execute. We also observed that the results of recommendation engine are accurate and the entities are well ranked.},
  timestamp = {2016-10-07T17:11:04Z},
  urldate = {2016-10-07},
  url = {http://scholarworks.sjsu.edu/etd_projects/456},
  school = {San Jose State University},
  author = {Kulkarni, Swapna},
  year = {2015},
  file = {Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/P3IFV5FS/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:application/pdf;Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5WS74UFU/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:text/html}
}

@article{altwaijry_query:_2015,
  title = {{{QuERy}}: {{A Framework}} for {{Integrating Entity Resolution}} with {{Query Processing}}},
  volume = {9},
  issn = {2150-8097},
  shorttitle = {{{QuERy}}},
  doi = {10.14778/2850583.2850587},
  abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
  timestamp = {2016-10-08T14:57:00Z},
  number = {3},
  urldate = {2016-10-07},
  url = {http://dx.doi.org/10.14778/2850583.2850587},
  journal = {Proc. VLDB Endow.},
  author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
  month = nov,
  year = {2015},
  pages = {120--131},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3EZ9K7HW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:application/pdf;QuERy:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ECSQGCBW/citation.html:text/html}
}

@inproceedings{christen_towards_2008,
  address = {Darlinghurst, Australia, Australia},
  series = {AusDM '08},
  title = {Towards {{Scalable Real}}-Time {{Entity Resolution Using}} a {{Similarity}}-Aware {{Inverted Index Approach}}},
  isbn = {978-1-920682-68-2},
  abstract = {Most research into entity resolution (also known as record linkage or data matching) has concentrated on the quality of the matching results. In this paper, we focus on matching time and scalability, with the aim to achieve large-scale real-time entity resolution. Traditional entity resolution techniques have assumed the matching of two static databases. In our networked and online world, however, it is becoming increasingly important for many organisations to be able to conduct entity resolution between a collection of often very large databases and a stream of query or update records. The matching should be done in (near) real-time, and be as automatic and accurate as possible, returning a ranked list of matched records for each given query record. This task therefore becomes similar to querying large document collections, as done for example by Web search engines, however based on a different type of documents: structured database records that, for example, contain personal information, such as names and addresses. In this paper, we investigate inverted indexing techniques, as commonly used in Web search engines, and employ them for real-time entity resolution. We present two variations of the traditional inverted index approach, aimed at facilitating fast approximate matching. We show encouraging initial results on large real-world data sets, with the inverted index approaches being up-to one hundred times faster than the traditionally used standard blocking approach. However, this improved matching speed currently comes at a cost, in that matching quality for larger data sets can be lower compared to when standard blocking is used, and thus more work is required.},
  timestamp = {2016-10-07T17:13:10Z},
  urldate = {2016-10-07},
  url = {http://dl.acm.org/citation.cfm?id=2449288.2449299},
  booktitle = {Proceedings of the 7th {{Australasian Data Mining Conference}} - {{Volume}} 87},
  publisher = {{Australian Computer Society, Inc.}},
  author = {Christen, Peter and Gayler, Ross},
  year = {2008},
  keywords = {approximate string comparisons,data matching,record linkage,Scalability,similarity measures},
  pages = {51--60},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KB3PP5VE/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:application/pdf;Towards scalable real-time entity resolution using a similarity-aware inverted index approach:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/27NVH45F/citation.html:text/html}
}

@article{bhattacharya_collective_2007,
  title = {Collective {{Entity Resolution}} in {{Relational Data}}},
  volume = {1},
  issn = {1556-4681},
  doi = {10.1145/1217299.1217304},
  abstract = {Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.},
  timestamp = {2016-10-09T17:56:02Z},
  number = {1},
  urldate = {2016-10-09},
  url = {http://doi.acm.org/10.1145/1217299.1217304},
  journal = {ACM Trans. Knowl. Discov. Data},
  author = {Bhattacharya, Indrajit and Getoor, Lise},
  month = mar,
  year = {2007},
  keywords = {data cleaning,entity resolution,graph clustering,record linkage},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MQXU79QF/Bhattacharya and Getoor - 2007 - Collective Entity Resolution in Relational Data.pdf:application/pdf;Collective entity resolution in relational data:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6HFK6XP5/citation.html:text/html}
}

@inproceedings{singla_entity_2006,
  title = {Entity {{Resolution}} with {{Markov Logic}}},
  doi = {10.1109/ICDM.2006.65},
  abstract = {Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. Interest in it has grown rapidly, and many approaches have been proposed. However, they tend to address only isolated aspects of the problem, and are often ad hoc. This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous approaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two citation databases show the utility of this approach, and evaluate the contribution of the different components.},
  timestamp = {2016-10-09T17:56:56Z},
  booktitle = {Sixth {{International Conference}} on {{Data Mining}} ({{ICDM}}'06)},
  author = {Singla, P. and Domingos, P.},
  month = dec,
  year = {2006},
  keywords = {citation databases,Computer science,Couplings,database management systems,Data engineering,data mining,entity-relationship modelling,entity resolution,first-order logic,formal logic,Graphical models,graph theory,inference mechanisms,inference problems,Joining processes,learning (artificial intelligence),learning problems,Logistics,Markov logic,Markov networks,Markov processes,Markov random fields,probabilistic graphical models,Probabilistic logic,probability,Spatial databases},
  pages = {572--582},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DRCB6AZ7/Singla and Domingos - 2006 - Entity Resolution with Markov Logic.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/EH5QQMKF/4053083.html:text/html}
}

@inproceedings{maddodi_data_2010,
  title = {Data {{Deduplication Techniques}} and {{Analysis}}},
  doi = {10.1109/ICETET.2010.42},
  abstract = {Data warehouses are the repositories of data collected from several data sources, which form the backbone of most of the decision support applications. As the data sources are independent, they may adopt independent and potentially inconsistent conventions. In data warehousing applications during ETL (Extraction, Transformation and Loading) or even in OLTP (On Line Transaction Processing) applications we are often encountered with duplicate records in table. Moreover, data entry mistakes at any of these sources introduce more errors. Since high quality data is essential for gaining the confidence of users of decision support applications, ensuring high data quality is critical to the success of data warehouse implementations. Therefore, significant amount of time and money are spent on the process of detecting and correcting errors and inconsistencies. The process of cleaning dirty data is often referred to as data cleaning. To make the table data consistent and accurate we need to get rid of these duplicate records from the table. In this paper we discuss different strategies of Deduplication along with their pros and cons and some of methods used to prevent duplication in database. In addition, we have made performance evaluation with Microsoft SQL-Server 2008 on Food Mart and AdventureDB Warehouses.},
  timestamp = {2016-10-09T17:58:01Z},
  booktitle = {2010 3rd {{International Conference}} on {{Emerging Trends}} in {{Engineering}} and {{Technology}} ({{ICETET}})},
  author = {Maddodi, S. and Attigeri, G. V. and Karunakar, A. K.},
  month = nov,
  year = {2010},
  keywords = {data cleaning,data deduplication,data extraction,data loading,data mining,data sources,data transformation,data warehouses,decision support applications,decision support systems,Deduplication,ETL,Microsoft SQL-Server 2008,OLTP,on line transaction processing,SQL,transaction processing},
  pages = {664--668},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FHHZE6ME/Maddodi et al. - 2010 - Data Deduplication Techniques and Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/57AIHQ2T/5698409.html:text/html;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/BJMMPD93/5698409.html:text/html}
}

@article{passos_lexicon_2014,
  title = {Lexicon {{Infused Phrase Embeddings}} for {{Named Entity Resolution}}},
  abstract = {Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.},
  timestamp = {2016-10-09T17:58:15Z},
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.5367},
  primaryClass = {cs},
  urldate = {2016-10-09},
  url = {http://arxiv.org/abs/1404.5367},
  journal = {arXiv:1404.5367 [cs]},
  author = {Passos, Alexandre and Kumar, Vineet and McCallum, Andrew},
  month = apr,
  year = {2014},
  keywords = {Computer Science - Computation and Language},
  annote = {Comment: Accepted in CoNLL 2014},
  file = {arXiv\:1404.5367 PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/T2SVD7G4/Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity.pdf:application/pdf;arXiv.org Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3XX7SWCX/1404.html:text/html}
}

@misc{_removing_????,
  title = {Removing {{Fully}} and {{Partially Duplicated Records}} through {{K}}-{{Means Clustering}} - {{ProQuest}}},
  timestamp = {2016-10-09T17:58:49Z},
  urldate = {2016-10-09},
  url = {http://search.proquest.com/openview/588b8624a99372af77a8a672824dbc10/1?pq-origsite=gscholar},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ZAGGEWV4/1.html:text/html}
}

@misc{_address_????,
  title = {Address and {{Participant Entity}}-{{Resolution}} in a {{Large}}, {{Cohort Observational Study Utilizing}} an {{Open}}-Source {{Entity Resolution Tool}} ({{OYSTER}}) - {{ProQuest}}},
  timestamp = {2016-10-09T17:59:07Z},
  urldate = {2016-10-09},
  url = {http://search.proquest.com/openview/7edcb89e055491b0ed4f8f6f47a58713/1?pq-origsite=gscholar},
  file = {Address and Participant Entity-Resolution in a Lar.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/E6HV83AA/Address and Participant Entity-Resolution in a Lar.html:text/html}
}

@misc{_duplicate_????,
  title = {Duplicate {{Detection}}, {{Record Linkage}}, and {{Identity Uncertainty}}: {{Datasets}}},
  timestamp = {2016-10-09T17:59:56Z},
  urldate = {2016-10-09},
  url = {http://www.cs.utexas.edu/users/ml/riddle/data.html},
  file = {Duplicate Detection, Record Linkage, and Identity Uncertainty\: Datasets:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5AQ7F8BX/data.html:text/html}
}

@phdthesis{kopcke_object_2014,
  title = {Object Matching on Real-World Problems /},
  language = {English},
  timestamp = {2016-10-09T18:05:27Z},
  author = {K{\"o}pcke, Hanna},
  year = {2014},
  file = {Dissertation_HannaKöpcke_online.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6TC3N9EZ/Dissertation_HannaKöpcke_online.pdf:application/pdf;Details\: Object matching on real-world problems /:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7BEMI7MH/0013004879.html:text/html}
}

@phdthesis{sehili_evaluierung_????,
  title = {Evaluierung Und {{Erweiterung}} von {{MapReduce}}-{{Algorithmen}} Zur {{Berechnung}} Der Transitiven {{H{\"u}lle}} Ungerichteter {{Graphen}} F{\"u}r {{Entity Resolution Workflows}}},
  abstract = {Im  Bereich  von
Entity-Resolution
oder
deduplication
werden  aufgrund  fehlen-
der  global  eindeutiger  Identifikatoren  Match-Techniken  verwendet,  um  zu
bestimmen,  ob  verschiedene  Datens{\"a}tze  dasselbe  Realweltobjekt  darstellen.
Die  inh{\"a}rente  quadratische  Komplexit{\"a}t  f{\"u}hrt  zu  sehr  langen  Laufzeiten  f{\"u}r
gro\ss{}e  Datenmengen,  was  eine  Parallelisierung  dieses  Prozesses  erfordert.
MapReduce  ist  wegen  seiner  Skalierbarkeit  und  Einsetzbarkeit  in  Cloud-
Infrastrukturen  eine  gute  L{\"o}sung  zur  Verbesserung  der  Laufzeit.  Au\ss{}erdem
kann unter bestimmten Voraussetzungen die Qualit{\"a}t des Match-Ergebnisses
durch  die  Berechnung  der  transitiven  H{\"u}lle  verbessert  werden.  Die  Berech-
nung  der  transitiven  H{\"u}lle  eines  Graphen  ist  von  Natur  aus  ein  iterativer
Prozess. Ein naiver Ansatz berechnet sie
linear
, i.e. nach
d
Iterationen, wobei
d
die  Tiefe  des  Graphen  ist.  In  dieser  Arbeit  wird  am  Beispiel  der  Entity
Resolution  die  Verwendung  von  MapReduce  f{\"u}r  die  iterative  und  verteilte
Berechnung der transitiven H{\"u}lle untersucht. Der vorgeschlagene Algorithmus
Smart-MR
operiert nur auf azyklischen Graphen und konvergiert nach genau
log
d
Iterationen. Die drei weiteren Algorithmen
Cyc-Smart-MR
,
Full-TC-MR
und
CC-MR
arbeiten alle auf beliebigen ungerichteten Graphen und weisen ebenso
ein logarithmisches Verhalten auf.},
  timestamp = {2016-10-09T18:09:07Z},
  urldate = {2016-10-09},
  url = {http://lips.informatik.uni-leipzig.de/files/trasitive_closure_with_mapreduce_0.pdf},
  school = {Universit{\"a}t Leipzig},
  author = {Sehili, Ziad},
  file = {... - trasitive_closure_with_mapreduce_0.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/X5FG4QMH/trasitive_closure_with_mapreduce_0.pdf:application/pdf}
}

@comment{jabref-meta: groupsversion:3;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Master_Thesis\;0\;kolb_parallel_2013\;shah_storm_2015\
;elmagarmid_duplicate_2007\;kopcke_frameworks_2010\;konda_magellan:_20
16\;cormode_summarizing_2005\;malhotra_graph-parallel_2014\;brizan_._2
015\;kwon_study_2011\;recasens_coreference_2010\;tirumali_efficient_20
16\;whang_pay-as-you-go_2013\;kulkarni_recommendation_2015\;altwaijry_
query:_2015\;christen_towards_2008\;bhattacharya_collective_2007\;sing
la_entity_2006\;maddodi_data_2010\;passos_lexicon_2014\;_removing_????
\;_address_????\;_duplicate_????\;kopcke_object_2014\;sehili_evaluieru
ng_????\;;
}

