
@article{KBHR:study:11,
  author = {Kwon, YongChul and Balazinska, Magdalena and Howe, Bill and Rolia, Jerome},
  title = {A Study of Skew in Mapreduce Applications},
  year = {2011},
  journal = {Open Cirrus Summit},
  url = {http://ejournal.narotama.ac.id/files/A\%20study\%20of\%20skew\%20in\%20mapreduce\%20applications.pdf},
  file = {Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/DNIQDC4X/Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:application/pdf},
  timestamp = {2016-10-11T08:55:16Z},
  urldate = {2016-10-06}
}

@article{KR:Parallel:13,
  author = {Kolb, Lars and Rahm, Erhard},
  title = {Parallel Entity Resolution with Dedoop},
  year = {2013},
  journal = {Datenbank-Spektrum},
  volume = {13},
  number = {1},
  pages = {23--32},
  url = {http://link.springer.com/article/10.1007/s13222-012-0110-x},
  file = {Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/PAMWB8N8/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:application/pdf;Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/QKBKW45K/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.html:text/html},
  timestamp = {2016-10-11T08:51:47Z},
  urldate = {2016-10-06}
}

@inproceedings{SK:Storm:15,
  author = {Shah, M. A. and Kulkarni, D. B.},
  shorttitle = {Storm {{Pub}}-{{Sub}}},
  title = {Storm {{Pub}}-{{Sub}}: {{High Performance}}, {{Scalable Content Based Event Matching System Using Storm}}},
  year = {2015},
  month = may,
  booktitle = {Parallel and {{Distributed Processing Symposium Workshop}} ({{IPDPSW}}), 2015 {{IEEE International}}},
  pages = {585--590},
  doi = {10.1109/IPDPSW.2015.95},
  file = {IEEE Xplore Full Text PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/TVN25BWB/Shah and Kulkarni - 2015 - Storm Pub-Sub High Performance, Scalable Content .pdf:application/pdf;IEEE Xplore Abstract Record:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/8JIUZ3UG/7284361.html:text/html},
  keywords = {Algorithm design and analysis,bolts,broker based architecture,broker overlays,content based pub-sub system,Distributed cluster,distributed cluster framework,Fasteners,high performance publish subscribe system,high performance scalable content based event matching system,High Throughput,local cluster,Matching time,message passing,middleware,overlay management,parallel processing,pattern matching,Publish subscribe,Publish subscribe systems,pub-sub system Siena,routing functionality,Scalability,software reliability,Storm pub-sub,Storms,Throughput,Topology},
  abstract = {Storm pub-sub is a novel high performance publish subscribe system designed to efficiently match events and the subscriptions with high throughput. Moving a content based pub-sub system first to a local cluster and then to a distributed cluster framework is for high performance and scalability. We depart from the use of broker overlays, where each server must support the whole range of operations of a pub-sub service, as well as overlay management and routing functionality. In this system different operations involved in pub-sub are separated to leverage their natural potential for parallelization using bolts. The storm pub-sub is compared with the traditional pub-sub system Siena, a broker based architecture. Through experimentation on local cluster as well as on distributed cluster we show that our approach of designing publish subscribe system on storm scales well for high volume of data. Storm pub-sub system approximately produces 2200 event/s on distributed cluster. In this paper we describe design and implementation of storm pub-sub and evaluate it in terms of scalability and throughput.},
  timestamp = {2016-10-11T08:57:05Z}
}

@article{EIV:Duplicate:07,
  author = {Elmagarmid, A. K. and Ipeirotis, P. G. and Verykios, V. S.},
  shorttitle = {Duplicate {{Record Detection}}},
  title = {Duplicate {{Record Detection}}: {{A Survey}}},
  year = {2007},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {19},
  number = {1},
  pages = {1--16},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2007.250581},
  file = {Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/UQHE5W3P/Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/2C5RCMZH/4016511.html:text/html;Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/8VPZFZ4N/Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.html:text/html},
  keywords = {Cleaning,Computer errors,Computer Society,Cost function,Couplings,database hardening,database management system,database management systems,data cleaning,data deduplication,data integration,data integrity,data mining,Detection algorithms,Duplicate detection,duplicate detection algorithm,duplicate record detection,entity matching.,entity resolution,fuzzy duplicate detection,identity uncertainty,instance identification,Mirrors,name matching,record linkage,Relational databases,Scalability,transcription error,Uncertainty},
  abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
  timestamp = {2016-10-11T08:55:52Z}
}

@article{KR:Frameworks:10,
  author = {K{\"o}pcke, Hanna and Rahm, Erhard},
  shorttitle = {Frameworks for Entity Matching},
  title = {Frameworks for Entity Matching: {{A}} Comparison},
  year = {2010},
  month = feb,
  journal = {Data \& Knowledge Engineering},
  volume = {69},
  number = {2},
  pages = {197--210},
  issn = {0169-023X},
  url = {http://www.sciencedirect.com/science/article/pii/S0169023X09001451},
  doi = {10.1016/j.datak.2009.10.003},
  file = {FrameworksForEntityMatchingAComparison_dke.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/QK42BHG7/FrameworksForEntityMatchingAComparison_dke.pdf:application/pdf;ScienceDirect Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/7CA5U7X4/S0169023X09001451.html:text/html},
  keywords = {Entity matching,entity resolution,Matcher combination,Match optimization,Training selection},
  abstract = {Entity matching is a crucial and difficult task for data integration. Entity matching frameworks provide several methods and their combination to effectively solve different match tasks. In this paper, we comparatively analyze 11 proposed frameworks for entity matching. Our study considers both frameworks which do or do not utilize training data to semi-automatically find an entity matching strategy to solve a given match task. Moreover, we consider support for blocking and the combination of different match algorithms. We further study how the different frameworks have been evaluated. The study aims at exploring the current state of the art in research prototypes of entity matching frameworks and their evaluations. The proposed criteria should be helpful to identify promising framework approaches and enable categorizing and comparatively assessing additional entity matching frameworks and their evaluations.},
  timestamp = {2016-10-11T08:56:24Z},
  urldate = {2016-10-06}
}

@incollection{CM:Summarizing:05,
  author = {Cormode, G. and Muthukrishnan, S.},
  title = {Summarizing and {{Mining Skewed Data Streams}}},
  year = {2005},
  month = apr,
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  isbn = {978-0-89871-593-4},
  pages = {44--55},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.5},
  file = {Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/MGR2ATBM/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:application/pdf;Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/G6AP6IGJ/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.html:text/html},
  abstract = {Many applications generate massive data streams. Summarizing such massive data requires fast, small space algorithms to support post-hoc queries and mining. An important observation is that such streams are rarely uniform, and real data sources typically exhibit significant skewness. These are well modeled by Zipf distributions, which are characterized by a parameter, z, that captures the amount of skew. We present a data stream summary that can answer point queries with $\smallin$ accuracy and show that the space needed is only O($\smallin$--min\{1,1/z\}). This is the first o(1/$\smallin$) space algorithm for this problem, and we show it is essentially tight for skewed distributions. We show that the same data structure can also estimate the L2 norm of the stream in o(1/$\smallin$2) space for z $>$ \textonehalf, another improvement over the existing $\Omega$(1/$\smallin$2) methods. We support our theoretical results with an experimental study over a large variety of real and synthetic data. We show that significant skew is present in both textual and telecommunication data. Our methods give strong accuracy, significantly better than other methods, and behave exactly in line with their analytic bounds.},
  timestamp = {2016-10-11T08:57:12Z},
  publisher = {{Society for Industrial and Applied Mathematics}},
  urldate = {2016-10-07},
  series = {Proceedings}
}

@article{AMK:QuERy:15,
  author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
  shorttitle = {{{QuERy}}},
  title = {{{QuERy}}: {{A Framework}} for {{Integrating Entity Resolution}} with {{Query Processing}}},
  year = {2015},
  month = nov,
  journal = {Proc. VLDB Endow.},
  volume = {9},
  number = {3},
  pages = {120--131},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/2850583.2850587},
  doi = {10.14778/2850583.2850587},
  file = {Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/3EZ9K7HW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:application/pdf;Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/ECSQGCBW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.html:text/html},
  abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
  timestamp = {2016-10-11T08:53:19Z},
  urldate = {2016-10-07}
}

@inproceedings{CG:Scalable:08,
  author = {Christen, Peter and Gayler, Ross},
  title = {Towards {{Scalable Real}}-Time {{Entity Resolution Using}} a {{Similarity}}-Aware {{Inverted Index Approach}}},
  year = {2008},
  booktitle = {Proceedings of the 7th {{Australasian Data Mining Conference}} - {{Volume}} 87},
  isbn = {978-1-920682-68-2},
  pages = {51--60},
  url = {http://dl.acm.org/citation.cfm?id=2449288.2449299},
  file = {Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/KB3PP5VE/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:application/pdf;Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/27NVH45F/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.html:text/html},
  keywords = {approximate string comparisons,data matching,record linkage,Scalability,similarity measures},
  abstract = {Most research into entity resolution (also known as record linkage or data matching) has concentrated on the quality of the matching results. In this paper, we focus on matching time and scalability, with the aim to achieve large-scale real-time entity resolution. Traditional entity resolution techniques have assumed the matching of two static databases. In our networked and online world, however, it is becoming increasingly important for many organisations to be able to conduct entity resolution between a collection of often very large databases and a stream of query or update records. The matching should be done in (near) real-time, and be as automatic and accurate as possible, returning a ranked list of matched records for each given query record. This task therefore becomes similar to querying large document collections, as done for example by Web search engines, however based on a different type of documents: structured database records that, for example, contain personal information, such as names and addresses. In this paper, we investigate inverted indexing techniques, as commonly used in Web search engines, and employ them for real-time entity resolution. We present two variations of the traditional inverted index approach, aimed at facilitating fast approximate matching. We show encouraging initial results on large real-world data sets, with the inverted index approaches being up-to one hundred times faster than the traditionally used standard blocking approach. However, this improved matching speed currently comes at a cost, in that matching quality for larger data sets can be lower compared to when standard blocking is used, and thus more work is required.},
  timestamp = {2016-10-11T08:57:26Z},
  publisher = {{Australian Computer Society, Inc.}},
  urldate = {2016-10-07},
  series = {AusDM '08},
  address = {Darlinghurst, Australia, Australia}
}

@phdthesis{Seh:Evaluierung:13,
  author = {Sehili, Ziad},
  title = {{Evaluierung und Erweiterung von MapReduce-Algorithmen zur Berechnung der transitiven H{\"u}lle ungerichteter Graphen f{\"u}r Entity Resolution Workflows}},
  year = {2013},
  url = {http://lips.informatik.uni-leipzig.de/files/trasitive_closure_with_mapreduce_0.pdf},
  file = {... - trasitive_closure_with_mapreduce_0.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/X5FG4QMH/trasitive_closure_with_mapreduce_0.pdf:application/pdf},
  abstract = {Im  Bereich  von
Entity-Resolution
oder
deduplication
werden  aufgrund  fehlen-
der  global  eindeutiger  Identifikatoren  Match-Techniken  verwendet,  um  zu
bestimmen,  ob  verschiedene  Datens{\"a}tze  dasselbe  Realweltobjekt  darstellen.
Die  inh{\"a}rente  quadratische  Komplexit{\"a}t  f{\"u}hrt  zu  sehr  langen  Laufzeiten  f{\"u}r
gro\ss{}e  Datenmengen,  was  eine  Parallelisierung  dieses  Prozesses  erfordert.
MapReduce  ist  wegen  seiner  Skalierbarkeit  und  Einsetzbarkeit  in  Cloud-
Infrastrukturen  eine  gute  L{\"o}sung  zur  Verbesserung  der  Laufzeit.  Au\ss{}erdem
kann unter bestimmten Voraussetzungen die Qualit{\"a}t des Match-Ergebnisses
durch  die  Berechnung  der  transitiven  H{\"u}lle  verbessert  werden.  Die  Berech-
nung  der  transitiven  H{\"u}lle  eines  Graphen  ist  von  Natur  aus  ein  iterativer
Prozess. Ein naiver Ansatz berechnet sie
linear
, i.e. nach
d
Iterationen, wobei
d
die  Tiefe  des  Graphen  ist.  In  dieser  Arbeit  wird  am  Beispiel  der  Entity
Resolution  die  Verwendung  von  MapReduce  f{\"u}r  die  iterative  und  verteilte
Berechnung der transitiven H{\"u}lle untersucht. Der vorgeschlagene Algorithmus
Smart-MR
operiert nur auf azyklischen Graphen und konvergiert nach genau
log
d
Iterationen. Die drei weiteren Algorithmen
Cyc-Smart-MR
,
Full-TC-MR
und
CC-MR
arbeiten alle auf beliebigen ungerichteten Graphen und weisen ebenso
ein logarithmisches Verhalten auf.},
  timestamp = {2016-10-11T08:56:18Z},
  school = {Universit{\"a}t Leipzig},
  urldate = {2016-10-09},
  language = {deutsch}
}

@inproceedings{MAK:Data:10,
  author = {Maddodi, S. and Attigeri, G. V. and Karunakar, A. K.},
  title = {Data {{Deduplication Techniques}} and {{Analysis}}},
  year = {2010},
  month = nov,
  booktitle = {2010 3rd {{International Conference}} on {{Emerging Trends}} in {{Engineering}} and {{Technology}} ({{ICETET}})},
  pages = {664--668},
  doi = {10.1109/ICETET.2010.42},
  file = {IEEE Xplore Full Text PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/FHHZE6ME/Maddodi et al. - 2010 - Data Deduplication Techniques and Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/57AIHQ2T/5698409.html:text/html;IEEE Xplore Abstract Record:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/BJMMPD93/5698409.html:text/html},
  keywords = {data cleaning,data deduplication,data extraction,data loading,data mining,data sources,data transformation,data warehouses,decision support applications,decision support systems,Deduplication,ETL,Microsoft SQL-Server 2008,OLTP,on line transaction processing,SQL,transaction processing},
  abstract = {Data warehouses are the repositories of data collected from several data sources, which form the backbone of most of the decision support applications. As the data sources are independent, they may adopt independent and potentially inconsistent conventions. In data warehousing applications during ETL (Extraction, Transformation and Loading) or even in OLTP (On Line Transaction Processing) applications we are often encountered with duplicate records in table. Moreover, data entry mistakes at any of these sources introduce more errors. Since high quality data is essential for gaining the confidence of users of decision support applications, ensuring high data quality is critical to the success of data warehouse implementations. Therefore, significant amount of time and money are spent on the process of detecting and correcting errors and inconsistencies. The process of cleaning dirty data is often referred to as data cleaning. To make the table data consistent and accurate we need to get rid of these duplicate records from the table. In this paper we discuss different strategies of Deduplication along with their pros and cons and some of methods used to prevent duplication in database. In addition, we have made performance evaluation with Microsoft SQL-Server 2008 on Food Mart and AdventureDB Warehouses.},
  timestamp = {2016-10-11T08:55:32Z}
}

@article{KDS.EA:Magellan:16,
  author = {Konda, Pradap and Das, Sanjib and Suganthan G. C., Paul and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R. and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay},
  shorttitle = {Magellan},
  title = {Magellan: {{Toward Building Entity Matching Management Systems}}},
  year = {2016},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {9},
  number = {12},
  pages = {1197--1208},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/2994509.2994535},
  doi = {10.14778/2994509.2994535},
  file = {ACM Full Text PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/I5JI48JD/Konda et al. - 2016 - Magellan Toward Building Entity Matching Manageme.pdf:application/pdf},
  abstract = {Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick "patching" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach.},
  timestamp = {2016-10-14T08:17:30Z},
  urldate = {2016-10-07}
}

@phdthesis{Tir:EFFICIENT:16,
  author = {Tirumali, Parineetha Gandhi},
  title = {{{EFFICIENT PAIR}}-{{WISE SIMILARITY COMPUTATION USING APACHE SPARK}}},
  year = {2016},
  url = {http://scholarworks.sjsu.edu/etd_projects/479},
  file = {Full Text PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/S5CC6TID/Tirumali - 2016 - EFFICIENT PAIR-WISE SIMILARITY COMPUTATION USING A.pdf:application/pdf;Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/4GCKPVVN/38.html:text/html},
  abstract = {Entity matching is the process of identifying different manifestations of the same real world entity. These entities can be referred to as objects(string) or data instances. These entities are in turn split over several databases or clusters based on the signatures of the entities. When entity matching algorithms are performed on these databases or clusters, there is a high possibility that a particular entity pair is compared more than once. The number of comparison for any two entities depend on the number of common signatures or keys they possess. This effects the performance of any entity matching algorithm. This paper is the implementation of the algorithm written by Erhard Rahm et al. for performing redundancy free pair-wise similarity computation using MapReduce. As an improvisation to the existing implementation, this project aims to implement the algorithm in Apache Spark in standalone mode for sample of data and in cluster mode for large volume of data.},
  timestamp = {2016-10-11T08:56:09Z},
  school = {San Jose State University},
  urldate = {2016-10-07}
}

@phdthesis{Kul:Recommendation:15,
  author = {Kulkarni, Swapna},
  title = {A {{Recommendation Engine Using Apache Spark}}},
  year = {2015},
  url = {http://scholarworks.sjsu.edu/etd_projects/456},
  file = {Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/P3IFV5FS/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:application/pdf;Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/5WS74UFU/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:text/html},
  abstract = {The volume of structured and unstructured data has grown at exponential scale in recent days. As a result of this rapid data growth, we are always inundated with plethora of choices in any product or service. It is very natural to get lost in the amazon of such choices and finding hard to make decisions. The project aims at addressing this problem by using entity recommendation. The two main aspects that the project concentrates on are implementing and presenting more accurate entity recommendations to the user and another is dealing with vast amount of data. The project aims at presenting recommendation results according to user's query with efficiency and accuracy. Project makes use of ListNet ranking algorithm to rank the recommendation results. Query independent features and query dependent features are used to come up with ranking scores. Ranking scores decide the order in which the recommendation results are presented to the user. Project makes use of Apache Spark, a distributed bigdata processing framework. Spark gives the advantage of handling iterative and interactive algorithms with efficiency and minimal processing time as compared to traditional mapreduce paradigm. We performed the experiments for recommendation engine using DBPedia as the dataset and tested the results for movie domain. We used both queryindependent (pagerank) and querydependent (clicklogs) features for ranking purposes. We observed that ListNet algorithm performs really well by making use of Apache Spark as the RDDs provide faster way for iterative algorithms to execute. We also observed that the results of recommendation engine are accurate and the entities are well ranked.},
  timestamp = {2016-10-11T08:55:12Z},
  school = {San Jose State University},
  urldate = {2016-10-07}
}

@misc{HCTL:Duplicate:03,
  author = {Hern{\'a}ndez, Mauricio and Cohen, William and Tejada, Sheila and Lawrence, Steve},
  title = {Duplicate {{Detection}}, {{Record Linkage}}, and {{Identity Uncertainty}}: {{Datasets}}},
  year = {2003},
  url = {http://www.cs.utexas.edu/users/ml/riddle/data.html},
  file = {Duplicate Detection, Record Linkage, and Identity Uncertainty\: Datasets:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/5AQ7F8BX/data.html:text/html},
  timestamp = {2016-10-11T12:57:11Z},
  urldate = {2016-10-09}
}

@article{PKM:Lexicon:14,
  author = {Passos, Alexandre and Kumar, Vineet and McCallum, Andrew},
  title = {Lexicon {{Infused Phrase Embeddings}} for {{Named Entity Resolution}}},
  year = {2014},
  month = apr,
  journal = {arXiv:1404.5367 [cs]},
  url = {http://arxiv.org/abs/1404.5367},
  file = {arXiv\:1404.5367 PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/T2SVD7G4/Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity.pdf:application/pdf;arXiv.org Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/3XX7SWCX/1404.html:text/html},
  keywords = {Computer Science - Computation and Language},
  abstract = {Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.},
  timestamp = {2016-10-11T08:48:08Z},
  annote = {Comment: Accepted in CoNLL 2014},
  urldate = {2016-10-09},
  primaryClass = {cs},
  eprint = {1404.5367},
  eprinttype = {arxiv},
  archivePrefix = {arXiv}
}

@article{WMG:Pay:13,
  author = {Whang, S. E. and Marmaros, D. and Garcia-Molina, H.},
  title = {Pay-{{As}}-{{You}}-{{Go Entity Resolution}}},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {25},
  number = {5},
  pages = {1111--1124},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2012.43},
  file = {Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/UX23534Q/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:application/pdf;Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/VP2G2BSJ/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.html:text/html},
  keywords = {Approximation algorithms,Clustering algorithms,Companies,data cleaning,Data structures,entity resolution,Erbium,Partitioning algorithms,pay-as-you-go,Tin},
  abstract = {Entity resolution (ER) is the problem of identifying which records in a database refer to the same entity. In practice, many applications need to resolve large data sets efficiently, but do not require the ER result to be exact. For example, people data from the web may simply be too large to completely resolve with a reasonable amount of work. As another example, real-time applications may not be able to tolerate any ER processing that takes longer than a certain amount of time. This paper investigates how we can maximize the progress of ER with a limited amount of work using ``hints,'' which give information on records that are likely to refer to the same real-world entity. A hint can be represented in various formats (e.g., a grouping of records based on their likelihood of matching), and ER can use this information as a guideline for which records to compare first. We introduce a family of techniques for constructing hints efficiently and techniques for using the hints to maximize the number of matching records identified using a limited amount of work. Using real data sets, we illustrate the potential gains of our pay-as-you-go approach compared to running ER without using hints.},
  timestamp = {2016-10-11T08:48:36Z}
}

@inproceedings{RH:Coreference:10,
  author = {Recasens, Marta and Hovy, Eduard},
  shorttitle = {Coreference Resolution across Corpora},
  title = {Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information},
  year = {2010},
  month = jul,
  pages = {1423--1432},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858825},
  file = {Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/CPX98HXU/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:application/pdf;Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/UIWID5C6/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:text/html},
  timestamp = {2016-10-11T08:55:29Z},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2016-10-07}
}

@inproceedings{SD:Entity:06,
  author = {Singla, P. and Domingos, P.},
  title = {Entity {{Resolution}} with {{Markov Logic}}},
  year = {2006},
  month = dec,
  booktitle = {Sixth {{International Conference}} on {{Data Mining}} ({{ICDM}}'06)},
  pages = {572--582},
  doi = {10.1109/ICDM.2006.65},
  file = {Singla and Domingos - 2006 - Entity Resolution with Markov Logic.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/DRCB6AZ7/Singla and Domingos - 2006 - Entity Resolution with Markov Logic.pdf:application/pdf;Singla and Domingos - 2006 - Entity Resolution with Markov Logic.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/EH5QQMKF/Singla and Domingos - 2006 - Entity Resolution with Markov Logic.html:text/html},
  keywords = {citation databases,Computer science,Couplings,database management systems,Data engineering,data mining,entity-relationship modelling,entity resolution,first-order logic,formal logic,Graphical models,graph theory,inference mechanisms,inference problems,Joining processes,learning (artificial intelligence),learning problems,Logistics,Markov logic,Markov networks,Markov processes,Markov random fields,probabilistic graphical models,Probabilistic logic,probability,Spatial databases},
  abstract = {Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. Interest in it has grown rapidly, and many approaches have been proposed. However, they tend to address only isolated aspects of the problem, and are often ad hoc. This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous approaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two citation databases show the utility of this approach, and evaluate the contribution of the different components.},
  timestamp = {2016-10-11T08:56:14Z}
}

@phdthesis{Kop:Object:14,
  author = {K{\"o}pcke, Hanna},
  title = {Object Matching on Real-World Problems /},
  year = {2014},
  file = {Dissertation_HannaKöpcke_online.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/6TC3N9EZ/Dissertation_HannaKöpcke_online.pdf:application/pdf;Details\: Object matching on real-world problems /:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/7BEMI7MH/0013004879.html:text/html},
  timestamp = {2016-10-11T08:50:42Z},
  language = {English}
}

@article{BG:Collective:07,
  author = {Bhattacharya, Indrajit and Getoor, Lise},
  title = {Collective {{Entity Resolution}} in {{Relational Data}}},
  year = {2007},
  month = mar,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {1},
  number = {1},
  issn = {1556-4681},
  url = {http://doi.acm.org/10.1145/1217299.1217304},
  doi = {10.1145/1217299.1217304},
  file = {ACM Full Text PDF:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/MQXU79QF/Bhattacharya and Getoor - 2007 - Collective Entity Resolution in Relational Data.pdf:application/pdf;Collective entity resolution in relational data:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/6HFK6XP5/citation.html:text/html},
  keywords = {data cleaning,entity resolution,graph clustering,record linkage},
  abstract = {Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.},
  timestamp = {2016-10-11T08:55:24Z},
  urldate = {2016-10-09}
}

@article{Joa:Svmlight:99,
  author = {Joachims, Thorsten},
  shorttitle = {Svmlight},
  title = {Svmlight: {{Support}} Vector Machine},
  year = {1999},
  journal = {SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund},
  volume = {19},
  number = {4},
  url = {https://akela.mendelu.cz/~zizka/Machine_Learning/SVM/SVM-light/SVM-Light\%20Support\%20Vector\%20Ma...pdf},
  file = {Joachims - 1999 - Svmlight Support vector machine.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/DE6C6F8D/Joachims - 1999 - Svmlight Support vector machine.pdf:application/pdf},
  timestamp = {2016-10-11T08:57:18Z},
  urldate = {2016-10-10}
}

@inproceedings{PMM.EA:Identity:02,
  author = {Pasula, Hanna and Marthi, Bhaskara and Milch, Brian and Russell, Stuart and Shpitser, Ilya},
  title = {Identity Uncertainty and Citation Matching},
  year = {2002},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {1401--1408},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AP01.pdf},
  file = {[PDF] wustl.edu:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/4UJ26N4J/Pasula et al. - 2002 - Identity uncertainty and citation matching.pdf:application/pdf;[PDF] wustl.edu:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/78NAN8XS/Pasula et al. - 2002 - Identity uncertainty and citation matching.pdf:application/pdf},
  timestamp = {2016-10-14T08:17:13Z},
  urldate = {2016-10-10}
}

@inproceedings{SB:Interactive:02,
  author = {Sarawagi, Sunita and Bhamidipaty, Anuradha},
  title = {Interactive Deduplication Using Active Learning},
  year = {2002},
  booktitle = {Proceedings of the Eighth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  pages = {269--278},
  url = {http://dl.acm.org/citation.cfm?id=775087},
  file = {[PDF] umd.edu:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/4BK89QBT/Sarawagi and Bhamidipaty - 2002 - Interactive deduplication using active learning.pdf:application/pdf;Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/WN3ZNRIS/citation.html:text/html},
  timestamp = {2016-10-11T08:48:01Z},
  annote = {ALIAS},
  publisher = {{ACM}},
  urldate = {2016-10-10}
}

@inproceedings{GIKS:Text:03,
  author = {Gravano, Luis and Ipeirotis, Panagiotis G. and Koudas, Nick and Srivastava, Divesh},
  title = {Text Joins in an {{RDBMS}} for Web Data Integration},
  year = {2003},
  booktitle = {Proceedings of the 12th International Conference on {{World Wide Web}}},
  pages = {90--101},
  url = {http://dl.acm.org/citation.cfm?id=775166},
  file = {Gravano et al. - 2003 - Text joins in an RDBMS for web data integration.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/XPDDA6NE/Gravano et al. - 2003 - Text joins in an RDBMS for web data integration.html:text/html},
  timestamp = {2016-10-11T08:57:22Z},
  annote = {Q-grams with tf.idf},
  publisher = {{ACM}},
  urldate = {2016-10-10}
}

@article{KRJK:Removing:12,
  author = {Khan, Bilal and Rauf, Azhar and Javed, Huma and Khusro, Shah},
  title = {Removing Fully and Partially Duplicated Records through {{K}}-{{Means}} Clustering},
  year = {2012},
  journal = {International Journal of Engineering and Technology},
  volume = {4},
  number = {6},
  pages = {750},
  url = {http://search.proquest.com/openview/588b8624a99372af77a8a672824dbc10/1?pq-origsite=gscholar},
  file = {588b8624a99372af77a8a672824dbc10.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/SDAFXQ82/588b8624a99372af77a8a672824dbc10.pdf:application/pdf},
  timestamp = {2016-10-10T11:54:20Z},
  urldate = {2016-10-10}
}

@techreport{GFS.EA:Declarative:01,
  author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric and Saita, Cristian},
  shorttitle = {Declarative {{Data Cleaning}}},
  title = {Declarative {{Data Cleaning}} : {{Language}}, {{Model}}, and {{Algorithms}}},
  year = {2001},
  url = {https://hal.inria.fr/inria-00072476/document},
  file = {Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/8I934KA5/Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.pdf:application/pdf;Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/4E32KCBD/Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.html:text/html},
  abstract = {The problem of data cleaning, which consists of emoving inconsistencies and errors from original data sets, is well known in the area of decision support systems and data warehouses. However, for non-conventional applications, such as the migration of largely unstructured data into structured one, or the integration of heterogeneous scientific data sets in inter-discipl- inary fields (e.g., in environmental science), existing ETL (Extraction Transformation Loading) and data cleaning tools for writing data cleaning programs are insufficient. The main challenge with them is the design of a data flow graph that effectively generates clean data, and can perform efficiently on large sets of input data. The difficulty with them comes from (i) a lack of clear separation between the logical specification of data transformations and their physical implementation and (ii) the lack of explanation of cleaning results and user interaction facilities to tune a data cleaning program. This paper addresses these two problems and presents a language, an execution model and algorithms that enable users to express data cleaning specifications declaratively and perform the cleaning efficiently. We use as an example a set of bibliographic references used to construct the Citeseer Web site. The underlying data integration problem is to derive structured and clean textual records so that meaningful queries can be performed. Experimental results report on the assessement of the proposed framework for data cleaning.},
  timestamp = {2016-10-14T08:16:53Z},
  annote = {AJAX},
  institution = {INRIA},
  urldate = {2016-10-10},
  language = {en},
  type = {report}
}

@article{BT:Survey:15,
  author = {Brizan, David Guy and Tansel, Abdullah Uz},
  title = {A. {{Survey}} of {{Entity Resolution}} and {{Record Linkage Methodologies}}},
  year = {2015},
  journal = {Communications of the IIMA},
  volume = {6},
  number = {3},
  pages = {5},
  url = {http://scholarworks.lib.csusb.edu/ciima/vol6/iss3/5/?utm_source=scholarworks.lib.csusb.edu\%2Fciima\%2Fvol6\%2Fiss3\%2F5\&utm_medium=PDF\&utm_campaign=PDFCoverPages},
  file = {Brizan and Tansel - 2015 - A. Survey of Entity Resolution and Record Linkage .pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/BUZZVR24/Brizan and Tansel - 2015 - A. Survey of Entity Resolution and Record Linkage .pdf:application/pdf;"A. Survey of Entity Resolution and Record Linkage Methodologies" by David Guy Brizan and Abdullah Uz Tansel:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/PA4AUFPQ/5.html:text/html},
  timestamp = {2016-10-10T12:01:09Z},
  urldate = {2016-10-10}
}

@inproceedings{LTH:Address:12,
  author = {Liu, B. and Topaloglu, U. and Hogan, W. R.},
  title = {Address and {{Participant Entity}}-{{Resolution}} in a {{Large}}, {{Cohort Observational Study Utilizing}} an {{Open}}-Source {{Entity Resolution Tool}} ({{OYSTER}})},
  year = {2012},
  booktitle = {Proceedings of the {{International Conference}} on {{Information}} and {{Knowledge Engineering}} ({{IKE}})},
  pages = {1},
  url = {http://search.proquest.com/openview/7edcb89e055491b0ed4f8f6f47a58713/1?pq-origsite=gscholar},
  file = {7edcb89e055491b0ed4f8f6f47a58713.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/SDEMTUNW/7edcb89e055491b0ed4f8f6f47a58713.pdf:application/pdf},
  timestamp = {2016-10-10T12:02:10Z},
  publisher = {{The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)}},
  urldate = {2016-10-10}
}

@inproceedings{MAS:Graph:14,
  author = {Malhotra, Pankaj and Agarwal, Puneet and Shroff, Gautam},
  title = {Graph-{{Parallel Entity Resolution}} Using {{LSH}} \& {{IMM}}.},
  year = {2014},
  booktitle = {{{EDBT}}/{{ICDT Workshops}}},
  pages = {41--49},
  url = {http://ceur-ws.org/Vol-1133/paper-07.pdf},
  file = {Malhotra et al. - 2014 - Graph-Parallel Entity Resolution using LSH & IMM..pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/C5E9W4S5/Malhotra et al. - 2014 - Graph-Parallel Entity Resolution using LSH & IMM..pdf:application/pdf},
  timestamp = {2016-10-11T09:12:35Z},
  urldate = {2016-10-11}
}

@article{HFH.EA:WEKA:09,
  author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
  shorttitle = {The {{WEKA}} Data Mining Software},
  title = {The {{WEKA}} Data Mining Software: An Update},
  year = {2009},
  journal = {ACM SIGKDD explorations newsletter},
  volume = {11},
  number = {1},
  pages = {10--18},
  url = {http://dl.acm.org/citation.cfm?id=1656278},
  file = {Hall et al. - 2009 - The WEKA data mining software an update.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/QHGF74R3/Hall et al. - 2009 - The WEKA data mining software an update.pdf:application/pdf;Weka 3 - Data Mining with Open Source Machine Learning Software in Java:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/9RSJAH5J/weka.html:text/html},
  timestamp = {2016-10-14T08:17:57Z},
  urldate = {2016-10-11}
}

@misc{.EA:Benchmark:,
  title = {Benchmark Datasets for Entity Resolution | {{Database Group Leipzig}}},
  url = {http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution},
  file = {Benchmark datasets for entity resolution | Database Group Leipzig:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/R58ZHMKD/benchmark_datasets_for_entity_resolution.html:text/html},
  timestamp = {2016-10-11T12:58:54Z},
  urldate = {2016-10-11}
}

@article{KTR:Evaluation:10,
  author = {K{\"o}pcke, Hanna and Thor, Andreas and Rahm, Erhard},
  title = {Evaluation of Entity Resolution Approaches on Real-World Match Problems},
  year = {2010},
  journal = {Proceedings of the VLDB Endowment},
  volume = {3},
  number = {1-2},
  pages = {484--493},
  url = {http://dl.acm.org/citation.cfm?id=1920904},
  file = {EvaluationOfEntityResolutionApproaches_vldb2010_CameraReady.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/T95UX97Z/EvaluationOfEntityResolutionApproaches_vldb2010_CameraReady.pdf:application/pdf},
  timestamp = {2016-10-11T12:59:26Z},
  urldate = {2016-10-11}
}

@article{DN:DuDe:10,
  author = {Draisbach, Uwe and Naumann, Felix},
  shorttitle = {{{DuDe}}},
  title = {{{DuDe}}: {{The Duplicate Detection Toolkit}}},
  year = {2010},
  journal = {ResearchGate},
  url = {https://www.researchgate.net/publication/228705072_DuDe_The_Duplicate_Detection_Toolkit},
  file = {Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.pdf:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/RX2Z9NQV/Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.pdf:application/pdf;Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.html:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/2AKT85SR/Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.html:text/html},
  abstract = {Duplicate detection, also known as entity matching or record linkage, was first defined by Newcombe et al. [19] and has been a research topic for several decades. The challenge is to effectively...},
  timestamp = {2016-10-11T13:08:04Z},
  urldate = {2016-10-11}
}

@article{GRS:Introducing:96,
  author = {Gilks, Walter R. and Richardson, Sylvia and Spiegelhalter, David J.},
  title = {Introducing Markov Chain Monte Carlo},
  year = {1996},
  journal = {Markov chain Monte Carlo in practice},
  volume = {1},
  pages = {19},
  url = {https://books.google.de/books?hl=de\&lr=\&id=TRXrMWY_i2IC\&oi=fnd\&pg=PA1\&dq=Markov+Chain+Monte+Carlo+in+Practice\&ots=7hYqxsNBqt\&sig=6ckJK4aXtrJKYBTp7ltufYwN5dw},
  file = {[PDF] utas.edu.au:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/HT6EJ4QV/Gilks et al. - 1996 - Introducing markov chain monte carlo.pdf:application/pdf;Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/3D6N4HT7/books.html:text/html},
  timestamp = {2016-10-17T10:36:29Z},
  urldate = {2016-10-17}
}

@misc{.EA:Publications:16,
  title = {Publications by Type},
  year = {2016-08-24T10:41:19+02:00},
  url = {https://hpi.de/nc/naumann/publications/publications-by-type/year/2016/103040.html},
  file = {Snapshot:/home/kevinsapper/.mozilla/firefox/imf34ncq.default/zotero/storage/X6EXDBKE/103040.html:text/html},
  timestamp = {2016-10-16T19:07:56Z},
  urldate = {2016-10-16}
}

@comment{jabref-meta: groupsversion:3;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Master_Thesis\;0\;KBHR:study:11\;KR:Parallel:13\;SK:St
orm:15\;EIV:Duplicate:07\;KR:Frameworks:10\;CM:Summarizing:05\;AMK:QuE
Ry:15\;CG:Scalable:08\;Seh:Evaluierung:13\;MAK:Data:10\;KDS+:Magellan:
16\;Tir:EFFICIENT:16\;Kul:Recommendation:15\;HCTL:Duplicate:03\;PKM:Le
xicon:14\;WMG:Pay:13\;RH:Coreference:10\;SD:Entity:06\;Kop:Object:14\;
BG:Collective:07\;Joa:Svmlight:99\;PMM+:Identity:02\;SB:Interactive:02
\;GIKS:Text:03\;KRJK:Removing:12\;GFS+:Declarative:01\;BT:Survey:15\;L
TH:Address:12\;MAS:Graph:14\;HFH+:WEKA:09\;+:Benchmark:\;KTR:Evaluatio
n:10\;DN:DuDe:10\;GRS:Introducing:96\;;
}

