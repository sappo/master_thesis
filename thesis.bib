
@inproceedings{CG:Scalable:08,
  author = {Christen, Peter and Gayler, Ross},
  title = {Towards {{Scalable Real}}-{{Time Entity Resolution}} Using a {{S}} Imilarity-{{Aware Inverted Index Approach}}},
  year = {2008},
  booktitle = {Proceedings of the 7th {{Australasian Data Mining Conference}} - {{Volume}} 87},
  isbn = {978-1-920682-68-2},
  pages = {51--60},
  url = {http://dl.acm.org/citation.cfm?id=2449288.2449299},
  file = {Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KB3PP5VE/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:application/pdf;Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/27NVH45F/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.html:text/html},
  keywords = {approximate string comparisons,data matching,record linkage,Scalability,similarity measures},
  abstract = {Most research into entity resolution (also known as record linkage or data matching) has concentrated on the quality of the matching results. In this paper, we focus on matching time and scalability, with the aim to achieve large-scale real-time entity resolution. Traditional entity resolution techniques have assumed the matching of two static databases. In our networked and online world, however, it is becoming increasingly important for many organisations to be able to conduct entity resolution between a collection of often very large databases and a stream of query or update records. The matching should be done in (near) real-time, and be as automatic and accurate as possible, returning a ranked list of matched records for each given query record. This task therefore becomes similar to querying large document collections, as done for example by Web search engines, however based on a different type of documents: structured database records that, for example, contain personal information, such as names and addresses. In this paper, we investigate inverted indexing techniques, as commonly used in Web search engines, and employ them for real-time entity resolution. We present two variations of the traditional inverted index approach, aimed at facilitating fast approximate matching. We show encouraging initial results on large real-world data sets, with the inverted index approaches being up-to one hundred times faster than the traditionally used standard blocking approach. However, this improved matching speed currently comes at a cost, in that matching quality for larger data sets can be lower compared to when standard blocking is used, and thus more work is required.},
  timestamp = {2016-11-21T09:07:18Z},
  groups = {Master\_Thesis},
  publisher = {{Australian Computer Society, Inc.}},
  urldate = {2016-10-07},
  series = {AusDM '08},
  address = {Darlinghurst, Australia, Australia}
}

@phdthesis{Seh:Evaluierung:13,
  author = {Sehili, Ziad},
  title = {{Evaluierung und Erweiterung von MapReduce-Algorithmen zur Berechnung der transitiven H{\"u}lle ungerichteter Graphen f{\"u}r Entity Resolution Workflows}},
  year = {2013},
  url = {http://lips.informatik.uni-leipzig.de/files/trasitive_closure_with_mapreduce_0.pdf},
  file = {... - trasitive_closure_with_mapreduce_0.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/X5FG4QMH/trasitive_closure_with_mapreduce_0.pdf:application/pdf},
  abstract = {Im  Bereich  von
Entity-Resolution
oder
deduplication
werden  aufgrund  fehlen-
der  global  eindeutiger  Identifikatoren  Match-Techniken  verwendet,  um  zu
bestimmen,  ob  verschiedene  Datens{\"a}tze  dasselbe  Realweltobjekt  darstellen.
Die  inh{\"a}rente  quadratische  Komplexit{\"a}t  f{\"u}hrt  zu  sehr  langen  Laufzeiten  f{\"u}r
gro{\ss}e  Datenmengen,  was  eine  Parallelisierung  dieses  Prozesses  erfordert.
MapReduce  ist  wegen  seiner  Skalierbarkeit  und  Einsetzbarkeit  in  Cloud-
Infrastrukturen  eine  gute  L{\"o}sung  zur  Verbesserung  der  Laufzeit.  Au{\ss}erdem
kann unter bestimmten Voraussetzungen die Qualit{\"a}t des Match-Ergebnisses
durch  die  Berechnung  der  transitiven  H{\"u}lle  verbessert  werden.  Die  Berech-
nung  der  transitiven  H{\"u}lle  eines  Graphen  ist  von  Natur  aus  ein  iterativer
Prozess. Ein naiver Ansatz berechnet sie
linear
, i.e. nach
d
Iterationen, wobei
d
die  Tiefe  des  Graphen  ist.  In  dieser  Arbeit  wird  am  Beispiel  der  Entity
Resolution  die  Verwendung  von  MapReduce  f{\"u}r  die  iterative  und  verteilte
Berechnung der transitiven H{\"u}lle untersucht. Der vorgeschlagene Algorithmus
Smart-MR
operiert nur auf azyklischen Graphen und konvergiert nach genau
log
d
Iterationen. Die drei weiteren Algorithmen
Cyc-Smart-MR
,
Full-TC-MR
und
CC-MR
arbeiten alle auf beliebigen ungerichteten Graphen und weisen ebenso
ein logarithmisches Verhalten auf.},
  timestamp = {2016-10-11T08:56:18Z},
  groups = {Master\_Thesis},
  school = {Universit{\"a}t Leipzig},
  urldate = {2016-10-09},
  language = {deutsch}
}

@inproceedings{MAK:Data:10,
  author = {Maddodi, S. and Attigeri, G. V. and Karunakar, A. K.},
  title = {Data {{Deduplication Techniques}} and {{Analysis}}},
  year = {2010},
  month = nov,
  booktitle = {2010 3rd {{International Conference}} on {{Emerging Trends}} in {{Engineering}} and {{Technology}} ({{ICETET}})},
  pages = {664--668},
  doi = {10.1109/ICETET.2010.42},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FHHZE6ME/Maddodi et al. - 2010 - Data Deduplication Techniques and Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/57AIHQ2T/5698409.html:text/html;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/BJMMPD93/5698409.html:text/html},
  keywords = {data cleaning,data deduplication,data extraction,data loading,data mining,data sources,data transformation,data warehouses,decision support applications,decision support systems,Deduplication,ETL,Microsoft SQL-Server 2008,OLTP,on line transaction processing,SQL,transaction processing},
  abstract = {Data warehouses are the repositories of data collected from several data sources, which form the backbone of most of the decision support applications. As the data sources are independent, they may adopt independent and potentially inconsistent conventions. In data warehousing applications during ETL (Extraction, Transformation and Loading) or even in OLTP (On Line Transaction Processing) applications we are often encountered with duplicate records in table. Moreover, data entry mistakes at any of these sources introduce more errors. Since high quality data is essential for gaining the confidence of users of decision support applications, ensuring high data quality is critical to the success of data warehouse implementations. Therefore, significant amount of time and money are spent on the process of detecting and correcting errors and inconsistencies. The process of cleaning dirty data is often referred to as data cleaning. To make the table data consistent and accurate we need to get rid of these duplicate records from the table. In this paper we discuss different strategies of Deduplication along with their pros and cons and some of methods used to prevent duplication in database. In addition, we have made performance evaluation with Microsoft SQL-Server 2008 on Food Mart and AdventureDB Warehouses.},
  timestamp = {2016-10-11T08:55:32Z},
  groups = {Master\_Thesis}
}

@inproceedings{LTH:Address:12,
  author = {Liu, B. and Topaloglu, U. and Hogan, W. R.},
  title = {Address and {{Participant Entity}}-{{Resolution}} in a {{Large}}, {{Cohort Observational Study Utilizing}} an {{Open}}-Source {{Entity Resolution Tool}} ({{OYSTER}})},
  year = {2012},
  booktitle = {Proceedings of the {{International Conference}} on {{Information}} and {{Knowledge Engineering}} ({{IKE}})},
  pages = {1},
  url = {http://search.proquest.com/openview/7edcb89e055491b0ed4f8f6f47a58713/1?pq-origsite=gscholar},
  file = {7edcb89e055491b0ed4f8f6f47a58713.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/SDEMTUNW/7edcb89e055491b0ed4f8f6f47a58713.pdf:application/pdf},
  timestamp = {2016-10-10T12:02:10Z},
  groups = {Master\_Thesis},
  publisher = {{The Steering Committee of The World Congress in Computer Science, Computer Engineering and Applied Computing (WorldComp)}},
  urldate = {2016-10-10}
}

@article{KR:Parallel:13,
  author = {Kolb, Lars and Rahm, Erhard},
  title = {Parallel Entity Resolution with Dedoop},
  year = {2013},
  journal = {Datenbank-Spektrum},
  volume = {13},
  number = {1},
  pages = {23--32},
  url = {http://link.springer.com/article/10.1007/s13222-012-0110-x},
  file = {Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PAMWB8N8/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:application/pdf;Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QKBKW45K/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.html:text/html},
  timestamp = {2016-10-11T08:51:47Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-06}
}

@article{KDS.EA:Magellan:16,
  author = {Konda, Pradap and Das, Sanjib and Suganthan G. C., Paul and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R. and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay},
  shorttitle = {Magellan},
  title = {Magellan: {{Toward Building Entity Matching Management Systems}}},
  year = {2016},
  month = aug,
  journal = {Proc. VLDB Endow.},
  volume = {9},
  number = {12},
  pages = {1197--1208},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/2994509.2994535},
  doi = {10.14778/2994509.2994535},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/I5JI48JD/Konda et al. - 2016 - Magellan Toward Building Entity Matching Manageme.pdf:application/pdf},
  abstract = {Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick "patching" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach.},
  timestamp = {2016-10-14T08:17:30Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-07}
}

@phdthesis{Tir:EFFICIENT:16,
  author = {Tirumali, Parineetha Gandhi},
  title = {{{EFFICIENT PAIR}}-{{WISE SIMILARITY COMPUTATION USING APACHE SPARK}}},
  year = {2016},
  url = {http://scholarworks.sjsu.edu/etd_projects/479},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/S5CC6TID/Tirumali - 2016 - EFFICIENT PAIR-WISE SIMILARITY COMPUTATION USING A.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4GCKPVVN/38.html:text/html},
  abstract = {Entity matching is the process of identifying different manifestations of the same real world entity. These entities can be referred to as objects(string) or data instances. These entities are in turn split over several databases or clusters based on the signatures of the entities. When entity matching algorithms are performed on these databases or clusters, there is a high possibility that a particular entity pair is compared more than once. The number of comparison for any two entities depend on the number of common signatures or keys they possess. This effects the performance of any entity matching algorithm. This paper is the implementation of the algorithm written by Erhard Rahm et al. for performing redundancy free pair-wise similarity computation using MapReduce. As an improvisation to the existing implementation, this project aims to implement the algorithm in Apache Spark in standalone mode for sample of data and in cluster mode for large volume of data.},
  timestamp = {2016-10-11T08:56:09Z},
  groups = {Master\_Thesis},
  school = {San Jose State University},
  urldate = {2016-10-07}
}

@incollection{CM:Summarizing:05,
  author = {Cormode, G. and Muthukrishnan, S.},
  title = {Summarizing and {{Mining Skewed Data Streams}}},
  year = {2005},
  month = apr,
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  isbn = {978-0-89871-593-4},
  pages = {44--55},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.5},
  file = {Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MGR2ATBM/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:application/pdf;Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/G6AP6IGJ/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.html:text/html},
  abstract = {Many applications generate massive data streams. Summarizing such massive data requires fast, small space algorithms to support post-hoc queries and mining. An important observation is that such streams are rarely uniform, and real data sources typically exhibit significant skewness. These are well modeled by Zipf distributions, which are characterized by a parameter, z, that captures the amount of skew. We present a data stream summary that can answer point queries with $\smallin$ accuracy and show that the space needed is only O($\smallin$--min\{1,1/z\}). This is the first o(1/$\smallin$) space algorithm for this problem, and we show it is essentially tight for skewed distributions. We show that the same data structure can also estimate the L2 norm of the stream in o(1/$\smallin$2) space for z $>$ \textonehalf, another improvement over the existing $\Omega$(1/$\smallin$2) methods. We support our theoretical results with an experimental study over a large variety of real and synthetic data. We show that significant skew is present in both textual and telecommunication data. Our methods give strong accuracy, significantly better than other methods, and behave exactly in line with their analytic bounds.},
  timestamp = {2016-10-11T08:57:12Z},
  groups = {Master\_Thesis},
  publisher = {{Society for Industrial and Applied Mathematics}},
  urldate = {2016-10-07},
  series = {Proceedings}
}

@phdthesis{Kul:Recommendation:15,
  author = {Kulkarni, Swapna},
  title = {A {{Recommendation Engine Using Apache Spark}}},
  year = {2015},
  url = {http://scholarworks.sjsu.edu/etd_projects/456},
  file = {Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/P3IFV5FS/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:application/pdf;Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5WS74UFU/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.html:text/html},
  abstract = {The volume of structured and unstructured data has grown at exponential scale in recent days. As a result of this rapid data growth, we are always inundated with plethora of choices in any product or service. It is very natural to get lost in the amazon of such choices and finding hard to make decisions. The project aims at addressing this problem by using entity recommendation. The two main aspects that the project concentrates on are implementing and presenting more accurate entity recommendations to the user and another is dealing with vast amount of data. The project aims at presenting recommendation results according to user's query with efficiency and accuracy. Project makes use of ListNet ranking algorithm to rank the recommendation results. Query independent features and query dependent features are used to come up with ranking scores. Ranking scores decide the order in which the recommendation results are presented to the user. Project makes use of Apache Spark, a distributed bigdata processing framework. Spark gives the advantage of handling iterative and interactive algorithms with efficiency and minimal processing time as compared to traditional mapreduce paradigm. We performed the experiments for recommendation engine using DBPedia as the dataset and tested the results for movie domain. We used both queryindependent (pagerank) and querydependent (clicklogs) features for ranking purposes. We observed that ListNet algorithm performs really well by making use of Apache Spark as the RDDs provide faster way for iterative algorithms to execute. We also observed that the results of recommendation engine are accurate and the entities are well ranked.},
  timestamp = {2016-10-11T08:55:12Z},
  groups = {Master\_Thesis},
  school = {San Jose State University},
  urldate = {2016-10-07}
}

@article{AMK:QuERy:15,
  author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
  shorttitle = {{{QuERy}}},
  title = {{{QuERy}}: {{A Framework}} for {{Integrating Entity Resolution}} with {{Query Processing}}},
  year = {2015},
  month = nov,
  journal = {Proc. VLDB Endow.},
  volume = {9},
  number = {3},
  pages = {120--131},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/2850583.2850587},
  doi = {10.14778/2850583.2850587},
  file = {Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3EZ9K7HW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:application/pdf;Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ECSQGCBW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.html:text/html},
  abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
  timestamp = {2016-10-11T08:53:19Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-07}
}

@inproceedings{PMM.EA:Identity:02,
  author = {Pasula, Hanna and Marthi, Bhaskara and Milch, Brian and Russell, Stuart and Shpitser, Ilya},
  title = {Identity Uncertainty and Citation Matching},
  year = {2002},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {1401--1408},
  url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AP01.pdf},
  file = {[PDF] wustl.edu:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4UJ26N4J/Pasula et al. - 2002 - Identity uncertainty and citation matching.pdf:application/pdf;[PDF] wustl.edu:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/78NAN8XS/Pasula et al. - 2002 - Identity uncertainty and citation matching.pdf:application/pdf},
  timestamp = {2016-10-14T08:17:13Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-10}
}

@misc{HCTL:Duplicate:03,
  author = {Hern{\'a}ndez, Mauricio and Cohen, William and Tejada, Sheila and Lawrence, Steve},
  title = {Duplicate {{Detection}}, {{Record Linkage}}, and {{Identity Uncertainty}}: {{Datasets}}},
  year = {2003},
  url = {http://www.cs.utexas.edu/users/ml/riddle/data.html},
  file = {Duplicate Detection, Record Linkage, and Identity Uncertainty\: Datasets:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5AQ7F8BX/data.html:text/html},
  timestamp = {2016-10-11T12:57:11Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-09}
}

@article{Joa:Svmlight:99,
  author = {Joachims, Thorsten},
  shorttitle = {Svmlight},
  title = {Svmlight: {{Support}} Vector Machine},
  year = {1999},
  journal = {SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund},
  volume = {19},
  number = {4},
  url = {https://akela.mendelu.cz/~zizka/Machine_Learning/SVM/SVM-light/SVM-Light\%20Support\%20Vector\%20Ma...pdf},
  file = {Joachims - 1999 - Svmlight Support vector machine.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DE6C6F8D/Joachims - 1999 - Svmlight Support vector machine.pdf:application/pdf},
  timestamp = {2016-10-11T08:57:18Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-10}
}

@article{DN:DuDe:10,
  author = {Draisbach, Uwe and Naumann, Felix},
  shorttitle = {{{DuDe}}},
  title = {{{DuDe}}: {{The Duplicate Detection Toolkit}}},
  year = {2010},
  journal = {ResearchGate},
  url = {https://www.researchgate.net/publication/228705072_DuDe_The_Duplicate_Detection_Toolkit},
  file = {Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/RX2Z9NQV/Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.pdf:application/pdf;Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2AKT85SR/Draisbach and Naumann - DuDe The Duplicate Detection Toolkit.html:text/html},
  abstract = {Duplicate detection, also known as entity matching or record linkage, was first defined by Newcombe et al. [19] and has been a research topic for several decades. The challenge is to effectively...},
  timestamp = {2016-10-11T13:08:04Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-11}
}

@article{BT:Survey:15,
  author = {Brizan, David Guy and Tansel, Abdullah Uz},
  title = {A. {{Survey}} of {{Entity Resolution}} and {{Record Linkage Methodologies}}},
  year = {2015},
  journal = {Communications of the IIMA},
  volume = {6},
  number = {3},
  pages = {5},
  url = {http://scholarworks.lib.csusb.edu/ciima/vol6/iss3/5/?utm_source=scholarworks.lib.csusb.edu\%2Fciima\%2Fvol6\%2Fiss3\%2F5\&utm_medium=PDF\&utm_campaign=PDFCoverPages},
  file = {Brizan and Tansel - 2015 - A. Survey of Entity Resolution and Record Linkage .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/BUZZVR24/Brizan and Tansel - 2015 - A. Survey of Entity Resolution and Record Linkage .pdf:application/pdf;"A. Survey of Entity Resolution and Record Linkage Methodologies" by David Guy Brizan and Abdullah Uz Tansel:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PA4AUFPQ/5.html:text/html},
  timestamp = {2016-10-10T12:01:09Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-10}
}

@inproceedings{MAS:Graph:14,
  author = {Malhotra, Pankaj and Agarwal, Puneet and Shroff, Gautam},
  title = {Graph-{{Parallel Entity Resolution}} Using {{LSH}} \& {{IMM}}.},
  year = {2014},
  booktitle = {{{EDBT}}/{{ICDT Workshops}}},
  pages = {41--49},
  url = {http://ceur-ws.org/Vol-1133/paper-07.pdf},
  file = {Malhotra et al. - 2014 - Graph-Parallel Entity Resolution using LSH & IMM..pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/C5E9W4S5/Malhotra et al. - 2014 - Graph-Parallel Entity Resolution using LSH & IMM..pdf:application/pdf},
  timestamp = {2016-10-11T09:12:35Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-11}
}

@article{KBHR:study:11,
  author = {Kwon, YongChul and Balazinska, Magdalena and Howe, Bill and Rolia, Jerome},
  title = {A Study of Skew in Mapreduce Applications},
  year = {2011},
  journal = {Open Cirrus Summit},
  url = {http://ejournal.narotama.ac.id/files/A\%20study\%20of\%20skew\%20in\%20mapreduce\%20applications.pdf},
  file = {Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DNIQDC4X/Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:application/pdf},
  timestamp = {2016-10-11T08:55:16Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-06}
}

@inproceedings{SK:Storm:15,
  author = {Shah, M. A. and Kulkarni, D. B.},
  shorttitle = {Storm {{Pub}}-{{Sub}}},
  title = {Storm {{Pub}}-{{Sub}}: {{High Performance}}, {{Scalable Content Based Event Matching System Using Storm}}},
  year = {2015},
  month = may,
  booktitle = {Parallel and {{Distributed Processing Symposium Workshop}} ({{IPDPSW}}), 2015 {{IEEE International}}},
  pages = {585--590},
  doi = {10.1109/IPDPSW.2015.95},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TVN25BWB/Shah and Kulkarni - 2015 - Storm Pub-Sub High Performance, Scalable Content .pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8JIUZ3UG/7284361.html:text/html},
  keywords = {Algorithm design and analysis,bolts,broker based architecture,broker overlays,content based pub-sub system,Distributed cluster,distributed cluster framework,Fasteners,high performance publish subscribe system,high performance scalable content based event matching system,High Throughput,local cluster,Matching time,message passing,middleware,overlay management,parallel processing,pattern matching,Publish subscribe,Publish subscribe systems,pub-sub system Siena,routing functionality,Scalability,software reliability,Storm pub-sub,Storms,Throughput,Topology},
  abstract = {Storm pub-sub is a novel high performance publish subscribe system designed to efficiently match events and the subscriptions with high throughput. Moving a content based pub-sub system first to a local cluster and then to a distributed cluster framework is for high performance and scalability. We depart from the use of broker overlays, where each server must support the whole range of operations of a pub-sub service, as well as overlay management and routing functionality. In this system different operations involved in pub-sub are separated to leverage their natural potential for parallelization using bolts. The storm pub-sub is compared with the traditional pub-sub system Siena, a broker based architecture. Through experimentation on local cluster as well as on distributed cluster we show that our approach of designing publish subscribe system on storm scales well for high volume of data. Storm pub-sub system approximately produces 2200 event/s on distributed cluster. In this paper we describe design and implementation of storm pub-sub and evaluate it in terms of scalability and throughput.},
  timestamp = {2016-10-11T08:57:05Z},
  groups = {Master\_Thesis}
}

@article{PKM:Lexicon:14,
  author = {Passos, Alexandre and Kumar, Vineet and McCallum, Andrew},
  title = {Lexicon {{Infused Phrase Embeddings}} for {{Named Entity Resolution}}},
  year = {2014},
  month = apr,
  journal = {arXiv:1404.5367 [cs]},
  url = {http://arxiv.org/abs/1404.5367},
  file = {arXiv\:1404.5367 PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/T2SVD7G4/Passos et al. - 2014 - Lexicon Infused Phrase Embeddings for Named Entity.pdf:application/pdf;arXiv.org Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3XX7SWCX/1404.html:text/html},
  keywords = {Computer Science - Computation and Language},
  abstract = {Most state-of-the-art approaches for named-entity recognition (NER) use semi supervised information in the form of word clusters and lexicons. Recently neural network-based language models have been explored, as they as a byproduct generate highly informative vector representations for words, known as word embeddings. In this paper we present two contributions: a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations, and the first system to use neural word embeddings to achieve state-of-the-art results on named-entity recognition in both CoNLL and Ontonotes NER. Our system achieves an F1 score of 90.90 on the test set for CoNLL 2003---significantly better than any previous system trained on public data, and matching a system employing massive private industrial query-log data.},
  timestamp = {2016-10-11T08:48:08Z},
  groups = {Master\_Thesis},
  annote = {Comment: Accepted in CoNLL 2014},
  urldate = {2016-10-09},
  primaryClass = {cs},
  eprint = {1404.5367},
  eprinttype = {arxiv},
  archivePrefix = {arXiv}
}

@article{EIV:Duplicate:07,
  author = {Elmagarmid, A. K. and Ipeirotis, P. G. and Verykios, V. S.},
  shorttitle = {Duplicate {{Record Detection}}},
  title = {Duplicate {{Record Detection}}: {{A Survey}}},
  year = {2007},
  month = jan,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {19},
  number = {1},
  pages = {1--16},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2007.250581},
  file = {Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UQHE5W3P/Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2C5RCMZH/4016511.html:text/html;Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8VPZFZ4N/Elmagarmid et al. - 2007 - Duplicate Record Detection A Survey.html:text/html},
  keywords = {Cleaning,Computer errors,Computer Society,Cost function,Couplings,database hardening,database management system,database management systems,data cleaning,data deduplication,data integration,data integrity,data mining,Detection algorithms,Duplicate detection,duplicate detection algorithm,duplicate record detection,entity matching.,entity resolution,fuzzy duplicate detection,identity uncertainty,instance identification,Mirrors,name matching,record linkage,Relational databases,Scalability,transcription error,Uncertainty},
  abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
  timestamp = {2016-10-11T08:55:52Z},
  groups = {Master\_Thesis}
}

@article{KRJK:Removing:12,
  author = {Khan, Bilal and Rauf, Azhar and Javed, Huma and Khusro, Shah},
  title = {Removing Fully and Partially Duplicated Records through {{K}}-{{Means}} Clustering},
  year = {2012},
  journal = {International Journal of Engineering and Technology},
  volume = {4},
  number = {6},
  pages = {750},
  url = {http://search.proquest.com/openview/588b8624a99372af77a8a672824dbc10/1?pq-origsite=gscholar},
  file = {588b8624a99372af77a8a672824dbc10.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/SDAFXQ82/588b8624a99372af77a8a672824dbc10.pdf:application/pdf},
  timestamp = {2016-10-10T11:54:20Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-10}
}

@inproceedings{GIKS:Text:03,
  author = {Gravano, Luis and Ipeirotis, Panagiotis G. and Koudas, Nick and Srivastava, Divesh},
  title = {Text Joins in an {{RDBMS}} for Web Data Integration},
  year = {2003},
  booktitle = {Proceedings of the 12th International Conference on {{World Wide Web}}},
  pages = {90--101},
  url = {http://dl.acm.org/citation.cfm?id=775166},
  file = {Gravano et al. - 2003 - Text joins in an RDBMS for web data integration.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/XPDDA6NE/Gravano et al. - 2003 - Text joins in an RDBMS for web data integration.html:text/html},
  timestamp = {2016-10-11T08:57:22Z},
  groups = {Master\_Thesis},
  annote = {Q-grams with tf.idf},
  publisher = {{ACM}},
  urldate = {2016-10-10}
}

@techreport{GFS.EA:Declarative:01,
  author = {Galhardas, Helena and Florescu, Daniela and Shasha, Dennis and Simon, Eric and Saita, Cristian},
  shorttitle = {Declarative {{Data Cleaning}}},
  title = {Declarative {{Data Cleaning}} : {{Language}}, {{Model}}, and {{Algorithms}}},
  year = {2001},
  url = {https://hal.inria.fr/inria-00072476/document},
  file = {Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8I934KA5/Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.pdf:application/pdf;Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4E32KCBD/Galhardas et al. - 2001 - Declarative Data Cleaning  Language, Model, and A.html:text/html},
  abstract = {The problem of data cleaning, which consists of emoving inconsistencies and errors from original data sets, is well known in the area of decision support systems and data warehouses. However, for non-conventional applications, such as the migration of largely unstructured data into structured one, or the integration of heterogeneous scientific data sets in inter-discipl- inary fields (e.g., in environmental science), existing ETL (Extraction Transformation Loading) and data cleaning tools for writing data cleaning programs are insufficient. The main challenge with them is the design of a data flow graph that effectively generates clean data, and can perform efficiently on large sets of input data. The difficulty with them comes from (i) a lack of clear separation between the logical specification of data transformations and their physical implementation and (ii) the lack of explanation of cleaning results and user interaction facilities to tune a data cleaning program. This paper addresses these two problems and presents a language, an execution model and algorithms that enable users to express data cleaning specifications declaratively and perform the cleaning efficiently. We use as an example a set of bibliographic references used to construct the Citeseer Web site. The underlying data integration problem is to derive structured and clean textual records so that meaningful queries can be performed. Experimental results report on the assessement of the proposed framework for data cleaning.},
  timestamp = {2016-10-14T08:16:53Z},
  groups = {Master\_Thesis},
  annote = {AJAX},
  institution = {INRIA},
  urldate = {2016-10-10},
  language = {en},
  type = {report}
}

@article{HFH.EA:WEKA:09,
  author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H.},
  shorttitle = {The {{WEKA}} Data Mining Software},
  title = {The {{WEKA}} Data Mining Software: An Update},
  year = {2009},
  journal = {ACM SIGKDD explorations newsletter},
  volume = {11},
  number = {1},
  pages = {10--18},
  url = {http://dl.acm.org/citation.cfm?id=1656278},
  file = {Hall et al. - 2009 - The WEKA data mining software an update.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QHGF74R3/Hall et al. - 2009 - The WEKA data mining software an update.pdf:application/pdf;Weka 3 - Data Mining with Open Source Machine Learning Software in Java:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/9RSJAH5J/weka.html:text/html},
  timestamp = {2016-10-14T08:17:57Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-11}
}

@misc{.EA:Benchmark:,
  title = {Benchmark Datasets for Entity Resolution | {{Database Group Leipzig}}},
  url = {http://dbs.uni-leipzig.de/en/research/projects/object_matching/fever/benchmark_datasets_for_entity_resolution},
  file = {Benchmark datasets for entity resolution | Database Group Leipzig:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/R58ZHMKD/benchmark_datasets_for_entity_resolution.html:text/html},
  timestamp = {2016-10-11T12:58:54Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-11}
}

@article{WMG:PayAsYouGo:13,
  author = {Whang, S. E. and Marmaros, D. and Garcia-Molina, H.},
  title = {Pay-{{As}}-{{You}}-{{Go Entity Resolution}}},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {25},
  number = {5},
  pages = {1111--1124},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2012.43},
  file = {Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UX23534Q/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:application/pdf;Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VP2G2BSJ/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.html:text/html},
  keywords = {Approximation algorithms,Clustering algorithms,Companies,data cleaning,Data structures,entity resolution,Erbium,Partitioning algorithms,pay-as-you-go,Tin},
  abstract = {Entity resolution (ER) is the problem of identifying which records in a database refer to the same entity. In practice, many applications need to resolve large data sets efficiently, but do not require the ER result to be exact. For example, people data from the web may simply be too large to completely resolve with a reasonable amount of work. As another example, real-time applications may not be able to tolerate any ER processing that takes longer than a certain amount of time. This paper investigates how we can maximize the progress of ER with a limited amount of work using ``hints,'' which give information on records that are likely to refer to the same real-world entity. A hint can be represented in various formats (e.g., a grouping of records based on their likelihood of matching), and ER can use this information as a guideline for which records to compare first. We introduce a family of techniques for constructing hints efficiently and techniques for using the hints to maximize the number of matching records identified using a limited amount of work. Using real data sets, we illustrate the potential gains of our pay-as-you-go approach compared to running ER without using hints.},
  timestamp = {2016-10-11T08:48:36Z},
  groups = {Master\_Thesis}
}

@article{KTR:Evaluation:10,
  author = {K{\"o}pcke, Hanna and Thor, Andreas and Rahm, Erhard},
  title = {Evaluation of Entity Resolution Approaches on Real-World Match Problems},
  year = {2010},
  journal = {Proceedings of the VLDB Endowment},
  volume = {3},
  number = {1-2},
  pages = {484--493},
  url = {http://dl.acm.org/citation.cfm?id=1920904},
  file = {EvaluationOfEntityResolutionApproaches_vldb2010_CameraReady.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/T95UX97Z/EvaluationOfEntityResolutionApproaches_vldb2010_CameraReady.pdf:application/pdf},
  timestamp = {2016-10-11T12:59:26Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-11}
}

@inproceedings{RH:Coreference:10,
  author = {Recasens, Marta and Hovy, Eduard},
  shorttitle = {Coreference Resolution across Corpora},
  title = {Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information},
  year = {2010},
  month = jul,
  pages = {1423--1432},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858825},
  file = {Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/CPX98HXU/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:application/pdf;Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UIWID5C6/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .html:text/html},
  timestamp = {2016-10-11T08:55:29Z},
  groups = {Master\_Thesis},
  publisher = {{Association for Computational Linguistics}},
  urldate = {2016-10-07}
}

@article{KR:Frameworks:10,
  author = {K{\"o}pcke, Hanna and Rahm, Erhard},
  shorttitle = {Frameworks for Entity Matching},
  title = {Frameworks for Entity Matching: {{A}} Comparison},
  year = {2010},
  month = feb,
  journal = {Data \& Knowledge Engineering},
  volume = {69},
  number = {2},
  pages = {197--210},
  issn = {0169-023X},
  url = {http://www.sciencedirect.com/science/article/pii/S0169023X09001451},
  doi = {10.1016/j.datak.2009.10.003},
  file = {FrameworksForEntityMatchingAComparison_dke.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QK42BHG7/FrameworksForEntityMatchingAComparison_dke.pdf:application/pdf;ScienceDirect Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7CA5U7X4/S0169023X09001451.html:text/html},
  keywords = {Entity matching,entity resolution,Matcher combination,Match optimization,Training selection},
  abstract = {Entity matching is a crucial and difficult task for data integration. Entity matching frameworks provide several methods and their combination to effectively solve different match tasks. In this paper, we comparatively analyze 11 proposed frameworks for entity matching. Our study considers both frameworks which do or do not utilize training data to semi-automatically find an entity matching strategy to solve a given match task. Moreover, we consider support for blocking and the combination of different match algorithms. We further study how the different frameworks have been evaluated. The study aims at exploring the current state of the art in research prototypes of entity matching frameworks and their evaluations. The proposed criteria should be helpful to identify promising framework approaches and enable categorizing and comparatively assessing additional entity matching frameworks and their evaluations.},
  timestamp = {2016-10-11T08:56:24Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-06}
}

@inproceedings{SB:Interactive:02,
  author = {Sarawagi, Sunita and Bhamidipaty, Anuradha},
  title = {Interactive Deduplication Using Active Learning},
  year = {2002},
  booktitle = {Proceedings of the Eighth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  pages = {269--278},
  url = {http://dl.acm.org/citation.cfm?id=775087},
  file = {Sarawagi and Bhamidipaty - 2002 - Interactive deduplication using active learning.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4BK89QBT/Sarawagi and Bhamidipaty - 2002 - Interactive deduplication using active learning.pdf:application/pdf;Sarawagi and Bhamidipaty - 2002 - Interactive deduplication using active learning.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/WN3ZNRIS/Sarawagi and Bhamidipaty - 2002 - Interactive deduplication using active learning.html:text/html},
  timestamp = {2016-10-11T08:48:01Z},
  groups = {Master\_Thesis},
  annote = {ALIAS},
  publisher = {{ACM}},
  urldate = {2016-10-10}
}

@inproceedings{SD:Entity:06,
  author = {Singla, P. and Domingos, P.},
  title = {Entity {{Resolution}} with {{Markov Logic}}},
  year = {2006},
  month = dec,
  booktitle = {Sixth {{International Conference}} on {{Data Mining}} ({{ICDM}}'06)},
  pages = {572--582},
  doi = {10.1109/ICDM.2006.65},
  file = {Singla and Domingos - 2006 - Entity Resolution with Markov Logic.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DRCB6AZ7/Singla and Domingos - 2006 - Entity Resolution with Markov Logic.pdf:application/pdf;Singla and Domingos - 2006 - Entity Resolution with Markov Logic.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/EH5QQMKF/Singla and Domingos - 2006 - Entity Resolution with Markov Logic.html:text/html},
  keywords = {citation databases,Computer science,Couplings,database management systems,Data engineering,data mining,entity-relationship modelling,entity resolution,first-order logic,formal logic,Graphical models,graph theory,inference mechanisms,inference problems,Joining processes,learning (artificial intelligence),learning problems,Logistics,Markov logic,Markov networks,Markov processes,Markov random fields,probabilistic graphical models,Probabilistic logic,probability,Spatial databases},
  abstract = {Entity resolution is the problem of determining which records in a database refer to the same entities, and is a crucial and expensive step in the data mining process. Interest in it has grown rapidly, and many approaches have been proposed. However, they tend to address only isolated aspects of the problem, and are often ad hoc. This paper proposes a well-founded, integrated solution to the entity resolution problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and viewing them as templates for features of Markov networks. We show how a number of previous approaches can be formulated and seamlessly combined in Markov logic, and how the resulting learning and inference problems can be solved efficiently. Experiments on two citation databases show the utility of this approach, and evaluate the contribution of the different components.},
  timestamp = {2016-10-11T08:56:14Z},
  groups = {Master\_Thesis}
}

@phdthesis{Kop:Object:14,
  author = {K{\"o}pcke, Hanna},
  title = {Object Matching on Real-World Problems /},
  year = {2014},
  file = {Dissertation_HannaKöpcke_online.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6TC3N9EZ/Dissertation_HannaKöpcke_online.pdf:application/pdf;Details\: Object matching on real-world problems /:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7BEMI7MH/0013004879.html:text/html},
  timestamp = {2016-10-11T08:50:42Z},
  groups = {Master\_Thesis},
  language = {English}
}

@article{BG:Collective:07,
  author = {Bhattacharya, Indrajit and Getoor, Lise},
  title = {Collective {{Entity Resolution}} in {{Relational Data}}},
  year = {2007},
  month = mar,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {1},
  number = {1},
  issn = {1556-4681},
  url = {http://doi.acm.org/10.1145/1217299.1217304},
  doi = {10.1145/1217299.1217304},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MQXU79QF/Bhattacharya and Getoor - 2007 - Collective Entity Resolution in Relational Data.pdf:application/pdf;Collective entity resolution in relational data:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6HFK6XP5/citation.html:text/html},
  keywords = {data cleaning,entity resolution,graph clustering,record linkage},
  abstract = {Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.},
  timestamp = {2016-10-11T08:55:24Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-09}
}

@article{GRS:Introducing:96,
  author = {Gilks, Walter R. and Richardson, Sylvia and Spiegelhalter, David J.},
  title = {Introducing Markov Chain Monte Carlo},
  year = {1996},
  journal = {Markov chain Monte Carlo in practice},
  volume = {1},
  pages = {19},
  url = {https://books.google.de/books?hl=de\&lr=\&id=TRXrMWY_i2IC\&oi=fnd\&pg=PA1\&dq=Markov+Chain+Monte+Carlo+in+Practice\&ots=7hYqxsNBqt\&sig=6ckJK4aXtrJKYBTp7ltufYwN5dw},
  file = {[PDF] utas.edu.au:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/HT6EJ4QV/Gilks et al. - 1996 - Introducing markov chain monte carlo.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3D6N4HT7/books.html:text/html},
  timestamp = {2016-10-17T10:36:29Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-17}
}

@article{ABE.EA:Stratosphere:14,
  author = {Alexandrov, Alexander and Bergmann, Rico and Ewen, Stephan and Freytag, Johann-Christoph and Hueske, Fabian and Heise, Arvid and Kao, Odej and Leich, Marcus and Leser, Ulf and Markl, Volker and Naumann, Felix and Peters, Mathias and Rheinl{\"a}nder, Astrid and Sax, Matthias J. and Schelter, Sebastian and H{\"o}ger, Mareike and Tzoumas, Kostas and Warneke, Daniel},
  title = {The {{Stratosphere Platform}} for {{Big Data Analytics}}},
  year = {2014},
  month = dec,
  journal = {The VLDB Journal},
  volume = {23},
  number = {6},
  pages = {939--964},
  issn = {1066-8888},
  url = {http://dx.doi.org/10.1007/s00778-014-0357-y},
  doi = {10.1007/s00778-014-0357-y},
  file = {Alexandrov et al. - 2014 - The Stratosphere Platform for Big Data Analytics.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KDZHRWA2/Alexandrov et al. - 2014 - The Stratosphere Platform for Big Data Analytics.pdf:application/pdf},
  keywords = {Big data,Data cleansing,Distributed systems,Graph processing,Parallel databases,Query Optimization,Query processing,Text mining},
  abstract = {We present Stratosphere, an open-source software stack for parallel data analysis. Stratosphere brings together a unique set of features that allow the expressive, easy, and efficient programming of analytical applications at very large scale. Stratosphere's features include "in situ" data processing, a declarative query language, treatment of user-defined functions as first-class citizens, automatic program parallelization and optimization, support for iterative programs, and a scalable and efficient execution engine. Stratosphere covers a variety of "Big Data" use cases, such as data warehousing, information extraction and integration, data cleansing, graph analysis, and statistical analysis applications. In this paper, we present the overall system architecture design decisions, introduce Stratosphere through example queries, and then dive into the internal workings of the system's components that relate to extensibility, programming model, optimization, and query execution. We experimentally compare Stratosphere against popular open-source alternatives, and we conclude with a research outlook for the next years.},
  timestamp = {2016-10-24T08:24:15Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-24}
}

@article{GKZN:CohEEL:16,
  author = {Gruetze, Toni and Kasneci, Gjergji and Zuo, Zhe and Naumann, Felix},
  shorttitle = {{{CohEEL}}},
  title = {{{CohEEL}}: {{Coherent}} and Efficient Named Entity Linking through Random Walks},
  year = {2016},
  journal = {Web Semantics: Science, Services and Agents on the World Wide Web},
  volume = {37},
  pages = {75--89},
  url = {http://www.sciencedirect.com/science/article/pii/S1570826816000172},
  file = {Gruetze et al. - 2016 - CohEEL Coherent and efficient named entity linkin.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UDSQKXIW/Gruetze et al. - 2016 - CohEEL Coherent and efficient named entity linkin.pdf:application/pdf},
  timestamp = {2016-10-25T13:12:37Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-24}
}

@phdthesis{Kol:Effiziente:14,
  author = {Kolb, Lars},
  title = {{Effiziente MapReduce-Parallelisierung von Entity Resolution-Workflows}},
  year = {2014},
  url = {http://nbn-resolving.de/urn:nbn:de:bsz:15-qucosa-157163},
  file = {Kolb - 2014 - Effiziente MapReduce-Parallelisierung von Entity R.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/WQMAWPEG/Kolb - 2014 - Effiziente MapReduce-Parallelisierung von Entity R.pdf:application/pdf},
  timestamp = {2016-10-24T08:36:14Z},
  groups = {Master\_Thesis},
  school = {University of Leipzig},
  language = {deutsch}
}

@inproceedings{Lo:curse:09,
  author = {Lin, Jimmy and {others}},
  shorttitle = {The Curse of Zipf and Limits to Parallelization},
  title = {The Curse of Zipf and Limits to Parallelization: {{A}} Look at the Stragglers Problem in Mapreduce},
  year = {2009},
  booktitle = {7th {{Workshop}} on {{Large}}-{{Scale Distributed Systems}} for {{Information Retrieval}}},
  volume = {1},
  url = {http://www.umiacs.umd.edu/~jimmylin/publications/Lin_2009.pdf},
  file = {[PDF] umd.edu:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DJ2AU6KV/Lin and others - 2009 - The curse of zipf and limits to parallelization A.pdf:application/pdf},
  timestamp = {2016-10-24T07:32:39Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-24}
}

@article{VHD.EA:Reach:14,
  author = {Vogel, Tobias and Heise, Arvid and Draisbach, Uwe and Lange, Dustin and Naumann, Felix},
  shorttitle = {Reach for Gold},
  title = {Reach for Gold: {{An}} Annealing Standard to Evaluate Duplicate Detection Results},
  year = {2014},
  month = sep,
  journal = {Journal of Data and Information Quality},
  volume = {5},
  number = {1-2},
  pages = {1--25},
  issn = {19361955},
  url = {http://dl.acm.org/citation.cfm?doid=2667565.2629687},
  doi = {10.1145/2629687},
  file = {Vogel et al. - 2014 - Reach for gold An annealing standard to evaluate .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/M87Q3WCV/Vogel et al. - 2014 - Reach for gold An annealing standard to evaluate .pdf:application/pdf},
  timestamp = {2016-10-24T08:22:58Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-24},
  language = {en}
}

@article{GBVR:Record:03,
  author = {Gu, Lifang and Baxter, Rohan and Vickers, Deanne and Rainsford, Chris},
  shorttitle = {Record Linkage},
  title = {Record Linkage: {{Current}} Practice and Future Directions},
  year = {2003},
  journal = {CSIRO Mathematical and Information Sciences Technical Report},
  volume = {3},
  pages = {83},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.14.8119\&rep=rep1\&type=pdf},
  file = {Gu et al. - 2003 - Record linkage Current practice and future direct.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/T92825NB/Gu et al. - 2003 - Record linkage Current practice and future direct.pdf:application/pdf},
  timestamp = {2016-10-24T09:22:22Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-24}
}

@inproceedings{KTR:Don:13,
  author = {Kolb, Lars and Thor, Andreas and Rahm, Erhard},
  shorttitle = {Don't Match Twice},
  title = {Don't Match Twice: Redundancy-Free Similarity Computation with {{MapReduce}}},
  year = {2013},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Data Analytics}} in the {{Cloud}}},
  pages = {1--5},
  url = {http://dl.acm.org/citation.cfm?id=2486768},
  file = {Kolb et al. - 2013 - Don't match twice redundancy-free similarity comp.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DPP9BMTF/Kolb et al. - 2013 - Don't match twice redundancy-free similarity comp.pdf:application/pdf},
  timestamp = {2016-10-24T08:32:02Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-10-24}
}

@article{HNST:Scalable:12,
  author = {Herschel, M. and Naumann, F. and Szott, S. and Taubert, M.},
  title = {Scalable {{Iterative Graph Duplicate Detection}}},
  year = {2012},
  month = nov,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {24},
  number = {11},
  pages = {2094--2108},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2011.99},
  file = {Herschel et al. - 2012 - Scalable Iterative Graph Duplicate Detection.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/W7IDEIN5/Herschel et al. - 2012 - Scalable Iterative Graph Duplicate Detection.pdf:application/pdf;Herschel et al. - 2012 - Scalable Iterative Graph Duplicate Detection.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UM2S4QW8/Herschel et al. - 2012 - Scalable Iterative Graph Duplicate Detection.html:text/html},
  keywords = {Classification algorithms,Databases,data cleaning,data integration,DDG,Duplicate detection,duplicate detection in graph data,entity resolution,graph theory,Image edge detection,iterative methods,Motion pictures,object representations,parallelization,real-world objects,record linkage,relational database management system,Relational databases,Runtime,Scalability,scalable iterative graph duplicate detection,Sorting},
  abstract = {Duplicate detection determines different representations of real-world objects in a database. Recent research has considered the use of relationships among object representations to improve duplicate detection. In the general case where relationships form a graph, research has mainly focused on duplicate detection quality/effectiveness. Scalability has been neglected so far, even though it is crucial for large real-world duplicate detection tasks. We scale-up duplicate detection in graph data (DDG) to large amounts of data and pairwise comparisons, using the support of a relational database management system. To this end, we first present a framework that generalizes the DDG process. We then present algorithms to scale DDG in space (amount of data processed with bounded main memory) and in time. Finally, we extend our framework to allow batched and parallel DDG, thus further improving efficiency. Experiments on data of up to two orders of magnitude larger than data considered so far in DDG show that our methods achieve the goal of scaling DDG to large volumes of data.},
  timestamp = {2016-10-24T08:25:33Z},
  groups = {Master\_Thesis}
}

@inproceedings{EPP.EA:Parallel:15,
  author = {Efthymiou, V. and Papadakis, G. and Papastefanatos, G. and Stefanidis, K. and Palpanas, T.},
  shorttitle = {Parallel Meta-Blocking},
  title = {Parallel Meta-Blocking: {{Realizing}} Scalable Entity Resolution over Large, Heterogeneous Data},
  year = {2015},
  month = oct,
  booktitle = {2015 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  pages = {411--420},
  doi = {10.1109/BigData.2015.7363782},
  file = {Efthymiou et al. - 2015 - Parallel meta-blocking Realizing scalable entity .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VW7WRG9C/Efthymiou et al. - 2015 - Parallel meta-blocking Realizing scalable entity .pdf:application/pdf;Efthymiou et al. - 2015 - Parallel meta-blocking Realizing scalable entity .html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ZMDGSZRA/Efthymiou et al. - 2015 - Parallel meta-blocking Realizing scalable entity .html:text/html},
  keywords = {Big data,blocking graph,Complexity theory,Context,data exchange,data handling,Erbium,load balancing algorithm,Load management,MapReduce,parallel meta-blocking,parallel processing,quadratic complexity,resource allocation,Scalability,scalable entity resolution,Servers},
  abstract = {Entity resolution constitutes a crucial task for many applications, but has an inherently quadratic complexity. Typically, it scales to large volumes of data through blocking: similar entities are clustered into blocks so that it suffices to perform comparisons only within each block. Meta-blocking further increases efficiency by cleaning the overlapping blocks from unnecessary comparisons. However, even Meta-blocking can be time-consuming: applying it to blocks with 7.4 million entities and 2.21011 comparisons takes almost 8 days on a modern high-end server. In this paper, we parallelize Meta-blocking based on MapReduce. We propose a simple strategy that explicitly creates the core concept of Meta-blocking, the blocking graph. We then describe an advanced strategy that creates the blocking graph implicitly, reducing the overhead of data exchange. We also introduce a load balancing algorithm that distributes the computationally intensive workload evenly among the available compute nodes. Our experimental analysis verifies the superiority of our advanced strategy and demonstrates an almost linear speedup for all meta-blocking techniques with respect to the number of available nodes.},
  timestamp = {2016-10-24T07:45:44Z},
  groups = {Master\_Thesis}
}

@article{PHN:Progressive:15,
  author = {Papenbrock, T. and Heise, A. and Naumann, F.},
  title = {Progressive {{Duplicate Detection}}},
  year = {2015},
  month = may,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {27},
  number = {5},
  pages = {1316--1329},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2014.2359666},
  file = {Papenbrock et al. - 2015 - Progressive Duplicate Detection.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3B2E9955/Papenbrock et al. - 2015 - Progressive Duplicate Detection.pdf:application/pdf;Papenbrock et al. - 2015 - Progressive Duplicate Detection.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5EPEKMXJ/Papenbrock et al. - 2015 - Progressive Duplicate Detection.html:text/html},
  keywords = {Algorithm design and analysis,Clustering algorithms,data cleaning,data handling,dataset quality maintenance,Detection algorithms,Duplicate detection,efficiency improvement,entity resolution,execution time,Heuristic algorithms,overall process gain maximization,Partitioning algorithms,pay-as-you-go,progressive duplicate detection methods,Progressiveness,real world entities,Runtime,Sorting},
  abstract = {Duplicate detection is the process of identifying multiple representations of same real world entities. Today, duplicate detection methods need to process ever larger datasets in ever shorter time: maintaining the quality of a dataset becomes increasingly difficult. We present two novel, progressive duplicate detection algorithms that significantly increase the efficiency of finding duplicates if the execution time is limited: They maximize the gain of the overall process within the time available by reporting most results much earlier than traditional approaches. Comprehensive experiments show that our progressive algorithms can double the efficiency over time of traditional duplicate detection and significantly improve upon related work.},
  timestamp = {2016-10-24T08:22:10Z},
  groups = {Master\_Thesis}
}

@inproceedings{SKLN:Combination:16,
  author = {Samiei, Ahmad and Koumarelas, Ioannis and Loster, Michael and Naumann, Felix},
  title = {Combination of {{Rule}}-Based and {{Textual Similarity Approaches}} to {{Match Financial Entities}}},
  year = {2016},
  booktitle = {Proceedings of the {{Second International Workshop}} on {{Data Science}} for {{Macro}}-{{Modeling}}},
  isbn = {978-1-4503-4407-4},
  pages = {4:1--4:2},
  url = {http://doi.acm.org/10.1145/2951894.2951905},
  doi = {10.1145/2951894.2951905},
  file = {Samiei et al. - 2016 - Combination of Rule-based and Textual Similarity A.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/NUVKZPAX/Samiei et al. - 2016 - Combination of Rule-based and Textual Similarity A.pdf:application/pdf;Samiei et al. - 2016 - Combination of Rule-based and Textual Similarity A.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/Q8H36H64/Samiei et al. - 2016 - Combination of Rule-based and Textual Similarity A.html:text/html},
  timestamp = {2016-10-24T08:20:13Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-10-24},
  series = {DSMM'16},
  address = {New York, NY, USA}
}

@article{Chr:Survey:12,
  author = {Christen, P.},
  title = {A {{Survey}} of {{Indexing Techniques}} for {{Scalable Record Linkage}} and {{Deduplication}}},
  year = {2012},
  month = sep,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {24},
  number = {9},
  pages = {1537--1555},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2011.127},
  file = {Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/GKUMF3ZW/Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.pdf:application/pdf;Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UZJE36QF/Christen - 2012 - A Survey of Indexing Techniques for Scalable Recor.html:text/html},
  keywords = {blocking,Complexity theory,Couplings,Data linkage,data matching,Encoding,entity resolution,experimental evaluation,Indexing,index techniques,Scalability},
  abstract = {Record linkage is the process of matching records from several databases that refer to the same entities. When applied on a single database, this process is known as deduplication. Increasingly, matched data are becoming important in many application areas, because they can contain information that is not available otherwise, or that is too costly to acquire. Removing duplicate records in a single database is a crucial step in the data cleaning process, because duplicates can severely influence the outcomes of any subsequent data processing or data mining. With the increasing size of today's databases, the complexity of the matching process becomes one of the major challenges for record linkage and deduplication. In recent years, various indexing techniques have been developed for record linkage and deduplication. They are aimed at reducing the number of record pairs to be compared in the matching process by removing obvious nonmatching pairs, while at the same time maintaining high matching quality. This paper presents a survey of 12 variations of 6 indexing techniques. Their complexity is analyzed, and their performance and scalability is evaluated within an experimental framework using both synthetic and real data sets. No such detailed survey has so far been published.},
  timestamp = {2016-10-24T07:44:23Z},
  groups = {Master\_Thesis}
}

@inproceedings{BCCo:comparison:03,
  author = {Baxter, Rohan and Christen, Peter and Churches, Tim and {others}},
  title = {A Comparison of Fast Blocking Methods for Record Linkage},
  year = {2003},
  booktitle = {{{ACM SIGKDD}}},
  volume = {3},
  pages = {25--27},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.7235\&rep=rep1\&type=pdf},
  file = {Baxter et al. - 2003 - A comparison of fast blocking methods for record l.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DS57B35S/Baxter et al. - 2003 - A comparison of fast blocking methods for record l.pdf:application/pdf},
  timestamp = {2016-10-25T13:12:14Z},
  groups = {Master\_Thesis},
  publisher = {{Citeseer}},
  urldate = {2016-10-25}
}

@article{FS:Theory:69,
  author = {Fellegi, Ivan P. and Sunter, Alan B.},
  title = {A {{Theory}} for {{Record Linkage}}},
  year = {1969},
  journal = {Journal of the American Statistical Association},
  volume = {64},
  number = {328},
  pages = {1183--1210},
  issn = {0162-1459},
  url = {http://www.jstor.org/stable/2286061},
  doi = {10.2307/2286061},
  file = {2286061.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/RE94UAAB/2286061.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/WQ2773FJ/01621459.1969.html:text/html},
  abstract = {A mathematical model is developed to provide a theoretical framework for a computer-oriented solution to the problem of recognizing those records in two files which represent identical persons, objects or events (said to be matched). A comparison is to be made between the recorded characteristics and values in two records (one from each file) and a decision made as to whether or not the members of the comparison-pair represent the same person or event, or whether there is insufficient evidence to justify either of these decisions at stipulated levels of error. These three decisions are referred to as link (A\textsubscript{1}), a non-link (A\textsubscript{3}), and a possible link (A\textsubscript{2}). The first two decisions are called positive dispositions. The two types of error are defined as the error of the decision A\textsubscript{1} when the members of the comparison pair are in fact unmatched, and the error of the decision A\textsubscript{3} when the members of the comparison pair are, in fact matched. The probabilities of these errors are defined as $\mu$ = $\sum$\textsubscript{$\gamma\epsilon\Gamma$} u($\gamma$)P(A\textsubscript{1}$\mid\gamma$) and $\lambda$ = $\sum$\textsubscript{$\gamma\epsilon\Gamma$} m($\gamma$)P(A\textsubscript{3}$\mid\gamma$) respectively where u($\gamma$), m($\gamma$) are the probabilities of realizing $\gamma$ (a comparison vector whose components are the coded agreements and disagreements on each characteristic) for unmatched and matched record pairs respectively. The summation is over the whole comparison space $\Gamma$ of possible realizations. A linkage rule assigns probabilities P(A\textsubscript{1}$\mid\gamma$), and P(A\textsubscript{2}$\mid\gamma$), and P(A\textsubscript{3}$\mid\gamma$) to each possible realization of $\gamma$ $\epsilon$ $\Gamma$. An optimal linkage rule L($\mu$, $\lambda$, $\Gamma$) is defined for each value of ($\mu$, $\lambda$) as the rule that minimizes P(A\textsubscript{2}) at those error levels. In other words, for fixed levels of error, the rule minimizes the probability of failing to make positive dispositions. A theorem describing the construction and properties of the optimal linkage rule and two corollaries to the theorem which make it a practical working tool are given.},
  timestamp = {2016-10-26T08:40:44Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@inproceedings{AO:Fast:05a,
  author = {Aizawa, A. and Oyama, K.},
  title = {A {{Fast Linkage Detection Scheme}} for {{Multi}}-{{Source Information Integration}}},
  year = {2005},
  month = apr,
  booktitle = {International {{Workshop}} on {{Challenges}} in {{Web Information Retrieval}} and {{Integration}}},
  pages = {30--39},
  doi = {10.1109/WIRI.2005.2},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/H83ZZRVW/Aizawa and Oyama - 2005 - A Fast Linkage Detection Scheme for Multi-Source I.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/X4TD5B3R/1552993.html:text/html},
  keywords = {Computer vision,Costs,Couplings,Databases,Explosives,Humans,Informatics,Information retrieval,Large-scale systems,Web sites},
  abstract = {Record linkage refers to techniques for identifying records associated with the same real-world entities. Record linkage is not only crucial in integrating multi-source databases that have been generated independently, but is also considered to be one of the key issues in integrating heterogeneous Web resources. However, when targeting large-scale data, the cost of enumerating all the possible linkages often becomes impracticably high. Based on this background, this paper proposes a fast and efficient method for linkage detection. The features of the proposed approach are: first, it exploits a suffix array structure that enables linkage detection using variable length n-grams. Second, it dynamically generates blocks of possibly associated records using `blocking keys' extracted from already known reliable linkages. The results from our preliminary experiments where the proposed method was applied to the integration of four bibliographic databases, which scale up to more than 10 million records, are also reported in the paper.},
  timestamp = {2016-10-26T08:42:44Z},
  groups = {Master\_Thesis}
}

@inproceedings{HS:Merge:95,
  author = {Hern{\'a}ndez, Mauricio A. and Stolfo, Salvatore J.},
  title = {The {{Merge}}/{{Purge Problem}} for {{Large Databases}}},
  year = {1995},
  booktitle = {Proceedings of the 1995 {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-0-89791-731-5},
  pages = {127--138},
  url = {http://doi.acm.org/10.1145/223784.223807},
  doi = {10.1145/223784.223807},
  file = {Hernández and Stolfo - 1995 - The MergePurge Problem for Large Databases.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VUPGWWPH/Hernández and Stolfo - 1995 - The MergePurge Problem for Large Databases.pdf:application/pdf},
  abstract = {Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Closure over the results of independent runs considering alternative primary key attributes in each pass.},
  timestamp = {2016-11-14T11:49:37Z},
  groups = {Master\_Thesis},
  annote = {Sorted Neighborhood Original},
  publisher = {{ACM}},
  urldate = {2016-10-26},
  series = {SIGMOD '95},
  address = {New York, NY, USA}
}

@inproceedings{DN:generalization:11,
  author = {Draisbach, Uwe and Naumann, Felix},
  title = {A Generalization of Blocking and Windowing Algorithms for Duplicate Detection},
  year = {2011},
  booktitle = {Data and {{Knowledge Engineering}} ({{ICDKE}}), 2011 {{International Conference}} On},
  pages = {18--24},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6053920},
  file = {Draisbach and Naumann - 2011 - A generalization of blocking and windowing algorit.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/NFJNNCJI/Draisbach and Naumann - 2011 - A generalization of blocking and windowing algorit.pdf:application/pdf},
  timestamp = {2016-10-26T08:49:01Z},
  groups = {Master\_Thesis},
  publisher = {{IEEE}},
  urldate = {2016-10-26}
}

@inproceedings{DN:comparison:09,
  author = {Draisbach, Uwe and Naumann, Felix},
  title = {A Comparison and Generalization of Blocking and Windowing Algorithms for Duplicate Detection},
  year = {2009},
  booktitle = {Proceedings of the {{International Workshop}} on {{Quality}} in {{Databases}} ({{QDB}})},
  pages = {51--56},
  url = {https://www.researchgate.net/profile/Felix_Naumann/publication/242075463_A_Comparison_and_Generalization_of_Blocking_and_Windowing_Algorithms_for_Duplicate_Detection/links/00b7d52b80d4816e7b000000.pdf},
  file = {Draisbach and Naumann - 2009 - A comparison and generalization of blocking and wi.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UNTRABPH/Draisbach and Naumann - 2009 - A comparison and generalization of blocking and wi.pdf:application/pdf},
  timestamp = {2016-10-26T08:51:05Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@inproceedings{DNSW:Adaptive:12,
  author = {Draisbach, Uwe and Naumann, Felix and Szott, Sascha and Wonneberg, Oliver},
  title = {Adaptive Windows for Duplicate Detection},
  year = {2012},
  booktitle = {2012 {{IEEE}} 28th {{International Conference}} on {{Data Engineering}}},
  pages = {1073--1083},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6228157},
  file = {Draisbach et al. - 2012 - Adaptive windows for duplicate detection.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6DAWG2Q4/Draisbach et al. - 2012 - Adaptive windows for duplicate detection.pdf:application/pdf},
  timestamp = {2016-10-26T08:52:34Z},
  groups = {Master\_Thesis},
  publisher = {{IEEE}},
  urldate = {2016-10-26}
}

@inproceedings{CRF:comparison:03,
  author = {Cohen, William and Ravikumar, Pradeep and Fienberg, Stephen},
  title = {A Comparison of String Metrics for Matching Names and Records},
  year = {2003},
  booktitle = {Kdd Workshop on Data Cleaning and Object Consolidation},
  volume = {3},
  pages = {73--78},
  url = {https://www.cs.cmu.edu/afs/cs/Web/People/wcohen/postscript/kdd-2003-match-ws.pdf},
  file = {Cohen et al. - 2003 - A comparison of string metrics for matching names .pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KJ8T5I3P/Cohen et al. - 2003 - A comparison of string metrics for matching names .pdf:application/pdf},
  timestamp = {2016-10-26T08:55:09Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@article{Nav:Guided:01,
  author = {Navarro, Gonzalo},
  title = {A {{Guided Tour}} to {{Approximate String Matching}}},
  year = {2001},
  month = mar,
  journal = {ACM Comput. Surv.},
  volume = {33},
  number = {1},
  pages = {31--88},
  issn = {0360-0300},
  url = {http://doi.acm.org/10.1145/375360.375365},
  doi = {10.1145/375360.375365},
  file = {Navarro - 2001 - A Guided Tour to Approximate String Matching.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6W5CXVTP/Navarro - 2001 - A Guided Tour to Approximate String Matching.pdf:application/pdf},
  keywords = {edit distance,Levenshtein distance,online string matching,text searching allowing errors},
  abstract = {We survey the current techniques to cope with the problem of string matching that allows errors. This is becoming a more and more relevant issue for many fast growing areas such as information retrieval and computational biology. We focus on online searching and mostly on edit distance, explaining the problem and its relevance, its statistical behavior, its history and current developments, and the central ideas of the algorithms and their complexities. We present a number of experiments to compare the performance of the different algorithms and show which are the best choices. We conclude with some directions for future work and open problems.},
  timestamp = {2016-10-26T09:00:29Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@inproceedings{BM:Adaptive:03,
  author = {Bilenko, Mikhail and Mooney, Raymond J.},
  title = {Adaptive {{Duplicate Detection Using Learnable String Similarity Measures}}},
  year = {2003},
  booktitle = {Proceedings of the {{Ninth ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-58113-737-8},
  pages = {39--48},
  url = {http://doi.acm.org/10.1145/956750.956759},
  doi = {10.1145/956750.956759},
  file = {Bilenko and Mooney - 2003 - Adaptive Duplicate Detection Using Learnable Strin.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/32P5MWRK/Bilenko and Mooney - 2003 - Adaptive Duplicate Detection Using Learnable Strin.pdf:application/pdf},
  keywords = {data cleaning,distance metric learning,record linkage,string edit distance,SVM applications,trained similarity measures},
  abstract = {The problem of identifying approximately duplicate records in databases is an essential step for data cleaning and data integration processes. Most existing approaches have relied on generic or manually tuned distance metrics for estimating the similarity of potential duplicates. In this paper, we present a framework for improving duplicate detection using trainable measures of textual similarity. We propose to employ learnable text distance functions for each database field, and show that such measures are capable of adapting to the specific notion of similarity that is appropriate for the field's domain. We present two learnable text similarity measures suitable for this task: an extended variant of learnable string edit distance, and a novel vector-space based measure that employs a Support Vector Machine (SVM) for training. Experimental results on a range of datasets show that our framework can improve duplicate detection accuracy over traditional techniques.},
  timestamp = {2016-10-26T09:04:11Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-10-26},
  series = {KDD '03},
  address = {New York, NY, USA}
}

@article{BMC.EA:Adaptive:03,
  author = {Bilenko, M. and Mooney, R. and Cohen, W. and Ravikumar, P. and Fienberg, S.},
  title = {Adaptive Name Matching in Information Integration},
  year = {2003},
  month = sep,
  journal = {IEEE Intelligent Systems},
  volume = {18},
  number = {5},
  pages = {16--23},
  issn = {1541-1672},
  doi = {10.1109/MIS.2003.1234765},
  file = {Bilenko et al. - 2003 - Adaptive name matching in information integration.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FCHMICM4/Bilenko et al. - 2003 - Adaptive name matching in information integration.pdf:application/pdf;Bilenko et al. - 2003 - Adaptive name matching in information integration.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/N8EXSCTW/Bilenko et al. - 2003 - Adaptive name matching in information integration.html:text/html},
  keywords = {adaptive name matching,Character recognition,Costs,Couplings,Databases,data mining,distributed databases,duplicate database records,heterogeneous information sources,information integration,Internet,learning (artificial intelligence),machine learning,Object detection,Optical character recognition software,Optical recording,string matching,string similarity measures,text analysis,textual similarity measures,Uncertainty,Web pages},
  abstract = {Identifying approximately duplicate database records that refer to the same entity is essential for information integration. The authors compare and describe methods for combining and learning textual similarity measures for name matching.},
  timestamp = {2016-10-26T09:10:09Z},
  groups = {Master\_Thesis}
}

@inproceedings{MNK.EA:heterogeneous:05,
  author = {Minton, S. N. and Nanjo, C. and Knoblock, C. A. and Michalowski, M. and Michelson, M.},
  title = {A Heterogeneous Field Matching Method for Record Linkage},
  year = {2005},
  month = nov,
  booktitle = {Fifth {{IEEE International Conference}} on {{Data Mining}} ({{ICDM}}'05)},
  pages = {8 pp.--},
  doi = {10.1109/ICDM.2005.7},
  file = {Minton et al. - 2005 - A heterogeneous field matching method for record l.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/87KT8WJX/Minton et al. - 2005 - A heterogeneous field matching method for record l.pdf:application/pdf;Minton et al. - 2005 - A heterogeneous field matching method for record l.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7ZREI847/Minton et al. - 2005 - A heterogeneous field matching method for record l.html:text/html},
  keywords = {Animals,Business,Couplings,database management systems,Databases,expert-like rules,heterogeneous field matching,heterogeneous transformations,Humans,learning (artificial intelligence),Learning systems,machine learning,Manufacturing,Marine technology,pattern matching,record linkage,Switches},
  abstract = {Record linkage is the process of determining that two records refer to the same entity. A key subprocess is evaluating how well the individual fields, or attributes, of the records match each other. One approach to matching fields is to use hand-written domain-specific rules. This "expert systems" approach may result in good performance for specific applications, but it is not scalable. This paper describes a new machine learning approach that creates expert-like rules for field matching. In our approach, the relationship between two field values is described by a set of heterogeneous transformations. Previous machine learning methods used simple models to evaluate the distance between two fields. However, our approach enables more sophisticated relationships to be modeled, which better capture the complex domain specific, common-sense phenomena that humans use to judge similarity. We compare our approach to methods that rely on simpler homogeneous models in several domains. By modeling more complex relationships we produce more accurate results.},
  timestamp = {2016-10-26T09:11:25Z},
  groups = {Master\_Thesis}
}

@article{CKLS:Efficient:01,
  author = {Cochinwala, Munir and Kurien, Verghese and Lalk, Gail and Shasha, Dennis},
  title = {Efficient Data Reconciliation},
  year = {2001},
  month = sep,
  journal = {Information Sciences},
  volume = {137},
  number = {1\textendash{}4},
  pages = {1--15},
  issn = {0020-0255},
  url = {http://www.sciencedirect.com/science/article/pii/S0020025500000700},
  doi = {10.1016/S0020-0255(00)00070-0},
  file = {Cochinwala et al. - 2001 - Efficient data reconciliation.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/P9T286KX/Cochinwala et al. - 2001 - Efficient data reconciliation.pdf:application/pdf;Cochinwala et al. - 2001 - Efficient data reconciliation.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VQUXSFDR/Cochinwala et al. - 2001 - Efficient data reconciliation.html:text/html},
  abstract = {Data reconciliation is the process of matching records across different databases. Data reconciliation requires ``joining'' on fields that have traditionally been non-key fields. Generally, the operational databases are of sufficient quality for the purposes for which they were initially designed but since the data in the different databases do not have a canonical structure and may have errors, approximate matching algorithms are required.

Approximate matching algorithms can have many different parameter settings. The number of parameters will affect the complexity of the algorithm due to the number of comparisons needed to identify matching records across different datasets. For large datasets that are prevalent in data warehouses, the increased complexity may result in impractical solutions.

In this paper, we describe an efficient method for data reconciliation. Our main contribution is the incorporation of machine learning and statistical techniques to reduce the complexity of the matching algorithms via identification and elimination of redundant or useless parameters. We have conducted experiments on actual data that demonstrate the validity of our techniques. In our experiments, the techniques reduced complexity by 50\% while significantly increasing matching accuracy.},
  timestamp = {2016-10-26T09:15:13Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@inproceedings{EVE:TAILOR:02,
  author = {Elfeky, Mohamed G. and Verykios, Vassilios S. and Elmagarmid, Ahmed K.},
  shorttitle = {{{TAILOR}}},
  title = {{{TAILOR}}: {{A}} Record Linkage Toolbox},
  year = {2002},
  booktitle = {Data {{Engineering}}, 2002. {{Proceedings}}. 18th {{International Conference}} On},
  pages = {17--28},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=994694},
  file = {Elfeky et al. - 2002 - TAILOR A record linkage toolbox.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/NV5W72BI/Elfeky et al. - 2002 - TAILOR A record linkage toolbox.pdf:application/pdf},
  timestamp = {2016-10-26T09:17:10Z},
  groups = {Master\_Thesis},
  publisher = {{IEEE}},
  urldate = {2016-10-26}
}

@inproceedings{KR:Training:08,
  author = {K{\"o}pcke, Hanna and Rahm, Erhard},
  title = {Training Selection for Tuning Entity Matching.},
  year = {2008},
  booktitle = {{{QDB}}/{{MUD}}},
  pages = {3--12},
  url = {https://www.researchgate.net/profile/Hanna_Koepcke/publication/221276212_Training_selection_for_tuning_entity_matching/links/54036c3f0cf23d9765a5cdbc.pdf},
  file = {Köpcke and Rahm - 2008 - Training selection for tuning entity matching..pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/W45ZP4T9/Köpcke and Rahm - 2008 - Training selection for tuning entity matching..pdf:application/pdf},
  timestamp = {2016-10-26T09:18:58Z},
  groups = {Master\_Thesis},
  urldate = {2016-10-26}
}

@book{MRS:Introduction:08,
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  title = {Introduction to {{Information Retrieval}}},
  year = {2008},
  isbn = {978-0-521-86571-5},
  file = {Manning et al. - 2008 - Introduction to Information Retrieval.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DTP2DTSA/Manning et al. - 2008 - Introduction to Information Retrieval.pdf:application/pdf},
  abstract = {Class-tested and coherent, this groundbreaking new textbook teaches web-era information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. Written from a computer science perspective by three leading experts in the field, it gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Although originally designed as the primary text for a graduate or advanced undergraduate course in information retrieval, the book will also create a buzz for researchers and professionals alike.},
  timestamp = {2016-11-02T10:06:13Z},
  groups = {Master\_Thesis},
  publisher = {{Cambridge University Press}},
  address = {New York, NY, USA}
}

@article{Ars:Fast:15,
  author = {Arslan, Abdullah N.},
  title = {Fast {{Algorithms}} for {{Local Similarity Queries}} in {{Two Sequences}}},
  year = {2015},
  month = aug,
  journal = {International Journal of Foundations of Computer Science},
  volume = {26},
  number = {05},
  pages = {625--642},
  issn = {0129-0541},
  url = {http://www.worldscientific.com/doi/abs/10.1142/S0129054115500355},
  doi = {10.1142/S0129054115500355},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5A7T3JBQ/S0129054115500355.html:text/html},
  abstract = {In sequence comparison, finding local similarities in given strings is a very important well-known problem. In this work we introduce two local sequence similarity query problems, and present algorithms for them. Our algorithms use a data structure that supports constant time longest common extension queries. This data structure is created only once, and in time linear in the size of the input strings. After this step all subsequent local similarity queries can be answered very fast. Existing algorithms take significantly more time in answering these queries.},
  timestamp = {2016-11-03T11:06:03Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-03}
}

@article{LLW.EA:Boosting:15,
  author = {Lu, Jiaheng and Lin, Chunbin and Wang, Wei and Li, Chen and Xiao, Xiaokui},
  title = {Boosting the {{Quality}} of {{Approximate String Matching}} by {{Synonyms}}},
  year = {2015},
  month = oct,
  journal = {ACM Trans. Database Syst.},
  volume = {40},
  number = {3},
  pages = {15:1--15:42},
  issn = {0362-5915},
  url = {http://doi.acm.org/10.1145/2818177},
  doi = {10.1145/2818177},
  file = {Lu et al. - 2015 - Boosting the Quality of Approximate String Matchin.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/A4UAB3PN/Lu et al. - 2015 - Boosting the Quality of Approximate String Matchin.pdf:application/pdf},
  keywords = {semantic search,similarity join,String similarity search},
  abstract = {A string-similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings ``Sam'' and ``Samuel'' can be considered to be similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, for example, number of common words or q-grams. While this is indeed an indicator of similarity, there are many important cases where syntactically-different strings can represent the same real-world object. For example, ``Bill'' is a short form of ``William,'' and ``Database Management Systems'' can be abbreviated as ``DBMS.'' Given a collection of predefined synonyms, the purpose of this article is to explore such existing knowledge to effectively evaluate the similarity between two strings and efficiently perform similarity searches and joins, thereby boosting the quality of approximate string matching. In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. We then study efficient algorithms for similarity searches and joins by proposing two novel indexes, called SI-trees and QP-trees, which combine signature-filtering and length-filtering strategies. In order to improve the efficiency of our algorithms, we develop an estimator to estimate the size of candidates to enable an online selection of signature filters. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the experimental results from a comprehensive study of the algorithms with three real datasets verify the effectiveness and efficiency of our approaches.},
  timestamp = {2016-11-03T11:11:56Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-03}
}

@inproceedings{MGAB:Efficient:13,
  author = {Mishra, Shashwat and Gandhi, Tejas and Arora, Akhil and Bhattacharya, Arnab},
  title = {Efficient {{Edit Distance Based String Similarity Search Using Deletion Neighborhoods}}},
  year = {2013},
  booktitle = {Proceedings of the {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}}},
  isbn = {978-1-4503-1599-9},
  pages = {375--383},
  url = {http://doi.acm.org/10.1145/2457317.2457387},
  doi = {10.1145/2457317.2457387},
  file = {Mishra et al. - 2013 - Efficient Edit Distance Based String Similarity Se.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7JNPKET2/Mishra et al. - 2013 - Efficient Edit Distance Based String Similarity Se.pdf:application/pdf},
  keywords = {deletion neighborhood,edit distance,string similarity},
  abstract = {This paper serves as a report for the participation of Special Interest Group In Data (SIGDATA), Indian Institute of Technology, Kanpur in the String Similarity Workshop, EDBT, 2013. We present a novel technique to efficiently process edit distance based string similarity queries. Our technique draws upon some previously conducted works in the field and introduces new methods to tackle the issues therein. We focus on achieving minimum possible execution time while being rather liberal with memory consumption. We propose and support the use of deletion neighborhoods for fast edit distance lookups in dictionaries. Our work emphasizes the power of deletion neighborhoods over other popular finger print based schemes for similarity search queries. Furthermore, we establish that it is possible to reduce the large space requirement of a deletion neighborhood based finger print scheme using simple hashing techniques, thereby making the scheme suitable for practical application. We compare our implementation with the state of the art libraries (Flamingo) and report speed ups of up to an order of magnitude.},
  timestamp = {2016-11-03T11:12:03Z},
  groups = {Matcher,Matcher},
  publisher = {{ACM}},
  urldate = {2016-11-03},
  series = {EDBT '13},
  address = {New York, NY, USA}
}

@article{LDF:Partitionbased:13,
  author = {Li, Guoliang and Deng, Dong and Feng, Jianhua},
  title = {A {{Partition}}-Based {{Method}} for {{String Similarity Joins}} with {{Edit}}-Distance {{Constraints}}},
  year = {2013},
  month = jul,
  journal = {ACM Trans. Database Syst.},
  volume = {38},
  number = {2},
  pages = {9:1--9:33},
  issn = {0362-5915},
  url = {http://doi.acm.org/10.1145/2487259.2487261},
  doi = {10.1145/2487259.2487261},
  file = {Li et al. - 2013 - A Partition-based Method for String Similarity Joi.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TX2TCXVC/Li et al. - 2013 - A Partition-based Method for String Similarity Joi.pdf:application/pdf},
  keywords = {edit distance,segment filter,String similarity join},
  abstract = {As an essential operation in data cleaning, the similarity join has attracted considerable attention from the database community. In this article, we study string similarity joins with edit-distance constraints, which find similar string pairs from two large sets of strings whose edit distance is within a given threshold. Existing algorithms are efficient either for short strings or for long strings, and there is no algorithm that can efficiently and adaptively support both short strings and long strings. To address this problem, we propose a new filter, called the segment filter. We partition a string into a set of segments and use the segments as a filter to find similar string pairs. We first create inverted indices for the segments. Then for each string, we select some of its substrings, identify the selected substrings from the inverted indices, and take strings on the inverted lists of the found substrings as candidates of this string. Finally, we verify the candidates to generate the final answer. We devise efficient techniques to select substrings and prove that our method can minimize the number of selected substrings. We develop novel pruning techniques to efficiently verify the candidates. We also extend our techniques to support normalized edit distance. Experimental results show that our algorithms are efficient for both short strings and long strings, and outperform state-of-the-art methods on real-world datasets.},
  timestamp = {2016-11-03T11:12:08Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-03}
}

@article{RW:Harry:16,
  author = {Rieck, Konrad and Wressnegger, Christian},
  shorttitle = {Harry},
  title = {Harry: {{A Tool}} for {{Measuring String Similarity}}},
  year = {2016},
  month = jan,
  journal = {J. Mach. Learn. Res.},
  volume = {17},
  number = {1},
  pages = {258--262},
  issn = {1532-4435},
  url = {http://dl.acm.org/citation.cfm?id=2946645.2946654},
  file = {Rieck and Wressnegger - 2016 - Harry A Tool for Measuring String Similarity.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/G32UFGIN/Rieck and Wressnegger - 2016 - Harry A Tool for Measuring String Similarity.pdf:application/pdf},
  keywords = {similarity measures for strings,string distances,string kernels},
  abstract = {Comparing strings and assessing their similarity is a basic operation in many application domains of machine learning, such as in information retrieval, natural language processing and bioinformatics. The practitioner can choose from a large variety of available similarity measures for this task, each emphasizing different aspects of the string data. In this article, we present Harry, a small tool specifically designed for measuring the similarity of strings. Harry implements over 20 similarity measures, including common string distances and string kernels, such as the Levenshtein distance and the Subsequence kernel. The tool has been designed with efficiency in mind and allows for multi-threaded as well as distributed computing, enabling the analysis of large data sets of strings. Harry supports common data formats and thus can interface with analysis environments, such as Matlab, Pylab and Weka.},
  timestamp = {2016-11-03T11:12:12Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-03}
}

@inproceedings{SWR:Scalable:13,
  author = {Siragusa, Enrico and Weese, David and Reinert, Knut},
  title = {Scalable {{String Similarity Search}}/{{Join}} with {{Approximate Seeds}} and {{Multiple Backtracking}}},
  year = {2013},
  booktitle = {Proceedings of the {{Joint EDBT}}/{{ICDT}} 2013 {{Workshops}}},
  isbn = {978-1-4503-1599-9},
  pages = {370--374},
  url = {http://doi.acm.org/10.1145/2457317.2457386},
  doi = {10.1145/2457317.2457386},
  file = {Siragusa et al. - 2013 - Scalable String Similarity SearchJoin with Approx.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TDVJ2338/Siragusa et al. - 2013 - Scalable String Similarity SearchJoin with Approx.pdf:application/pdf},
  keywords = {approximate seeds,backtracking,banded dynamic programming,banded Myers bit-vector,filtration,radix tree,suffix tree},
  abstract = {We present in this paper scalable algorithms for optimal string similarity search and join. Our methods are variations of those applied in Masai [15], our recently published tool for mapping high-throughput DNA sequencing data with unpreceded speed and accuracy. The key features of our approach are filtration with approximate seeds and methods for multiple backtracking. Approximate seeds, compared to exact seeds, increase filtration specificity while preserving sensitivity. Multiple backtracking amortizes the cost of searching a large set of seeds. Combined together, these two methods significantly speed up string similarity search and join operations. Our tool is implemented in C++ and OpenMP using the SeqAn library. The source code is distributed under the BSD license and can be freely downloaded from http://www.seqan.de/projects/edbt2013.},
  timestamp = {2016-11-03T11:12:20Z},
  groups = {Matcher,Matcher},
  publisher = {{ACM}},
  urldate = {2016-11-03},
  series = {EDBT '13},
  address = {New York, NY, USA}
}

@article{Kes:How:07,
  author = {Keshav, S.},
  title = {How to Read a Paper},
  year = {2007},
  journal = {ACM SIGCOMM Computer Communication Review},
  volume = {37},
  number = {3},
  pages = {83--84},
  url = {http://dl.acm.org/citation.cfm?id=1273458},
  file = {Keshav - 2007 - How to read a paper.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DE65XVHS/Keshav - 2007 - How to read a paper.pdf:application/pdf},
  timestamp = {2016-11-04T09:45:26Z},
  groups = {Basics},
  urldate = {2016-11-04}
}

@inproceedings{Abb:Information:05,
  author = {Abbasi, R. A.},
  title = {Information {{Extraction Techniques}} for {{Postal Address Standardization}}},
  year = {2005},
  month = dec,
  booktitle = {2005 {{Pakistan Section Multitopic Conference}}},
  pages = {1--6},
  doi = {10.1109/INMIC.2005.334455},
  file = {Abbasi - 2005 - Information Extraction Techniques for Postal Addre.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8DFENG3I/Abbasi - 2005 - Information Extraction Techniques for Postal Addre.pdf:application/pdf;Abbasi - 2005 - Information Extraction Techniques for Postal Addre.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FMRZS579/Abbasi - 2005 - Information Extraction Techniques for Postal Addre.html:text/html},
  keywords = {Business,Cities and towns,Communication industry,Data cleansing,data mining,data warehouse,data warehouses,hidden Markov model,hidden Markov models,HMM,Humans,information extraction technique,Information retrieval,postal address standardization,rule-based method,Standardization,statistical model,Tagging,Telecommunications},
  abstract = {The unique frames of reference of humans result in various definitions of the same details. They develop addresses of same places in different ways, which might result in inconsistent format of addresses ultimately leading to misapprehensions. A major motivation for standardization of the addresses is cleansing of addresses in data warehouses. Since almost every organization deals with a variety of addresses of its customers and employees therefore, a consistent format of addresses can ensure better knowledge of the organization about its customers. This paper presents various information extraction techniques which can also be used in address standardization. It focuses on a statistical model, hidden Markov model (HMM), and two rule-based methods, RAPIER and GRID that extract information from free text. The paper also discusses some personal experience for address standardization},
  timestamp = {2016-11-06T19:37:46Z},
  groups = {Address,Address}
}

@inproceedings{KKG.EA:Address:11,
  author = {Kaleem, Abdul and Khawaja, Moyeezullah and Ghori and Khanzada, Zahra and Malik, M. Noman},
  title = {Address {{Standardization}} Using {{Supervised Machine Learning}}},
  year = {2011},
  month = jan,
  booktitle = {{{ResearchGate}}},
  url = {https://www.researchgate.net/publication/283706723_Address_Standardization_using_Supervised_Machine_Learning},
  file = {Kaleem et al. - 2011 - Address Standardization using Supervised Machine L.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ZFX76QX4/Kaleem et al. - 2011 - Address Standardization using Supervised Machine L.pdf:application/pdf;Kaleem et al. - 2011 - Address Standardization using Supervised Machine L.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2TGFSD6I/Kaleem et al. - 2011 - Address Standardization using Supervised Machine L.html:text/html},
  abstract = {Data mining has become an important task of today's rich information environments. Strong results are obtained through accurate historical reporting. Inaccurate and dirty records yield weak and...},
  timestamp = {2016-11-06T19:38:39Z},
  groups = {Address,Address},
  urldate = {2016-11-06}
}

@misc{Gol:Geocoding:08,
  author = {Goldberg, Daniel},
  title = {A {{Geocoding Best Practice Guide}}},
  year = {2008},
  month = nov,
  url = {http://www.naaccr.org/LinkClick.aspx?fileticket=ZKekM8k_IQ0\%3d\&tabid=239\&mid=699},
  file = {Goldberg - 2008 - A Geocoding Best Practice Guide.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7KN5R4EH/Goldberg - 2008 - A Geocoding Best Practice Guide.pdf:application/pdf},
  timestamp = {2016-11-06T19:43:25Z},
  groups = {Address,Address},
  language = {en}
}

@misc{.EA:International:,
  title = {International {{Address Formats}} ({{Postal}}/{{Mailing Addresses}}) and {{Other International Mailing Information}}},
  url = {http://www.bitboost.com/ref/international-address-formats.html},
  file = {International Address Formats (Postal/Mailing Addresses) and Other International Mailing Information:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/696UAWJ4/international-address-formats.html:text/html},
  timestamp = {2016-11-06T19:44:18Z},
  groups = {Address,Address},
  urldate = {2016-11-06}
}

@misc{.EA:OASIS:,
  title = {{{OASIS}} - {{OASIS Customer Information Quality TC}}},
  url = {https://www.oasis-open.org/committees/ciq/download.shtml},
  file = {OASIS - OASIS Customer Information Quality TC:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/WJ4S5F3Q/download.html:text/html},
  timestamp = {2016-11-06T19:45:17Z},
  groups = {Address,Address},
  urldate = {2016-11-06}
}

@misc{Uni:Birkbeck:,
  author = {{University of Oxford Text Archive}, Oxford University Computing Services},
  title = {Birkbeck Spelling Error Corpus [{{Electronic}} Resource] / {{Roger Mitton}}},
  url = {http://ota.ox.ac.uk/headers/0643.xml},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/58GZKFFJ/0643.html:text/html},
  timestamp = {2016-11-06T19:45:42Z},
  groups = {Address,Address},
  urldate = {2016-11-06},
  type = {Text}
}

@inproceedings{CCWo:probabilistic:04,
  author = {Christen, Peter and Churches, Tim and Willmore, Alan and {others}},
  title = {A Probabilistic Geocoding System Based on a National Address File},
  year = {2004},
  booktitle = {Proceedings of the 3rd {{Australasian Data Mining Conference}}, {{Cairns}}},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.543\&rep=rep1\&type=pdf},
  file = {Christen et al. - 2004 - A probabilistic geocoding system based on a nation.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MDSRUFXS/Christen et al. - 2004 - A probabilistic geocoding system based on a nation.pdf:application/pdf},
  timestamp = {2016-11-06T19:47:08Z},
  groups = {Address,Address},
  publisher = {{Citeseer}},
  urldate = {2016-11-06}
}

@inproceedings{Chr:Comparison:06,
  author = {Christen, P.},
  shorttitle = {A {{Comparison}} of {{Personal Name Matching}}},
  title = {A {{Comparison}} of {{Personal Name Matching}}: {{Techniques}} and {{Practical Issues}}},
  year = {2006},
  month = dec,
  booktitle = {Sixth {{IEEE International Conference}} on {{Data Mining}} - {{Workshops}} ({{ICDMW}}'06)},
  pages = {290--294},
  doi = {10.1109/ICDMW.2006.2},
  file = {Christen - 2006 - A Comparison of Personal Name Matching Techniques.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QUBWVI7T/Christen - 2006 - A Comparison of Personal Name Matching Techniques.pdf:application/pdf;Christen - 2006 - A Comparison of Personal Name Matching Techniques.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/R88PAK9G/Christen - 2006 - A Comparison of Personal Name Matching Techniques.html:text/html},
  keywords = {Computer errors,Computer science,Conferences,data linkage system,data mining,Deduplication,information extraction,Information retrieval,Model driven engineering,personal name matching,Radio frequency,search engine,string matching,Terrorism,text analysis,Text mining,Tin,Web mining},
  abstract = {Finding and matching personal names is at the core of an increasing number of applications: from text and Web mining, search engines, to information extraction, deduplication and data linkage systems. Variations and errors in names make exact string matching problematic, and approximate matching techniques have to be applied. When compared to general text, however, personal names have different characteristics that need to be considered. In this paper, we discuss the characteristics of personal names and present potential sources of variations and errors. We then overview a comprehensive number of commonly used, as well as some recently developed name matching techniques. Experimental comparisons using four large name data sets indicate that there is no clear best matching technique},
  timestamp = {2016-11-06T19:48:47Z},
  groups = {Master\_Thesis}
}

@article{PWS:knowledgepoor:09,
  author = {Piskorski, Jakub and Wieloch, Karol and Sydow, Marcin},
  title = {On Knowledge-Poor Methods for Person Name Matching and Lemmatization for Highly Inflectional Languages},
  year = {2009},
  month = jan,
  journal = {Information Retrieval},
  volume = {12},
  number = {3},
  pages = {275--299},
  issn = {1386-4564, 1573-7659},
  url = {http://link.springer.com/article/10.1007/s10791-008-9085-5},
  doi = {10.1007/s10791-008-9085-5},
  file = {Piskorski et al. - 2009 - On knowledge-poor methods for person name matching.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FSK468V4/Piskorski et al. - 2009 - On knowledge-poor methods for person name matching.pdf:application/pdf;Piskorski et al. - 2009 - On knowledge-poor methods for person name matching.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/Z3G3F4NB/Piskorski et al. - 2009 - On knowledge-poor methods for person name matching.html:text/html},
  abstract = {Web person search is one of the most common activities of Internet users. Recently, a vast amount of work on applying various NLP techniques for person name disambiguation in large web document collections has been reported, where the main focus was on English and few other major languages. This article reports on knowledge-poor methods for tackling person name matching and lemmatization in Polish, a highly inflectional language with complex person name declension paradigm. These methods apply mainly well-established string distance metrics, some new variants thereof, automatically acquired simple suffix-based lemmatization patterns and some combinations of the aforementioned techniques. Furthermore, we also carried out some initial experiments on deploying techniques that utilize the context, in which person names appear. Results of numerous experiments are presented. The evaluation carried out on a data set extracted from a corpus of on-line news articles revealed that achieving lemmatization accuracy figures greater than 90\% seems to be difficult, whereas combining string distance metrics with suffix-based patterns results in 97.6\textendash{}99\% accuracy for the name matching task. Interestingly, no significant additional gain could be achieved through integrating some basic techniques, which try to exploit the local context the names appear in. Although our explorations were focused on Polish, we believe that the work presented in this article constitutes practical guidelines for tackling the same problem for other highly inflectional languages with similar phenomena.},
  timestamp = {2016-11-06T19:50:31Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-06},
  language = {en}
}

@inproceedings{GZG.EA:Address:09,
  author = {Guo, Honglei and Zhu, Huijia and Guo, Zhili and Zhang, XiaoXun and Su, Zhong},
  title = {Address {{Standardization}} with {{Latent Semantic Association}}},
  year = {2009},
  booktitle = {Proceedings of the 15th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-60558-495-9},
  pages = {1155--1164},
  url = {http://doi.acm.org/10.1145/1557019.1557144},
  doi = {10.1145/1557019.1557144},
  file = {Guo et al. - 2009 - Address Standardization with Latent Semantic Assoc.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/BK6NU2S9/Guo et al. - 2009 - Address Standardization with Latent Semantic Assoc.pdf:application/pdf},
  keywords = {address,data standardization},
  abstract = {Address standardization is a very challenging task in data cleansing. To provide better customer relationship management and business intelligence for customer-oriented cooperates, millions of free-text addresses need to be converted to a standard format for data integration, de-duplication and householding. Existing commercial tools usually employ lots of hand-craft, domain-specific rules and reference data dictionary of cities, states etc. These rules work better for the region they are designed. However, rule-based methods usually require more human efforts to rewrite these rules for each new domain since address data are very irregular and varied with countries and regions. Supervised learning methods usually are more adaptable than rule-based approaches. However, supervised methods need large-scale labeled training data. It is a labor-intensive and time-consuming task to build a large-scale annotated corpus for each target domain. For minimizing human efforts and the size of labeled training data set, we present a free-text address standardization method with latent semantic association (LaSA). LaSA model is constructed to capture latent semantic association among words from the unlabeled corpus. The original term space of the target domain is projected to a concept space using LaSA model at first, then the address standardization model is active learned from LaSA features and informative samples. The proposed method effectively captures the data distribution of the domain. Experimental results on large-scale English and Chinese corpus show that the proposed method significantly enhances the performance of standardization with less efforts and training data.},
  timestamp = {2016-11-07T20:03:38Z},
  groups = {Address,Address},
  publisher = {{ACM}},
  urldate = {2016-11-07},
  series = {KDD '09},
  address = {New York, NY, USA}
}

@article{AE:Optimal:,
  author = {Altschul, Stephen F. and Erickson, Bruce W.},
  title = {Optimal Sequence Alignment Using Affine Gap Costs},
  journal = {Bulletin of Mathematical Biology},
  volume = {48},
  number = {5-6},
  pages = {603--616},
  issn = {0092-8240, 1522-9602},
  url = {http://link.springer.com/article/10.1007/BF02462326},
  doi = {10.1007/BF02462326},
  file = {Altschul and Erickson - Optimal sequence alignment using affine gap costs.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4N6PV2E8/Altschul and Erickson - Optimal sequence alignment using affine gap costs.pdf:application/pdf;Altschul and Erickson - Optimal sequence alignment using affine gap costs.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/RGSVE5WR/Altschul and Erickson - Optimal sequence alignment using affine gap costs.html:text/html},
  abstract = {When comparing two biological sequences, it is often desirable for a gap to be assigned a cost not directly proportional to its length. If affine gap costs are employed, in other words if opening a gap costsv and each null in the gap costsu, the algorithm of Gotoh (1982,J. molec. Biol.162, 705) finds the minimum cost of aligning two sequences in orderMN steps. Gotoh's algorithm attempts to find only one from among possibly many optimal (minimum-cost) alignments, but does not always succeed. This paper provides an example for which this part of Gotoh's algorithm fails and describes an algorithm that finds all and only the optimal alignments. This modification of Gotoh's algorithm still requires orderMN steps. A more precise form of path graph than previously used is needed to represent accurately all optimal alignments for affine gap costs.},
  timestamp = {2016-11-07T20:04:05Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-07},
  language = {en}
}

@inproceedings{MEo:Field:96,
  author = {Monge, Alvaro E. and Elkan, Charles and {others}},
  shorttitle = {The {{Field Matching Problem}}},
  title = {The {{Field Matching Problem}}: {{Algorithms}} and {{Applications}}.},
  year = {1996},
  booktitle = {{{KDD}}},
  pages = {267--270},
  url = {http://www.aaai.org/Papers/KDD/1996/KDD96-044.pdf},
  file = {Monge et al. - 1996 - The Field Matching Problem Algorithms and Applica.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/EFX2AJ7E/Monge et al. - 1996 - The Field Matching Problem Algorithms and Applica.pdf:application/pdf},
  timestamp = {2016-11-08T13:47:46Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-08}
}

@inproceedings{RLM:Efficient:06,
  author = {Rieck, Konrad and Laskov, Pavel and M{\"u}ller, Klaus-Robert},
  shorttitle = {Efficient Algorithms for Similarity Measures over Sequential Data},
  title = {Efficient Algorithms for Similarity Measures over Sequential Data: {{A}} Look beyond Kernels},
  year = {2006},
  booktitle = {Joint {{Pattern Recognition Symposium}}},
  pages = {374--383},
  url = {http://link.springer.com/chapter/10.1007/11861898_38},
  file = {2006-dagm.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8TFIS2T9/2006-dagm.pdf:application/pdf},
  timestamp = {2016-11-11T15:18:50Z},
  groups = {Matcher,Matcher},
  publisher = {{Springer}},
  urldate = {2016-11-11}
}

@article{SRR:Large:07,
  author = {Sonnenburg, S{\"o}ren and R{\"a}tsch, Gunnar and Rieck, Konrad},
  title = {Large Scale Learning with String Kernels},
  year = {2007},
  journal = {Large Scale Kernel Machines},
  pages = {73--103},
  url = {http://core.ac.uk/download/pdf/21002.pdf},
  file = {lskm_book.dvi - 2007-lskm.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/W4GP6Q53/2007-lskm.pdf:application/pdf},
  timestamp = {2017-02-14T14:53:28Z},
  groups = {Master\_Thesis,Matcher,Matcher},
  urldate = {2016-11-11}
}

@inproceedings{MSC.EA:Distributed:13,
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S. and Dean, Jeff},
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  year = {2013},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {3111--3119},
  url = {http://papers.nips.cc/paper/5021-distributed-representations},
  file = {Distributed Representations of Words and Phrases and their Compositionality - 5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/H9663B4R/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf:application/pdf},
  timestamp = {2016-11-11T15:11:37Z},
  groups = {Matcher,Matcher},
  urldate = {2016-11-11}
}

@article{RBK:Approximate:08,
  author = {Rieck, Konrad and Brefeld, Ulf and Krueger, Tammo},
  title = {Approximate Kernels for Trees},
  year = {2008},
  journal = {Fraunhofer Publica},
  file = {atk_tr.dvi - 2008-first.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/XVT57FW2/2008-first.pdf:application/pdf},
  timestamp = {2016-11-11T15:18:03Z},
  groups = {Matcher,Matcher}
}

@inproceedings{KKTR:Learningbased:11,
  author = {Kolb, Lars and K{\"o}pcke, Hanna and Thor, Andreas and Rahm, Erhard},
  title = {Learning-Based Entity Resolution with {{MapReduce}}},
  year = {2011},
  booktitle = {Proceedings of the Third International Workshop on {{Cloud}} Data Management},
  pages = {1--6},
  url = {http://dl.acm.org/citation.cfm?id=2064087},
  file = {Learning-based Entity Resolution with MapReduce - learning_based_er_with_mr.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2FMKII7V/learning_based_er_with_mr.pdf:application/pdf},
  timestamp = {2016-11-18T09:05:41Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-11-18}
}

@inproceedings{KTR:Load:12,
  author = {Kolb, L. and Thor, A. and Rahm, E.},
  title = {Load {{Balancing}} for {{MapReduce}}-Based {{Entity Resolution}}},
  year = {2012},
  month = apr,
  booktitle = {2012 {{IEEE}} 28th {{International Conference}} on {{Data Engineering}}},
  pages = {618--629},
  doi = {10.1109/ICDE.2012.22},
  file = {Kolb et al. - 2012 - Load Balancing for MapReduce-based Entity Resoluti.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KXAUQ7HV/Kolb et al. - 2012 - Load Balancing for MapReduce-based Entity Resoluti.pdf:application/pdf;Kolb et al. - 2012 - Load Balancing for MapReduce-based Entity Resoluti.html:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/6AZ5KK8J/Kolb et al. - 2012 - Load Balancing for MapReduce-based Entity Resoluti.html:text/html},
  keywords = {blocking technique,cloud computing,complex data-intensive task,Computational modeling,data integration,data redistribution,entity resolution,Erbium,Image color analysis,Indexes,load balancing,Load management,MapReduce,Memory management,real cloud infrastructure,Scalability,search space,skewed data,skew handling},
  abstract = {The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. In the presence of skewed data, sophisticated redistribution approaches thus become necessary to achieve load balancing among all reduce tasks to be executed in parallel. For the complex problem of entity resolution, we propose and evaluate two approaches for such skew handling and load balancing. The approaches support blocking techniques to reduce the search space of entity resolution, utilize a preprocessing MapReduce job to analyze the data distribution, and distribute the entities of large blocks among multiple reduce tasks. The evaluation on a real cloud infrastructure shows the value and effectiveness of the proposed load balancing approaches.},
  timestamp = {2016-11-18T09:07:04Z},
  groups = {Master\_Thesis}
}

@article{KTR:Multipass:12,
  author = {Kolb, Lars and Thor, Andreas and Rahm, Erhard},
  title = {Multi-Pass Sorted Neighborhood Blocking with {{MapReduce}}},
  year = {2012},
  journal = {Computer Science-Research and Development},
  volume = {27},
  number = {1},
  pages = {45--63},
  url = {http://link.springer.com/article/10.1007/s00450-011-0177-x},
  file = {Multi-pass Sorted Neighborhood Blocking with MapReduce - multi_pass_sn_with_mr.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ZGPT3TRF/multi_pass_sn_with_mr.pdf:application/pdf},
  timestamp = {2016-11-18T09:08:21Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-18}
}

@article{PKPN:Metablocking:14,
  author = {Papadakis, George and Koutrika, Georgia and Palpanas, Themis and Nejdl, Wolfgang},
  shorttitle = {Meta-Blocking},
  title = {Meta-Blocking: {{Taking}} Entity Resolutionto the next Level},
  year = {2014},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {26},
  number = {8},
  pages = {1946--1960},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6487505},
  file = {tkde13-metablocking.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TDBNDX2E/tkde13-metablocking.pdf:application/pdf},
  timestamp = {2016-11-18T10:41:25Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-18}
}

@inproceedings{WSD:Fast:12,
  author = {Welch, Michael J. and Sane, Aamod and Drome, Chris},
  title = {Fast and {{Accurate Incremental Entity Resolution Relative}} to an {{Entity Knowledge Base}}},
  year = {2012},
  booktitle = {Proceedings of the 21st {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-4503-1156-4},
  pages = {2667--2670},
  url = {http://doi.acm.org/10.1145/2396761.2398719},
  doi = {10.1145/2396761.2398719},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/844TPWNT/Welch et al. - 2012 - Fast and Accurate Incremental Entity Resolution Re.pdf:application/pdf},
  keywords = {Deduplication,entity resolution,knowledge base},
  abstract = {User facing topical web applications such as events or shopping sites rely on large collections of data records about real world entities that are updated at varying latencies ranging from days to seconds. For example, event venue details are changed relatively infrequently whereas ticket pricing and availability for an event is often updated in near-realtime. Users regard these sites as high quality if they seldom show duplicates, the URLs are stable, and their content is fresh, so it is important to resolve duplicate entity records with high quality and low latencies. High quality entity resolution typically evaluates the entire record corpus for similar record clusters at the cost of latency, while low latency resolution examines the least possible entities to keep time to a minimum, even at the cost of quality. In this paper we show how to keep low latency while achieving high quality, combining the best of both approaches: given an entity to be resolved, our incremental Fastpath system, in a matter of milliseconds, makes approximately the same decisions that the underlying batch system would have made. Our experiments show that the Fastpath system makes matching decisions for previously unseen entities with 90\% precision and 98\% recall relative to batch decisions, with latencies under 20ms on commodity hardware.},
  timestamp = {2016-11-18T10:47:43Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-11-18},
  series = {CIKM '12},
  address = {New York, NY, USA}
}

@article{LL:Two:13,
  author = {Li, Shouheng and Liang, Huizhi Elly},
  title = {A {{Two Stage Similarity}}-Aware {{Indexing}} for {{Real}}-Time {{Large Scale Entity Resolution}}},
  year = {2013},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.367.2117\&rep=rep1\&type=pdf},
  file = {download.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KID2V6J3/download.pdf:application/pdf},
  timestamp = {2016-11-18T10:48:30Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-18}
}

@article{KHKJ:Entity:16,
  author = {Kim, Taehong and Hwang, Mi-Nyeong and Kim, Young-Min and Jeong, Do-Heon},
  title = {Entity {{Resolution Approach}} of {{Data Stream Management Systems}}},
  year = {2016},
  month = apr,
  journal = {Wireless Personal Communications},
  pages = {1--14},
  issn = {0929-6212, 1572-834X},
  url = {http://link.springer.com/article/10.1007/s11277-016-3275-z},
  doi = {10.1007/s11277-016-3275-z},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/JZ5IDR6Z/Kim et al. - 2016 - Entity Resolution Approach of Data Stream Manageme.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2KUDASUT/s11277-016-3275-z.html:text/html},
  abstract = {Owing to the technological advancements in Semantic Web and sensor networks, a large amount of data has been produced in association with the open data policy. However, data stream management systems that process stream data have focused on the processing of a large amount of data with little priority on data identification, integration, and external linkage. Furthermore, entity resolution is focused mainly on static database-based technologies. In this study, a real-time stream data processing architecture that can perform the integration and entity resolution of streaming-type heterogeneous input data and interlink with external data is designed. To achieve this goal, a light adapter to integrate heterogeneous data into standard scheme and blocking technique to reduce comparison candidates are applied. The implemented data adapters shows 4 times higher throughput than open source data parsers and the entity resolution results with streaming data shows similar performance with the static data sets. The proposed streaming data entity resolution architecture is expected to form the basis of data integration research that can integrate various information sources of data efficiently, enrich internal data.},
  timestamp = {2016-11-18T10:49:05Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-18},
  language = {en}
}

@article{SH:Semantic:07,
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  title = {Semantic Hashing},
  year = {2007},
  journal = {RBM},
  volume = {500},
  number = {3},
  pages = {500},
  url = {http://www.utstat.toronto.edu/~rsalakhu/papers/semantic_final.pdf},
  file = {Salakhutdinov and Hinton - 2007 - Semantic hashing.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/W7A9JNCB/Salakhutdinov and Hinton - 2007 - Semantic hashing.pdf:application/pdf},
  timestamp = {2016-11-18T11:16:23Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-18}
}

@inproceedings{CGH:Similarityaware:09,
  author = {Christen, Peter and Gayler, Ross and Hawking, David},
  title = {Similarity-Aware {{Indexing}} for {{Real}}-Time {{Entity Resolution}}},
  year = {2009},
  booktitle = {Proceedings of the 18th {{ACM Conference}} on {{Information}} and {{Knowledge Management}}},
  isbn = {978-1-60558-512-3},
  pages = {1565--1568},
  url = {http://doi.acm.org/10.1145/1645953.1646173},
  doi = {10.1145/1645953.1646173},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/T9JBMDK2/Christen et al. - 2009 - Similarity-aware Indexing for Real-time Entity Res.pdf:application/pdf},
  keywords = {approximate string matching,data matching,inverted indexing,phonetic encoding,record linkage,Scalability,similarity query},
  abstract = {Entity resolution, also known as data matching or record linkage, is the task of identifying and matching records from several databases that refer to the same entities. Traditionally, entity resolution has been applied in batch-mode and on static databases. However, many organisations are increasingly faced with the challenge of having large databases containing entities that need to be matched in real-time with a stream of query records also containing entities, such that the best matching records are retrieved. Example applications include online law enforcement and national security databases, public health surveillance and emergency response systems, financial verification systems, online retail stores, eGovernment services, and digital libraries. A novel inverted index based approach for real-time entity resolution is presented in this paper. At build time, similarities between attribute values are computed and stored to support the fast matching of records at query time. The presented approach differs from other approaches to approximate query matching in that it allows any similarity comparison function, and any 'blocking' (encoding) function, both possibly domain specific, to be incorporated. Experimental results on a real-world database indicate that the total size of all data structures of this novel index approach grows sub-linearly with the size of the database, and that it allows matching of query records in sub-second time, more than two orders of magnitude faster than a traditional entity resolution index approach. The interested reader is referred to the longer version of this paper [5].},
  timestamp = {2016-11-21T09:06:42Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-11-21},
  series = {CIKM '09},
  address = {New York, NY, USA}
}

@inproceedings{RCL.EA:Dynamic:13,
  author = {Ramadan, Banda and Christen, Peter and Liang, Huizhi and Gayler, Ross W. and Hawking, David},
  title = {Dynamic Similarity-Aware Inverted Indexing for Real-Time Entity Resolution},
  year = {2013},
  booktitle = {Pacific-{{Asia Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  pages = {47--58},
  url = {http://link.springer.com/chapter/10.1007/978-3-642-40319-4_5},
  file = {ramadan2013dmapps.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3IZZMMJ9/ramadan2013dmapps.pdf:application/pdf},
  timestamp = {2017-01-27T20:34:13Z},
  groups = {Master\_Thesis},
  publisher = {{Springer}},
  urldate = {2016-11-21}
}

@article{INNV:Onthefly:10,
  author = {Ioannou, Ekaterini and Nejdl, Wolfgang and Nieder{\'e}e, Claudia and Velegrakis, Yannis},
  title = {On-the-Fly {{Entity}}-Aware {{Query Processing}} in the {{Presence}} of {{Linkage}}},
  year = {2010},
  month = sep,
  journal = {Proc. VLDB Endow.},
  volume = {3},
  number = {1-2},
  pages = {429--438},
  issn = {2150-8097},
  url = {http://dx.doi.org/10.14778/1920841.1920898},
  doi = {10.14778/1920841.1920898},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/68PC6Q3S/Ioannou et al. - 2010 - On-the-fly Entity-aware Query Processing in the Pr.pdf:application/pdf},
  abstract = {Entity linkage is central to almost every data integration and data cleaning scenario. Traditional techniques use some computed similarity among data structure to perform merges and then answer queries on the merged data. We describe a novel framework for entity linkage with uncertainty. Instead of using the linkage information to merge structures a-priori, possible linkages are stored alongside the data with their belief value. A new probabilistic query answering technique is used to take the probabilistic linkage into consideration. The framework introduces a series of novelties: (i) it performs merges at run time based not only on existing linkages but also on the given query; (ii) it allows results that may contain structures not explicitly represented in the data, but generated as a result of a reasoning on the linkages; and (iii) enables an evaluation of the query conditions that spans across linked structures, offering a functionality not currently supported by any traditional probabilistic databases. We formally define the semantics, describe an efficient implementation and report on the findings of our experimental evaluation.},
  timestamp = {2016-11-21T09:10:56Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-21}
}

@article{DML:Efficient:11,
  author = {Dey, Debabrata and Mookerjee, Vijay and Liu, Dengpan},
  title = {Efficient {{Techniques}} for {{Online Record Linkage}}},
  year = {2011},
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {23},
  number = {3},
  pages = {373--387},
  issn = {1041-4347},
  url = {http://ieeexplore.ieee.org/document/5551133/},
  doi = {10.1109/TKDE.2010.134},
  file = {05551133.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/C5FSEDG6/05551133.pdf:application/pdf},
  timestamp = {2016-11-21T09:14:00Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-21}
}

@article{RCLG:Dynamic:15,
  author = {Ramadan, Banda and Christen, Peter and Liang, Huizhi and Gayler, Ross W.},
  title = {Dynamic {{Sorted Neighborhood Indexing}} for {{Real}}-{{Time Entity Resolution}}},
  year = {2015},
  month = oct,
  journal = {J. Data and Information Quality},
  volume = {6},
  number = {4},
  pages = {15:1--15:29},
  issn = {1936-1955},
  url = {http://doi.acm.org/10.1145/2816821},
  doi = {10.1145/2816821},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3VE7U2SV/Ramadan et al. - 2015 - Dynamic Sorted Neighborhood Indexing for Real-Time.pdf:application/pdf},
  keywords = {Braided tree,data matching,dynamic indexing,real-time query,record linkage},
  abstract = {Real-time Entity Resolution (ER) is the process of matching query records in subsecond time with records in a database that represent the same real-world entity. Indexing techniques are generally used to efficiently extract a set of candidate records from the database that are similar to a query record, and that are to be compared with the query record in more detail. The sorted neighborhood indexing method, which sorts a database and compares records within a sliding window, has been successfully used for ER of large static databases. However, because it is based on static sorted arrays and is designed for batch ER that resolves all records in a database rather than resolving those relating to a single query record, this technique is not suitable for real-time ER on dynamic databases that are constantly updated. We propose a tree-based technique that facilitates dynamic indexing based on the sorted neighborhood method, which can be used for real-time ER, and investigate both static and adaptive window approaches. We propose an approach to reduce query matching times by precalculating the similarities between attribute values stored in neighboring tree nodes. We also propose a multitree solution where different sorting keys are used to reduce the effects of errors and variations in attribute values on matching quality by building several distinct index trees. We experimentally evaluate our proposed techniques on large real datasets, as well as on synthetic data with different data quality characteristics. Our results show that as the index grows, no appreciable increase occurs in both record insertion and query times, and that using multiple trees gives noticeable improvements on matching quality with only a small increase in query time. Compared to earlier indexing techniques for real-time ER, our approach achieves significantly reduced indexing and query matching times while maintaining high matching accuracy.},
  timestamp = {2016-11-21T10:45:06Z},
  groups = {Master\_Thesis},
  urldate = {2016-11-21}
}

@inproceedings{FCWR:ClusteringBased:15,
  author = {Fisher, Jeffrey and Christen, Peter and Wang, Qing and Rahm, Erhard},
  title = {A {{Clustering}}-{{Based Framework}} to {{Control Block Sizes}} for {{Entity Resolution}}},
  year = {2015},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-1-4503-3664-2},
  pages = {279--288},
  url = {http://doi.acm.org/10.1145/2783258.2783396},
  doi = {10.1145/2783258.2783396},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/GKGETHAP/Fisher et al. - 2015 - A Clustering-Based Framework to Control Block Size.pdf:application/pdf},
  keywords = {blocking,data cleaning,Indexing,record linkage},
  abstract = {Entity resolution (ER) is a common data cleaning task that involves determining which records from one or more data sets refer to the same real-world entities. Because a pairwise comparison of all records scales quadratically with the number of records in the data sets to be matched, it is common to use blocking or indexing techniques to reduce the number of comparisons required. These techniques split the data sets into blocks and only records within blocks are compared with each other. Most existing blocking techniques do not provide control over the size of the generated blocks, despite this control being important in many practical applications of ER, such as privacy-preserving record linkage and real-time ER. We propose two novel hierarchical clustering approaches which can generate blocks within a specified size range, and we present a penalty function which allows control of the trade-off between block quality and block size in the clustering process. We evaluate our techniques on three real-world data sets and compare them against three baseline approaches. The results show our proposed techniques perform well on the measures of pairs completeness and reduction ratio compared to the baseline approaches, while also satisfying the block size restrictions.},
  timestamp = {2016-11-22T11:22:48Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2016-11-21},
  series = {KDD '15},
  address = {New York, NY, USA}
}

@inproceedings{CG:ContextAware:15,
  author = {Christen, P. and Gayler, R. W.},
  title = {Context-{{Aware Approximate String Matching}} for {{Large}}-{{Scale Real}}-{{Time Entity Resolution}}},
  year = {2015},
  month = nov,
  booktitle = {2015 {{IEEE International Conference}} on {{Data Mining Workshop}} ({{ICDMW}})},
  pages = {211--217},
  doi = {10.1109/ICDMW.2015.152},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/I32JNMIX/Christen and Gayler - 2015 - Context-Aware Approximate String Matching for Larg.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FU8KVNFQ/7395673.html:text/html},
  keywords = {Australia,Conferences,context-aware approximate string matching,contextual information,data matching,edit distance,Electronic mail,Indexes,large-scale real-time entity resolution,Positron emission tomography,Query processing,query records,real-time matching,Real-time systems,similarity calculation,string databases,string matching,ubiquitous computing},
  abstract = {Techniques for approximate string matching have been widely studied over several decades. They are required in many applications, including entity resolution, spell checking, similarity joins, and biological sequence comparison. Most existing techniques for approximate string matching used in entity resolution only consider the two strings that are compared. They neglect contextual information such as the frequency of how often strings occur in a database, the likelihood of the character edits between strings, or how many other similar strings there are in a database. In this paper we investigate if incorporating such contextual information into edit distance based approximate string matching can improve matching quality for real-time entity resolution. In this application, query records have to be matched in sub-second time to records in a large database that refer to the same entity. We evaluate our approach on two large real data sets and compare it to several baseline approaches. Our results show that considering edit frequency and the neighborhood size of a string can improve matching results, while taking string frequencies into account can actually make results worse.},
  timestamp = {2016-11-24T16:50:08Z},
  groups = {Master\_Thesis}
}

@inproceedings{CVW:Efficient:15,
  author = {Christen, P. and Vatsalan, D. and Wang, Q.},
  title = {Efficient {{Entity Resolution}} with {{Adaptive}} and {{Interactive Training Data Selection}}},
  year = {2015},
  month = nov,
  booktitle = {2015 {{IEEE International Conference}} on {{Data Mining}}},
  pages = {727--732},
  doi = {10.1109/ICDM.2015.63},
  file = {26317119.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MGGGD6PX/26317119.pdf:application/pdf;IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TU9JRH4C/Christen et al. - 2015 - Efficient Entity Resolution with Adaptive and Inte.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2STWCKMU/7373380.html:text/html;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/K2M8GE22/7373380.html:text/html},
  keywords = {active learning,adaptive training data selection,Clustering algorithms,cluster structure,Databases,data matching,entity resolution,ER applications,Erbium,ER classifier,fully supervised classifiers,hierarchical clustering,interactive labeling,interactive systems,interactive training data selection,Labeling,manual labeling,manual labels,Manuals,matching quality,noisy oracle,pattern classification,pattern matching,public data sets,record linkage,Silicon,Training,Training data,user defined sampling error margin,weight vectors},
  abstract = {Entity resolution (ER) is the task of deciding which records in one or more databases refer to the same real-world entities. A crucial step in ER is the accurate classification of pairs of records into matches and non-matches. In most practical ER applications, obtaining training data \%of high quality is costly and time consuming. Various techniques have been proposed for ER to interactively generate training data and learn an accurate classifier. We propose an approach for training data selection for ER that exploits the cluster structure of the weight vectors (similarities) calculated from compared record pairs. Our approach adaptively selects an optimal number of informative training examples for manual labeling based on a user defined sampling error margin, and recursively splits the set of weight vectors to find pure enough subsets for training. We consider two aspects of ER that are highly significant in practice: a limited budget for the number of manual labeling that can be done, and a noisy oracle where manual labels might be incorrect. Experiments on four real public data sets show that our approach can significantly reduce manual labeling efforts for training an ER classifier while achieving matching quality comparative to fully supervised classifiers.},
  timestamp = {2017-01-13T10:39:07Z},
  groups = {Master\_Thesis}
}

@inproceedings{BLB.EA:API:13,
  author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and VanderPlas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"e}l},
  title = {{{API}} Design for Machine Learning Software: Experiences from the Scikit-Learn Project},
  year = {2013},
  booktitle = {{{ECML PKDD Workshop}}: {{Languages}} for {{Data Mining}} and {{Machine Learning}}},
  pages = {108--122},
  timestamp = {2016-11-24T20:00:36Z},
  groups = {Maschine Learning,Maschine Learning}
}

@article{PVG.EA:Scikitlearn:11,
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2825--2830},
  timestamp = {2016-11-24T20:01:08Z},
  groups = {Maschine Learning,Maschine Learning}
}

@incollection{RC:Unsupervised:15,
  author = {Ramadan, Banda and Christen, Peter},
  title = {Unsupervised {{Blocking Key Selection}} for {{Real}}-{{Time Entity Resolution}}},
  year = {2015},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-3-319-18031-1 978-3-319-18032-8},
  volume = {9078},
  pages = {574--585},
  url = {http://link.springer.com/10.1007/978-3-319-18032-8_45},
  file = {56e7210f08ae438aab87fe2c.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7AZZISC5/56e7210f08ae438aab87fe2c.pdf:application/pdf},
  timestamp = {2016-11-28T11:33:08Z},
  groups = {Master\_Thesis},
  editor = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
  publisher = {{Springer International Publishing}},
  urldate = {2016-11-28},
  address = {Cham}
}

@book{LLRo:Two:13,
  author = {Li, Shouheng and Liang, H. and Ramadan, Banda and {others}},
  title = {Two {{Stage Similarityaware Indexing}} for {{Large Scale Realtime Entity Resolution}}},
  year = {2013},
  url = {https://cs.anu.edu.au/courses/csprojects/13S1/Final_presentations/Presentations\%20by\%20student/Shouheng_Li_Final.pdf},
  file = {[PDF] researchgate.net:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/E7TR4GDM/Li et al. - 2013 - A Two Stage Similarityaware Indexing for Large Sca.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/XVXPFC2D/Shouheng_Li_Final.pdf:application/pdf},
  timestamp = {2016-11-29T08:38:26Z},
  groups = {Master\_Thesis},
  publisher = {{AusDM}},
  urldate = {2016-11-29}
}

@article{HC:note:,
  author = {Hand, David and Christen, Peter},
  title = {A Note on Using the {{F}}-Measure for Evaluating Data Linkage Algorithms},
  url = {https://www.newton.ac.uk/files/preprints/ni16047.pdf},
  file = {ni16047.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/Z2QUTGTU/ni16047.pdf:application/pdf},
  timestamp = {2016-12-15T15:58:27Z},
  groups = {Master\_Thesis},
  urldate = {2016-12-15}
}

@book{LAS.EA:CIKM:14,
  shorttitle = {{{CIKM}}'14},
  title = {{{CIKM}}'14: Proceedings of the 2014 {{ACM International Conference}} on {{Information}} and {{Knowledge Management}} : {{November}} 3-7, 2014, {{Shanghai}}, {{China}}. {{DOLAP}}'14. {{DTMBIO}}'14. {{DUBMOD}}'14. {{ESAIR}}'14. {{LocWeb}}'14. {{PIKM}}'14. {{PSBD}}'14. {{Web}}-{{KR}}'14.},
  year = {2014},
  isbn = {978-1-4503-2598-1},
  file = {01 Ramadan  Indexing techniques for 2016.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2CUK5UD5/01 Ramadan  Indexing techniques for 2016.pdf:application/pdf},
  timestamp = {2016-12-19T09:54:56Z},
  groups = {Master\_Thesis},
  note = {OCLC: 934149886},
  editor = {Li, Jianzhong and {Association for Computing Machinery} and {Special Interest Group on Information Retrieval} and {Association for Computing Machinery} and {Special Interest Group on Hypertext}, Hypermedia and Web and {CIKM} and {DOLAP} and {DTMBIO} and {DUBMOD} and {ESAIR} and {LocWeb} and {PIKM} and {PSBD} and {Web-KR}},
  publisher = {{ACM}},
  language = {English},
  address = {New York, NY}
}

@inproceedings{KM:Unsupervised:13,
  author = {Kejriwal, M. and Miranker, D. P.},
  title = {An {{Unsupervised Algorithm}} for {{Learning Blocking Schemes}}},
  year = {2013},
  month = dec,
  booktitle = {2013 {{IEEE}} 13th {{International Conference}} on {{Data Mining}}},
  pages = {340--349},
  doi = {10.1109/ICDM.2013.60},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/M4UZTJUN/Kejriwal and Miranker - 2013 - An Unsupervised Algorithm for Learning Blocking Sc.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/V46W3M4Z/6729518.html:text/html},
  keywords = {blocking,Complexity theory,cost reduction,Couplings,data mining,data objects,domain expert,expert systems,Fisher feature selection problem,Indexing,learning blocking schemes,Measurement,Personnel,quadratic complexity,record linkage,supervised blocking key discovery algorithm,supervised learning algorithms,tabular data sets,Training,unsupervised learning,unsupervised learning algorithm,weakly labeled training set},
  abstract = {A pair wise comparison of data objects is a requisite step in many data mining applications, but has quadratic complexity. In applications such as record linkage, blocking methods may be applied to reduce the cost. That is, the data is first partitioned into a set of blocks, and pair wise comparisons computed for pairs within each block. To date, blocking methods have required the blocking scheme be given, or the provision of training data enabling supervised learning algorithms to determine a blocking scheme. In either case, a domain expert is required. This paper develops an unsupervised method for learning a blocking scheme for tabular data sets. The method is divided into two phases. First, a weakly labeled training set is generated automatically in time linear in the number of records of the entire dataset. The second phase casts blocking key discovery as a Fisher feature selection problem. The approach is compared to a state-of-the-art supervised blocking key discovery algorithm on three real-world databases and achieves favorable results.},
  timestamp = {2016-12-20T13:13:16Z},
  groups = {Master\_Thesis}
}

@inproceedings{BN:Schema:05,
  author = {Bilke, Alexander and Naumann, Felix},
  title = {Schema Matching Using Duplicates},
  year = {2005},
  booktitle = {21st {{International Conference}} on {{Data Engineering}} ({{ICDE}}'05)},
  pages = {69--80},
  url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1410107},
  file = {2424X0HVRpwc.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/EF6JAT87/2424X0HVRpwc.pdf:application/pdf},
  timestamp = {2016-12-29T14:12:42Z},
  groups = {Master\_Thesis},
  publisher = {{IEEE}},
  urldate = {2016-12-29}
}

@article{GLH:Generalized:12,
  author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
  title = {Generalized Fisher Score for Feature Selection},
  year = {2012},
  journal = {arXiv preprint arXiv:1202.3725},
  url = {http://arxiv.org/abs/1202.3725},
  file = {uai11.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/43S3I5B7/uai11.pdf:application/pdf},
  timestamp = {2016-12-30T16:47:06Z},
  groups = {Master\_Thesis},
  urldate = {2016-12-30}
}

@misc{.EA:Efficient:,
  title = {Efficient {{Interactive Training Selection}} for {{Large}}-{{Scale Entity Resolution}}},
  url = {https://www.researchgate.net/publication/280216506_Efficient_Interactive_Training_Selection_for_Large-Scale_Entity_Resolution},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/D8W9UGTJ/280216506_Efficient_Interactive_Training_Selection_for_Large-Scale_Entity_Resolution.pdf:application/pdf},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  timestamp = {2017-01-12T09:32:59Z},
  groups = {Master\_Thesis},
  urldate = {2017-01-12}
}

@incollection{WVC:Efficient:15,
  author = {Wang, Qing and Vatsalan, Dinusha and Christen, Peter},
  title = {Efficient {{Interactive Training Selection}} for {{Large}}-{{Scale Entity Resolution}}},
  year = {2015},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}}},
  isbn = {978-3-319-18031-1 978-3-319-18032-8},
  volume = {9078},
  pages = {562--573},
  url = {http://link.springer.com/10.1007/978-3-319-18032-8_44},
  file = {587711ee08ae329d622611ec.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/X7Z67EF8/587711ee08ae329d622611ec.pdf:application/pdf},
  timestamp = {2017-01-12T09:33:17Z},
  groups = {Master\_Thesis},
  editor = {Cao, Tru and Lim, Ee-Peng and Zhou, Zhi-Hua and Ho, Tu-Bao and Cheung, David and Motoda, Hiroshi},
  publisher = {{Springer International Publishing}},
  urldate = {2017-01-12},
  address = {Cham}
}

@article{YB:Normalized:07,
  author = {Yujian, L. and Bo, L.},
  title = {A {{Normalized Levenshtein Distance Metric}}},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {29},
  number = {6},
  pages = {1091--1095},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2007.1078},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/9N5Z9E5R/Yujian and Bo - 2007 - A Normalized Levenshtein Distance Metric.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7B4D9NRH/4160958.html:text/html},
  keywords = {AESA.,Algorithms,Artificial Intelligence,Biomedical signal processing,Computational biology,Cost function,Error correction,Handwriting recognition,Image Enhancement,Image Interpretation; Computer-Assisted,Image recognition,Imaging; Three-Dimensional,Information retrieval,Information Storage and Retrieval,Levenshtein distance,metric,Models; Statistical,normalized edit distance,Pattern recognition,Pattern Recognition; Automated,Sequence comparison,Sequences,Signal processing algorithms},
  abstract = {Although a number of normalized edit distances presented so far may offer good performance in some applications, none of them can be regarded as a genuine metric between strings because they do not satisfy the triangle inequality. Given two strings X and Y over a finite alphabet, this paper defines a new normalized edit distance between X and Y as a simple function of their lengths (|X| and |Y|) and the Generalized Levenshtein Distance (GLD) between them. The new distance can be easily computed through GLD with a complexity of O(|X| cdot |Y|) and it is a metric valued in [0, 1] under the condition that the weight function is a metric over the set of elementary edit operations with all costs of insertions/deletions having the same weight. Experiments using the AESA algorithm in handwritten digit recognition show that the new distance can generally provide similar results to some other normalized edit distances and may perform slightly better if the triangle inequality is violated in a particular data set.},
  timestamp = {2017-01-12T16:47:24Z},
  groups = {Matcher,Matcher}
}

@article{Chr:Preparation:13,
  author = {Christen, Peter},
  title = {Preparation of a Real Temporal Voter Data Set for Record Linkage and Duplicate Detection Research},
  year = {2013},
  url = {http://cs.anu.edu.au/~./Peter.Christen/publications/ncvoter-report-29june2014.pdf},
  file = {ncvoter-report-29june2014.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KIMZNK7A/ncvoter-report-29june2014.pdf:application/pdf},
  timestamp = {2017-01-13T10:38:57Z},
  groups = {Master\_Thesis},
  urldate = {2017-01-13}
}

@inproceedings{Lev:Binary:66,
  author = {Levenshtein, Vladimir I.},
  title = {Binary Codes Capable of Correcting Deletions, Insertions, and Reversals},
  year = {1966},
  booktitle = {Soviet Physics Doklady},
  volume = {10},
  pages = {707--710},
  url = {https://nymity.ch/sybilhunting/pdf/Levenshtein1966a.pdf},
  file = {[PDF] nymity.ch:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/AM88ACNG/Levenshtein - 1966 - Binary codes capable of correcting deletions, inse.pdf:application/pdf},
  timestamp = {2017-02-10T14:46:58Z},
  groups = {Master\_Thesis},
  urldate = {2017-02-10}
}

@article{Dam:technique:64,
  author = {Damerau, Fred J.},
  title = {A Technique for Computer Detection and Correction of Spelling Errors},
  year = {1964},
  journal = {Communications of the ACM},
  volume = {7},
  number = {3},
  pages = {171--176},
  url = {http://dl.acm.org/citation.cfm?id=363994},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/J2IP5X23/citation.html:text/html},
  timestamp = {2017-02-10T14:56:47Z},
  groups = {Master\_Thesis},
  urldate = {2017-02-10}
}

@misc{Wiki:String:16,
  title = {String Kernel},
  year = {2016},
  month = mar,
  journal = {Wikipedia},
  url = {https://en.wikipedia.org/w/index.php?title=String_kernel\&oldid=709763814},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/NUNAR99H/index.html:text/html},
  abstract = {In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be.
Using string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors. String kernels are used in domains where sequence data are to be clustered or classified, e.g. in text mining and gene analysis.},
  timestamp = {2017-02-11T13:17:57Z},
  groups = {Master\_Thesis},
  note = {Page Version ID: 709763814},
  urldate = {2017-02-11},
  language = {en},
  copyright = {Creative Commons Attribution-ShareAlike License}
}

@article{Coh:WHIRL:00,
  author = {Cohen, William W.},
  shorttitle = {{{WHIRL}}},
  title = {{{WHIRL}}: {{A}} Word-Based Information Representation Language},
  year = {2000},
  month = apr,
  journal = {Artificial Intelligence},
  volume = {118},
  number = {1\textendash{}2},
  pages = {163--196},
  issn = {0004-3702},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370299001022},
  doi = {10.1016/S0004-3702(99)00102-2},
  file = {ScienceDirect Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DPJAEAZ5/Cohen - 2000 - WHIRL A word-based information representation lan.pdf:application/pdf;ScienceDirect Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KFXP5V66/S0004370299001022.html:text/html},
  keywords = {Heterogeneous databases,information extraction,information integration,Information retrieval,Knowledge representation,Text categorization,Textual similarity},
  abstract = {We describe WHIRL, an ``information representation language'' that synergistically combines properties of logic-based and text-based representation systems. WHIRL is a subset of Datalog that has been extended by introducing an atomic type for textual entities, an atomic operation for computing textual similarity, and a ``soft'' semantics; that is, inferences in WHIRL are associated with numeric scores, and presented to the user in decreasing order by score. This paper briefly describes WHIRL, and then surveys a number of applications. We show that WHIRL strictly generalizes both ranked retrieval of documents, and logical deduction; that nontrivial queries about large databases can be answered efficiently; that WHIRL can be used to accurately integrate data from heterogeneous information sources, such as those found on the Web; that WHIRL can be used effectively for inductive classification of text; and finally, that WHIRL can be used to semi-automatically generate extraction programs for structured documents.},
  timestamp = {2017-02-13T12:01:25Z},
  groups = {Master\_Thesis},
  urldate = {2017-02-13}
}

@article{LSS.EA:Text:02,
  author = {Lodhi, Huma and Saunders, Craig and Shawe-Taylor, John and Cristianini, Nello and Watkins, Chris},
  title = {Text Classification Using String Kernels},
  year = {2002},
  journal = {Journal of Machine Learning Research},
  volume = {2},
  number = {Feb},
  pages = {419--444},
  url = {http://www.jmlr.org/papers/v2/lodhi02a.html},
  file = {lodhi02a.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KEWJJ7CS/lodhi02a.pdf:application/pdf},
  timestamp = {2017-02-13T15:46:27Z},
  groups = {Master\_Thesis},
  urldate = {2017-02-13}
}

@inproceedings{BGV:Training:92,
  author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
  title = {A {{Training Algorithm}} for {{Optimal Margin Classifiers}}},
  year = {1992},
  booktitle = {Proceedings of the {{Fifth Annual Workshop}} on {{Computational Learning Theory}}},
  isbn = {978-0-89791-497-0},
  pages = {144--152},
  url = {http://doi.acm.org/10.1145/130385.130401},
  doi = {10.1145/130385.130401},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VZM9J2UI/Boser et al. - 1992 - A Training Algorithm for Optimal Margin Classifier.pdf:application/pdf},
  abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.},
  timestamp = {2017-02-13T15:48:04Z},
  groups = {Master\_Thesis},
  publisher = {{ACM}},
  urldate = {2017-02-13},
  series = {COLT '92},
  address = {New York, NY, USA}
}

@book{Chr:Data:12,
  author = {Christen, Peter},
  shorttitle = {Data Matching},
  title = {Data Matching: Concepts and Techniques for Record Linkage, Entity Resolution, and Duplicate Detection},
  year = {2012},
  url = {https://books.google.de/books?hl=de\&lr=\&id=LZrT6eWf9NMC\&oi=fnd\&pg=PR7\&dq=data+matching+concepts+and+techniques+for+record+linkage+entity+resolution+and+duplicate+detection\&ots=TeF9DuEtDL\&sig=RbDdjgCizuz6naFDUkRUIW-uzq8},
  file = {[PDF] anu.edu.au:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/FQCRFGVX/Christen - 2012 - Data matching concepts and techniques for record .pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/WXZVBNUB/books.html:text/html},
  timestamp = {2017-02-14T14:51:50Z},
  groups = {Master\_Thesis},
  publisher = {{Springer Science \& Business Media}},
  urldate = {2017-02-14}
}

@comment{jabref-meta: databaseType:bibtex;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Basics\;0\;;
1 ExplicitGroup:Master_Thesis\;0\;;
2 ExplicitGroup:Matcher\;0\;;
2 ExplicitGroup:Address\;0\;;
2 ExplicitGroup:Maschine Learning\;0\;;
1 ExplicitGroup:Matcher\;0\;;
1 ExplicitGroup:Address\;0\;;
1 ExplicitGroup:Maschine Learning\;0\;;
}

