
@article{kolb_parallel_2013,
  title = {Parallel Entity Resolution with Dedoop},
  volume = {13},
  timestamp = {2016-10-07T17:02:24Z},
  number = {1},
  urldate = {2016-10-06},
  url = {http://link.springer.com/article/10.1007/s13222-012-0110-x},
  journal = {Datenbank-Spektrum},
  author = {Kolb, Lars and Rahm, Erhard},
  year = {2013},
  pages = {23--32},
  file = {[PDF] uni-leipzig.de:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PAMWB8N8/Kolb and Rahm - 2013 - Parallel entity resolution with dedoop.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QKBKW45K/s13222-012-0110-x.html:text/html}
}

@inproceedings{shah_storm_2015,
  title = {Storm {{Pub}}-{{Sub}}: {{High Performance}}, {{Scalable Content Based Event Matching System Using Storm}}},
  shorttitle = {Storm {{Pub}}-{{Sub}}},
  doi = {10.1109/IPDPSW.2015.95},
  abstract = {Storm pub-sub is a novel high performance publish subscribe system designed to efficiently match events and the subscriptions with high throughput. Moving a content based pub-sub system first to a local cluster and then to a distributed cluster framework is for high performance and scalability. We depart from the use of broker overlays, where each server must support the whole range of operations of a pub-sub service, as well as overlay management and routing functionality. In this system different operations involved in pub-sub are separated to leverage their natural potential for parallelization using bolts. The storm pub-sub is compared with the traditional pub-sub system Siena, a broker based architecture. Through experimentation on local cluster as well as on distributed cluster we show that our approach of designing publish subscribe system on storm scales well for high volume of data. Storm pub-sub system approximately produces 2200 event/s on distributed cluster. In this paper we describe design and implementation of storm pub-sub and evaluate it in terms of scalability and throughput.},
  timestamp = {2016-10-07T17:02:24Z},
  booktitle = {Parallel and {{Distributed Processing Symposium Workshop}} ({{IPDPSW}}), 2015 {{IEEE International}}},
  author = {Shah, M. A. and Kulkarni, D. B.},
  month = may,
  year = {2015},
  keywords = {Algorithm design and analysis,bolts,broker based architecture,broker overlays,content based pub-sub system,Distributed cluster,distributed cluster framework,Fasteners,high performance publish subscribe system,high performance scalable content based event matching system,High Throughput,local cluster,Matching time,message passing,middleware,overlay management,parallel processing,pattern matching,Publish subscribe,Publish subscribe systems,pub-sub system Siena,routing functionality,Scalability,software reliability,Storm pub-sub,Storms,Throughput,Topology},
  pages = {585--590},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/TVN25BWB/Shah and Kulkarni - 2015 - Storm Pub-Sub High Performance, Scalable Content .pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8JIUZ3UG/7284361.html:text/html}
}

@article{elmagarmid_duplicate_2007,
  title = {Duplicate {{Record Detection}}: {{A Survey}}},
  volume = {19},
  issn = {1041-4347},
  shorttitle = {Duplicate {{Record Detection}}},
  doi = {10.1109/TKDE.2007.250581},
  abstract = {Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a difficult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats, or any combination of these factors. In this paper, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar field entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the efficiency and scalability of approximate duplicate detection algorithms. We conclude with coverage of existing tools and with a brief discussion of the big open problems in the area},
  timestamp = {2016-10-07T07:10:54Z},
  number = {1},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Elmagarmid, A. K. and Ipeirotis, P. G. and Verykios, V. S.},
  month = jan,
  year = {2007},
  keywords = {Cleaning,Computer errors,Computer Society,Cost function,Couplings,database hardening,database management system,database management systems,data cleaning,data deduplication,data integration,data integrity,data mining,Detection algorithms,Duplicate detection,duplicate detection algorithm,duplicate record detection,entity matching.,entity resolution,fuzzy duplicate detection,identity uncertainty,instance identification,Mirrors,name matching,record linkage,Relational databases,Scalability,transcription error,Uncertainty},
  pages = {1--16},
  file = {[PDF] nyu.edu:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UQHE5W3P/Elmagarmid et al. - 2007 - Duplicate record detection A survey.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/2C5RCMZH/4016511.html:text/html;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/8VPZFZ4N/4016511.html:text/html}
}

@article{kopcke_frameworks_2010,
  title = {Frameworks for Entity Matching: {{A}} Comparison},
  volume = {69},
  issn = {0169-023X},
  shorttitle = {Frameworks for Entity Matching},
  doi = {10.1016/j.datak.2009.10.003},
  abstract = {Entity matching is a crucial and difficult task for data integration. Entity matching frameworks provide several methods and their combination to effectively solve different match tasks. In this paper, we comparatively analyze 11 proposed frameworks for entity matching. Our study considers both frameworks which do or do not utilize training data to semi-automatically find an entity matching strategy to solve a given match task. Moreover, we consider support for blocking and the combination of different match algorithms. We further study how the different frameworks have been evaluated. The study aims at exploring the current state of the art in research prototypes of entity matching frameworks and their evaluations. The proposed criteria should be helpful to identify promising framework approaches and enable categorizing and comparatively assessing additional entity matching frameworks and their evaluations.},
  timestamp = {2016-10-07T17:02:24Z},
  number = {2},
  urldate = {2016-10-06},
  url = {http://www.sciencedirect.com/science/article/pii/S0169023X09001451},
  journal = {Data \& Knowledge Engineering},
  author = {K{\"o}pcke, Hanna and Rahm, Erhard},
  month = feb,
  year = {2010},
  keywords = {Entity matching,entity resolution,Matcher combination,Match optimization,Training selection},
  pages = {197--210},
  file = {FrameworksForEntityMatchingAComparison_dke.pdf:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/QK42BHG7/FrameworksForEntityMatchingAComparison_dke.pdf:application/pdf;ScienceDirect Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/7CA5U7X4/S0169023X09001451.html:text/html}
}

@article{konda_magellan:_2016,
  title = {Magellan: {{Toward Building Entity Matching Management Systems}}},
  volume = {9},
  issn = {2150-8097},
  shorttitle = {Magellan},
  doi = {10.14778/2994509.2994535},
  abstract = {Entity matching (EM) has been a long-standing challenge in data management. Most current EM works focus only on developing matching algorithms. We argue that far more efforts should be devoted to building EM systems. We discuss the limitations of current EM systems, then present as a solution Magellan, a new kind of EM systems. Magellan is novel in four important aspects. (1) It provides how-to guides that tell users what to do in each EM scenario, step by step. (2) It provides tools to help users do these steps; the tools seek to cover the entire EM pipeline, not just matching and blocking as current EM systems do. (3) Tools are built on top of the data analysis and Big Data stacks in Python, allowing Magellan to borrow a rich set of capabilities in data cleaning, IE, visualization, learning, etc. (4) Magellan provides a powerful scripting environment to facilitate interactive experimentation and quick "patching" of the system. We describe research challenges raised by Magellan, then present extensive experiments with 44 students and users at several organizations that show the promise of the Magellan approach.},
  timestamp = {2016-10-07T17:02:14Z},
  number = {12},
  urldate = {2016-10-07},
  url = {http://dx.doi.org/10.14778/2994509.2994535},
  journal = {Proc. VLDB Endow.},
  author = {Konda, Pradap and Das, Sanjib and {Suganthan G. C.}, Paul and Doan, AnHai and Ardalan, Adel and Ballard, Jeffrey R. and Li, Han and Panahi, Fatemah and Zhang, Haojun and Naughton, Jeff and Prasad, Shishir and Krishnan, Ganesh and Deep, Rohit and Raghavendra, Vijay},
  month = aug,
  year = {2016},
  pages = {1197--1208},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/I5JI48JD/Konda et al. - 2016 - Magellan Toward Building Entity Matching Manageme.pdf:application/pdf}
}

@incollection{cormode_summarizing_2005,
  series = {Proceedings},
  title = {Summarizing and {{Mining Skewed Data Streams}}},
  isbn = {978-0-89871-593-4},
  abstract = {Many applications generate massive data streams. Summarizing such massive data requires fast, small space algorithms to support post-hoc queries and mining. An important observation is that such streams are rarely uniform, and real data sources typically exhibit significant skewness. These are well modeled by Zipf distributions, which are characterized by a parameter, z, that captures the amount of skew. We present a data stream summary that can answer point queries with $\smallin$ accuracy and show that the space needed is only O($\smallin$--min\{1,1/z\}). This is the first o(1/$\smallin$) space algorithm for this problem, and we show it is essentially tight for skewed distributions. We show that the same data structure can also estimate the L2 norm of the stream in o(1/$\smallin$2) space for z $>$ \textonehalf, another improvement over the existing $\Omega$(1/$\smallin$2) methods. We support our theoretical results with an experimental study over a large variety of real and synthetic data. We show that significant skew is present in both textual and telecommunication data. Our methods give strong accuracy, significantly better than other methods, and behave exactly in line with their analytic bounds.},
  timestamp = {2016-10-07T13:51:46Z},
  urldate = {2016-10-07},
  url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972757.5},
  booktitle = {Proceedings of the 2005 {{SIAM International Conference}} on {{Data Mining}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  author = {Cormode, G. and Muthukrishnan, S.},
  month = apr,
  year = {2005},
  pages = {44--55},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/MGR2ATBM/Cormode and Muthukrishnan - 2005 - Summarizing and Mining Skewed Data Streams.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/G6AP6IGJ/1.9781611972757.html:text/html}
}

@article{malhotra_graph-parallel_2014,
  title = {Graph-Parallel Entity Resolution Using {{LSH}} and {{IMM}}},
  volume = {1133},
  abstract = {In this paper we describe graph-based parallel algorithms for entity resolution that improve over the map-reduce approach. We compare two approaches to parallelize a Locality Sensitive Hashing...},
  timestamp = {2016-10-07T12:02:19Z},
  urldate = {2016-10-07},
  url = {https://www.researchgate.net/publication/288130692_Graph-parallel_entity_resolution_using_LSH_and_IMM},
  journal = {ResearchGate},
  author = {Malhotra, P. and Agarwal, P. and Shroff, G.},
  month = jan,
  year = {2014},
  pages = {41--49},
  file = {Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/HNSUD9A9/288130692_Graph-parallel_entity_resolution_using_LSH_and_IMM.html:text/html}
}

@article{brizan_._2015,
  title = {A. {{Survey}} of {{Entity Resolution}} and {{Record Linkage Methodologies}}},
  volume = {6},
  issn = {1941-6687},
  timestamp = {2016-10-06T13:34:41Z},
  number = {3},
  url = {http://scholarworks.lib.csusb.edu/ciima/vol6/iss3/5},
  journal = {Communications of the IIMA},
  author = {Brizan, David and Tansel, Abdullah},
  month = jan,
  year = {2015},
  file = {"A. Survey of Entity Resolution and Record Linkage Methodologies" by David Guy Brizan and Abdullah Uz Tansel:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/PA4AUFPQ/5.html:text/html}
}

@article{kwon_study_2011,
  title = {A Study of Skew in Mapreduce Applications},
  timestamp = {2016-10-06T12:07:11Z},
  urldate = {2016-10-06},
  url = {http://ejournal.narotama.ac.id/files/A\%20study\%20of\%20skew\%20in\%20mapreduce\%20applications.pdf},
  journal = {Open Cirrus Summit},
  author = {Kwon, YongChul and Balazinska, Magdalena and Howe, Bill and Rolia, Jerome},
  year = {2011},
  file = {[PDF] narotama.ac.id:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/DNIQDC4X/Kwon et al. - 2011 - A study of skew in mapreduce applications.pdf:application/pdf}
}

@inproceedings{recasens_coreference_2010,
  title = {Coreference Resolution across Corpora: Languages, Coding Schemes, and Preprocessing Information},
  shorttitle = {Coreference Resolution across Corpora},
  timestamp = {2016-10-07T17:03:43Z},
  urldate = {2016-10-07},
  url = {http://dl.acm.org/citation.cfm?id=1858681.1858825},
  publisher = {{Association for Computational Linguistics}},
  author = {Recasens, Marta and Hovy, Eduard},
  month = jul,
  year = {2010},
  pages = {1423--1432},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/CPX98HXU/Recasens and Hovy - 2010 - Coreference resolution across corpora languages, .pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UIWID5C6/46.html:text/html}
}

@phdthesis{tirumali_efficient_2016,
  title = {{{EFFICIENT PAIR}}-{{WISE SIMILARITY COMPUTATION USING APACHE SPARK}}},
  abstract = {Entity matching is the process of identifying different manifestations of the same real world entity. These entities can be referred to as objects(string) or data instances. These entities are in turn split over several databases or clusters based on the signatures of the entities. When entity matching algorithms are performed on these databases or clusters, there is a high possibility that a particular entity pair is compared more than once. The number of comparison for any two entities depend on the number of common signatures or keys they possess. This effects the performance of any entity matching algorithm. This paper is the implementation of the algorithm written by Erhard Rahm et al. for performing redundancy free pair-wise similarity computation using MapReduce. As an improvisation to the existing implementation, this project aims to implement the algorithm in Apache Spark in standalone mode for sample of data and in cluster mode for large volume of data.},
  timestamp = {2016-10-07T17:05:11Z},
  urldate = {2016-10-07},
  url = {http://scholarworks.sjsu.edu/etd_projects/479},
  school = {San Jose State University},
  author = {Tirumali, Parineetha Gandhi},
  year = {2016},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/S5CC6TID/Tirumali - 2016 - EFFICIENT PAIR-WISE SIMILARITY COMPUTATION USING A.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/4GCKPVVN/38.html:text/html}
}

@article{whang_pay-as-you-go_2013,
  title = {Pay-{{As}}-{{You}}-{{Go Entity Resolution}}},
  volume = {25},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2012.43},
  abstract = {Entity resolution (ER) is the problem of identifying which records in a database refer to the same entity. In practice, many applications need to resolve large data sets efficiently, but do not require the ER result to be exact. For example, people data from the web may simply be too large to completely resolve with a reasonable amount of work. As another example, real-time applications may not be able to tolerate any ER processing that takes longer than a certain amount of time. This paper investigates how we can maximize the progress of ER with a limited amount of work using ``hints,'' which give information on records that are likely to refer to the same real-world entity. A hint can be represented in various formats (e.g., a grouping of records based on their likelihood of matching), and ER can use this information as a guideline for which records to compare first. We introduce a family of techniques for constructing hints efficiently and techniques for using the hints to maximize the number of matching records identified using a limited amount of work. Using real data sets, we illustrate the potential gains of our pay-as-you-go approach compared to running ER without using hints.},
  timestamp = {2016-10-07T17:10:20Z},
  number = {5},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  author = {Whang, S. E. and Marmaros, D. and Garcia-Molina, H.},
  month = may,
  year = {2013},
  keywords = {Approximation algorithms,Clustering algorithms,Companies,data cleaning,Data structures,entity resolution,Erbium,Partitioning algorithms,pay-as-you-go,Tin},
  pages = {1111--1124},
  file = {IEEE Xplore Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/UX23534Q/Whang et al. - 2013 - Pay-As-You-Go Entity Resolution.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/VP2G2BSJ/6155721.html:text/html}
}

@phdthesis{kulkarni_recommendation_2015,
  title = {A {{Recommendation Engine Using Apache Spark}}},
  abstract = {The volume of structured and unstructured data has grown at exponential scale in recent days. As a result of this rapid data growth, we are always inundated with plethora of choices in any product or service. It is very natural to get lost in the amazon of such choices and finding hard to make decisions. The project aims at addressing this problem by using entity recommendation. The two main aspects that the project concentrates on are implementing and presenting more accurate entity recommendations to the user and another is dealing with vast amount of data. The project aims at presenting recommendation results according to user's query with efficiency and accuracy. Project makes use of ListNet ranking algorithm to rank the recommendation results. Query independent features and query dependent features are used to come up with ranking scores. Ranking scores decide the order in which the recommendation results are presented to the user. Project makes use of Apache Spark, a distributed bigdata processing framework. Spark gives the advantage of handling iterative and interactive algorithms with efficiency and minimal processing time as compared to traditional mapreduce paradigm. We performed the experiments for recommendation engine using DBPedia as the dataset and tested the results for movie domain. We used both queryindependent (pagerank) and querydependent (clicklogs) features for ranking purposes. We observed that ListNet algorithm performs really well by making use of Apache Spark as the RDDs provide faster way for iterative algorithms to execute. We also observed that the results of recommendation engine are accurate and the entities are well ranked.},
  timestamp = {2016-10-07T17:11:04Z},
  urldate = {2016-10-07},
  url = {http://scholarworks.sjsu.edu/etd_projects/456},
  school = {San Jose State University},
  author = {Kulkarni, Swapna},
  year = {2015},
  file = {Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/P3IFV5FS/Kulkarni - 2015 - A Recommendation Engine Using Apache Spark.pdf:application/pdf;Snapshot:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/5WS74UFU/456.html:text/html}
}

@article{altwaijry_query:_2015,
  title = {{{QuERy}}: {{A Framework}} for {{Integrating Entity Resolution}} with {{Query Processing}}},
  volume = {9},
  issn = {2150-8097},
  shorttitle = {{{QuERy}}},
  doi = {10.14778/2850583.2850587},
  abstract = {This paper explores an analysis-aware data cleaning architecture for a large class of SPJ SQL queries. In particular, we propose QuERy, a novel framework for integrating entity resolution (ER) with query processing. The aim of QuERy is to correctly and efficiently answer complex queries issued on top of dirty data. The comprehensive empirical evaluation of the proposed solution demonstrates its significant advantage in terms of efficiency over the traditional techniques for the given problem settings.},
  timestamp = {2016-10-07T17:12:12Z},
  number = {3},
  urldate = {2016-10-07},
  url = {http://dx.doi.org/10.14778/2850583.2850587},
  journal = {Proc. VLDB Endow.},
  author = {Altwaijry, Hotham and Mehrotra, Sharad and Kalashnikov, Dmitri V.},
  month = nov,
  year = {2015},
  pages = {120--131},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/3EZ9K7HW/Altwaijry et al. - 2015 - QuERy A Framework for Integrating Entity Resoluti.pdf:application/pdf;QuERy:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/ECSQGCBW/citation.html:text/html}
}

@inproceedings{christen_towards_2008,
  address = {Darlinghurst, Australia, Australia},
  series = {AusDM '08},
  title = {Towards {{Scalable Real}}-Time {{Entity Resolution Using}} a {{Similarity}}-Aware {{Inverted Index Approach}}},
  isbn = {978-1-920682-68-2},
  abstract = {Most research into entity resolution (also known as record linkage or data matching) has concentrated on the quality of the matching results. In this paper, we focus on matching time and scalability, with the aim to achieve large-scale real-time entity resolution. Traditional entity resolution techniques have assumed the matching of two static databases. In our networked and online world, however, it is becoming increasingly important for many organisations to be able to conduct entity resolution between a collection of often very large databases and a stream of query or update records. The matching should be done in (near) real-time, and be as automatic and accurate as possible, returning a ranked list of matched records for each given query record. This task therefore becomes similar to querying large document collections, as done for example by Web search engines, however based on a different type of documents: structured database records that, for example, contain personal information, such as names and addresses. In this paper, we investigate inverted indexing techniques, as commonly used in Web search engines, and employ them for real-time entity resolution. We present two variations of the traditional inverted index approach, aimed at facilitating fast approximate matching. We show encouraging initial results on large real-world data sets, with the inverted index approaches being up-to one hundred times faster than the traditionally used standard blocking approach. However, this improved matching speed currently comes at a cost, in that matching quality for larger data sets can be lower compared to when standard blocking is used, and thus more work is required.},
  timestamp = {2016-10-07T17:13:10Z},
  urldate = {2016-10-07},
  url = {http://dl.acm.org/citation.cfm?id=2449288.2449299},
  booktitle = {Proceedings of the 7th {{Australasian Data Mining Conference}} - {{Volume}} 87},
  publisher = {{Australian Computer Society, Inc.}},
  author = {Christen, Peter and Gayler, Ross},
  year = {2008},
  keywords = {approximate string comparisons,data matching,record linkage,Scalability,similarity measures},
  pages = {51--60},
  file = {ACM Full Text PDF:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/KB3PP5VE/Christen and Gayler - 2008 - Towards Scalable Real-time Entity Resolution Using.pdf:application/pdf;Towards scalable real-time entity resolution using a similarity-aware inverted index approach:/home/sappo/.mozilla/firefox/o9qk03fu.default/zotero/storage/27NVH45F/citation.html:text/html}
}

@comment{jabref-meta: groupsversion:3;}
@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:Master_Thesis\;0\;kolb_parallel_2013\;shah_storm_2015\
;elmagarmid_duplicate_2007\;kopcke_frameworks_2010\;konda_magellan:_20
16\;cormode_summarizing_2005\;malhotra_graph-parallel_2014\;brizan_._2
015\;kwon_study_2011\;undefined\;recasens_coreference_2010\;tirumali_e
fficient_2016\;whang_pay-as-you-go_2013\;kulkarni_recommendation_2015\
;altwaijry_query:_2015\;christen_towards_2008\;;
}

